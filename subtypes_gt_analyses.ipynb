{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T10:35:33.435479",
     "start_time": "2017-02-14T10:35:33.430056"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from mne import read_label\n",
    "from pandas import read_csv\n",
    "from pandas import read_table\n",
    "\n",
    "## Specify directories and parameters.\n",
    "fast_dir = '/space/will/3/users/EMBARC/EMBARC-FAST'\n",
    "fsd = 'rest'\n",
    "run = '001'\n",
    "\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "### Load cortical labels.\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "\n",
    "parc_dir = '/space/lilli/1/users/DARPA-Recons/fscopy/label/yeo114_orig'\n",
    "lh_labels = [read_label(os.path.join(parc_dir, l)) for l in sorted(os.listdir(parc_dir)) if 'LH' in l]\n",
    "rh_labels = [read_label(os.path.join(parc_dir, l)) for l in sorted(os.listdir(parc_dir)) if 'RH' in l]\n",
    "\n",
    "labels = np.append(lh_labels,rh_labels)\n",
    "\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "### Load subcortical labels.\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "\n",
    "## Read Freesurfer Color Lookup Table. \n",
    "rois = ['Left-Accumbens-area', 'Left-Thalamus-Proper', 'Left-Caudate', 'Left-Putamen', 'Left-Hippocampus', 'Left-Amygdala', \n",
    "        'Right-Accumbens-area', 'Right-Thalamus-Proper', 'Right-Caudate', 'Right-Putamen', 'Right-Hippocampus', 'Right-Amygdala']\n",
    "\n",
    "subcort_labels = dict.fromkeys([l for l in rois])\n",
    "\n",
    "clut = read_table(os.path.join('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/labels', 'FreeSurferColorLUT_truncated.txt'), \n",
    "                                              sep=' *', skiprows=4, names=['No', 'Label','R','G','B','A'], engine='python')\n",
    "clut = clut.loc[np.where(np.in1d(clut.Label,rois))]\n",
    "\n",
    "## Load subcortical segmentation.\n",
    "aseg_obj = nib.load('/space/lilli/1/users/DARPA-Recons/fsaverage/mri.2mm/aseg.mgz')\n",
    "aseg_dat = aseg_obj.get_data()\n",
    "\n",
    "for l in rois: subcort_labels[l] = np.where(aseg_dat == int(clut.No[clut.Label == l]))\n",
    "print 'cortical lh: %d,' %len(lh_labels), 'rh: %d\\n' %len(rh_labels), 'subcortical: %d' %len(subcort_labels)\n",
    "\n",
    "## Save list of regions. \n",
    "regions = np.array([l.name for l in labels] + rois)\n",
    "np.savetxt(os.path.join(fast_dir, 'scripts', 'subtypes_gt', 'gt_regions'), regions, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data. Average by Labels. (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-31T14:02:15.928162",
     "start_time": "2017-01-31T13:55:39.325130"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from pandas import DataFrame\n",
    "from scipy.stats import pearsonr\n",
    "from mne.filter import band_pass_filter\n",
    "\n",
    "## Define functions and directories.\n",
    "def zscore(arr): return (arr - arr.mean()) / arr.std()\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Define matrices.\n",
    "n_preds, n_subs = qmat.shape\n",
    "n_regions = len(labels) + len(subcort_labels)\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "# ## Subjects with manual coregistration.\n",
    "# subjects = ['CU0092CUMR1R1', 'CU0087CUMR1R1', 'CU0095CUMR1R1', 'CU0104CUMR1R1', 'CU0131CUMR1R1', 'CU0069CUMR1R1', \n",
    "#             'CU0025CUMR1R1', 'CU0094CUMR1R1', 'CU0067CUMR1R1', 'CU0106CUMR1R1', 'CU0009CUMR1R1', 'CU0105CUMR1R1', \n",
    "#             'CU0070CUMR1R1', 'CU0129CUMR1R1', 'CU0113CUMR1R1', 'CU0125CUMR1R1', 'CU0133CUMR1R1']\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print i, subject\n",
    "\n",
    "    ## Load cortical/subcortical data.\n",
    "    lh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'lh'))).get_data().squeeze()\n",
    "    rh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'rh'))).get_data().squeeze()\n",
    "    sc_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.%s.2mm.nii.gz' %('mni305')))).get_data()\n",
    "    \n",
    "    ## Drop first 4 volumes. \n",
    "    lh_dat = np.delete(lh_dat, np.arange(4), axis=1)\n",
    "    rh_dat = np.delete(rh_dat, np.arange(4), axis=1)\n",
    "    sc_dat = np.delete(sc_dat, np.arange(4), axis=3)\n",
    "\n",
    "    n_times = sc_dat.shape[3]\n",
    "    n_verts = lh_dat.shape[0]\n",
    "    \n",
    "    ## Remove any extra timepoints collected.\n",
    "    if n_times > 176:\n",
    "        print 'Removing last %d timepoints for %s' %(n_times-176,subject)\n",
    "        lh_dat = np.delete(lh_dat, np.arange(176,n_times), axis=1)\n",
    "        rh_dat = np.delete(rh_dat, np.arange(176,n_times), axis=1)\n",
    "        sc_dat = np.delete(sc_dat, np.arange(176,n_times), axis=3)\n",
    "        n_times = sc_dat.shape[3]\n",
    "            \n",
    "    elif n_times < 176: \n",
    "        print 'Mismatch in number of timepoints. %s, n_times: %d. Missing %d timepoints.' %(subject, n_times, 176-n_times)\n",
    "        break\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Average by labels.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "    mean_dat = np.zeros((n_regions,n_times))\n",
    "    \n",
    "    for idx, label in enumerate(lh_labels):\n",
    "        mean_dat[idx] = lh_dat[label.vertices].mean(axis=0)\n",
    "        \n",
    "    for idx, label in enumerate(rh_labels): \n",
    "        mean_dat[idx+len(lh_labels)] = rh_dat[label.vertices].mean(axis=0)\n",
    "            \n",
    "    for idx,k in zip(np.arange(len(labels), n_regions),subcort_labels.keys()):\n",
    "        x,y,z = subcort_labels[k]\n",
    "        mean_dat[idx] = sc_dat[x,y,z].mean(axis=0)\n",
    "        \n",
    "    ## Save data averaged by labels. \n",
    "    np.savez_compressed(os.path.join(gt_dir, 'raw', '%s_raw' %subject), mean_dat=mean_dat)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T16:09:31.472985",
     "start_time": "2017-02-02T16:09:31.470878"
    }
   },
   "source": [
    "### Plot average signal by label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save out ratio signal. (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-10T13:25:47.394092",
     "start_time": "2017-02-10T11:51:56.550217"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "\n",
    "def get_signal_ratio(arr, idx): return np.count_nonzero(arr[idx]) / float(arr[idx].shape[0] * arr[idx].shape[1])\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "n_regions = len(labels) + len(subcort_labels)\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "sig_ratio = np.zeros((len(subjects), n_regions))\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print i, subject\n",
    "\n",
    "    ## Load cortical/subcortical data.\n",
    "    lh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'lh'))).get_data().squeeze()\n",
    "    rh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'rh'))).get_data().squeeze()\n",
    "    sc_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.%s.2mm.nii.gz' %('mni305')))).get_data()\n",
    "    \n",
    "    ## Drop first 4 volumes. \n",
    "    lh_dat = np.delete(lh_dat, np.arange(4), axis=1)\n",
    "    rh_dat = np.delete(rh_dat, np.arange(4), axis=1)\n",
    "    sc_dat = np.delete(sc_dat, np.arange(4), axis=3)\n",
    "\n",
    "    n_times = lh_dat.shape[1]\n",
    "    n_verts = lh_dat.shape[0]\n",
    "    \n",
    "    ## Remove any extra timepoints collected.\n",
    "    if n_times > 176:\n",
    "        print 'Removing last %d timepoints for %s' %(n_times-176,subject)\n",
    "        lh_dat = np.delete(lh_dat, np.arange(176,n_times), axis=1)\n",
    "        rh_dat = np.delete(rh_dat, np.arange(176,n_times), axis=1)\n",
    "        sc_dat = np.delete(sc_dat, np.arange(176,n_times), axis=3)\n",
    "        n_times = lh_dat.shape[1]\n",
    "            \n",
    "    elif n_times < 176: \n",
    "        print 'Mismatch in number of timepoints. %s, n_times: %d. Missing %d timepoints.' %(subject, n_times, 176-n_times)\n",
    "        break\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Calcualate label signal ratio.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "    \n",
    "    sub_ratio = np.zeros((n_regions))\n",
    "    \n",
    "    for idx, label in enumerate(lh_labels):\n",
    "        sub_ratio[idx] = get_signal_ratio(lh_dat, label.vertices)\n",
    "        \n",
    "    for idx, label in enumerate(rh_labels): \n",
    "        sub_ratio[idx+len(lh_labels)] = get_signal_ratio(rh_dat, label.vertices)\n",
    "            \n",
    "    for idx,k in zip(np.arange(len(labels), n_regions),subcort_labels.keys()):\n",
    "        x,y,z = subcort_labels[k]\n",
    "        sub_ratio[idx] = get_signal_ratio(sc_dat, [x,y,z])\n",
    "        \n",
    "    sig_ratio[i] = sub_ratio\n",
    "\n",
    "## Save. \n",
    "np.save(os.path.join(gt_dir, 'signal_ratio'), sig_ratio)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ratio signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-10T14:59:08.071454",
     "start_time": "2017-02-10T14:59:07.540907"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pylab as plt \n",
    "\n",
    "rs = np.load(os.path.join(gt_dir, 'signal_ratio.npy')).T\n",
    "rs = rs * 100\n",
    "rs_mean = np.mean(rs, axis=1)\n",
    "\n",
    "label_names = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_cortical_labels\", 'r')])\n",
    "\n",
    "## Scatterplot for percent signal across subjects. \n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "y = (rs_mean)\n",
    "mask = np.where(y<99.9)[0]\n",
    "colors = np.where(y<99.9, '#d7191c', '#2b83ba')\n",
    "\n",
    "plt.scatter(np.arange(rs_mean.shape[0]),y,c=colors)\n",
    "\n",
    "for i,j,k in zip(label_names[mask], mask, y[mask]): \n",
    "    plt.annotate('%s: %.2f' %('_'.join(i.split('_')[2:4]),y[j]), (j,k))\n",
    "\n",
    "plt.title('Percent label signal', fontsize=18)\n",
    "plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/percent_signal_labels.png')\n",
    "\n",
    "## Plot histogram for each label. \n",
    "for r in mask: \n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    \n",
    "    plt.hist(rs[r]*100, normed=True)\n",
    "    plt.title(label_names[r])\n",
    "    plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/%s.png' %label_names[r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Nuisance Regress. (Includes GSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T16:08:07.108475",
     "start_time": "2017-02-02T16:08:06.693624"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from pandas import DataFrame\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import detrend\n",
    "from sklearn.decomposition import PCA\n",
    "from mne.filter import band_pass_filter\n",
    "\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print i, subject\n",
    "    \n",
    "    ## Load data.\n",
    "    mean_dat = np.load(os.path.join(gt_dir, 'raw', '%s_raw.npz' %subject))['mean_dat']\n",
    "    \n",
    "    ## Remove first 4 timepoints from data.\n",
    "    mean_dat = np.delete(mean_dat, np.arange(4), axis=1)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Perform Nuissance Regression and Filtering.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    n_components = 5        # No. of components to use from PCA.\n",
    "\n",
    "    ## Load global signal regressors. \n",
    "    gs = os.path.join(fast_dir, subject,fsd, run, 'global.waveform.dat')\n",
    "    gs = read_table(gs, sep=' *', header=None, engine='python').as_matrix()\n",
    "    \n",
    "    ## Load wm and csf regressors. \n",
    "    wm = read_table(os.path.join(fast_dir, subject,fsd, run, 'wm.dat'), sep=' *', header=None, engine='python')\n",
    "    vcsf = read_table(os.path.join(fast_dir, subject,fsd, run, 'vcsf.dat'), sep=' *', header=None, engine='python')\n",
    "    \n",
    "    ## Keep only top 5 components.\n",
    "    n_components = 5 \n",
    "    wm = wm[wm.columns[:n_components]].as_matrix()\n",
    "    vcsf = vcsf[vcsf.columns[:n_components]].as_matrix()\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if gs.shape[0] > 176: gs = np.delete(gs, np.arange(176,gs.shape[0]), axis=0)\n",
    "    if wm.shape[0] > 176: wm = np.delete(wm, np.arange(176,wm.shape[0]), axis=0)\n",
    "    if vcsf.shape[0] > 176: vcsf = np.delete(vcsf, np.arange(176,vcsf.shape[0]), axis=0)\n",
    "    \n",
    "    ## Load motion regressors.\n",
    "    mc = os.path.join(fast_dir, subject, fsd, run, 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:7]\n",
    "    if mc.shape[0] > 176:\n",
    "        mc = np.delete(mc, np.arange(176,mc.shape[0]), axis=0)\n",
    "\n",
    "    ## Demean/detrend data.\n",
    "    mc = detrend(mc, axis=0, type='constant')\n",
    "    mc = detrend(mc, axis=0, type='linear') ## Keeps breaking\n",
    "\n",
    "    ## Construct basis sets.\n",
    "    mc = np.concatenate([mc, np.roll(mc, -1, 0)], axis=-1)\n",
    "    mc = np.concatenate([mc, np.power(mc,2)], axis=-1)\n",
    "\n",
    "    ## Replace last timepoint with zeros \n",
    "    ## Otherwise you are putting motion from the first frame into the last of the run\n",
    "    mc[-1] = np.zeros((mc[-1].shape))\n",
    "\n",
    "    ## Apply PCA.  \n",
    "    pca = PCA(n_components=24)\n",
    "    mc = pca.fit_transform(mc)  \n",
    "\n",
    "    ## Concatenate nuisance regressors. \n",
    "    ns = np.hstack((mc,gs,wm,vcsf))\n",
    "    \n",
    "    ## Remove first 4 timepoints.\n",
    "    ns = np.delete(ns, np.arange(4), axis=0) ## remove first 4 timepoints\n",
    "    \n",
    "    ## Filter parameters.\n",
    "    tr = 3\n",
    "    Fs = 1. / tr \n",
    "    Fp1 = 0.01\n",
    "    Fp2 = 0.08\n",
    "\n",
    "    ## Filter data and regressors.\n",
    "    mean_dat = band_pass_filter(mean_dat.astype(np.float64), Fs, Fp1, Fp2, filter_length='120s', method='iir')\n",
    "    ns = band_pass_filter(ns.T, Fs, Fp1, Fp2, filter_length='120s', method='iir').T\n",
    "    \n",
    "    ## Regress. \n",
    "    betas,_,_,_ = np.linalg.lstsq(ns,mean_dat.T)\n",
    "    regressed = mean_dat - np.dot(ns, betas).T\n",
    "    \n",
    "    ## Save regressed data.\n",
    "    np.savez_compressed(os.path.join(gt_dir, 'regressed', '%s_regressed' %subject), regressed=regressed)\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Nuisance Regress. (NOT including GSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-10T15:16:41.986081",
     "start_time": "2017-02-10T15:15:52.683588"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from pandas import DataFrame\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import detrend\n",
    "from sklearn.decomposition import PCA\n",
    "from mne.filter import band_pass_filter\n",
    "\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print i, subject\n",
    "    \n",
    "    ## Load data.\n",
    "    mean_dat = np.load(os.path.join(gt_dir, 'raw', '%s_raw.npz' %subject))['mean_dat']\n",
    "    \n",
    "    ## Remove first 4 timepoints from data.\n",
    "    mean_dat = np.delete(mean_dat, np.arange(4), axis=1)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Perform Nuissance Regression and Filtering.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    n_components = 5        # No. of components to use from PCA.\n",
    "    \n",
    "    ## Load wm and csf regressors. \n",
    "    wm = read_table(os.path.join(fast_dir, subject,fsd, run, 'wm.dat'), sep=' *', header=None, engine='python')\n",
    "    vcsf = read_table(os.path.join(fast_dir, subject,fsd, run, 'vcsf.dat'), sep=' *', header=None, engine='python')\n",
    "    \n",
    "    ## Keep only top 5 components.\n",
    "    n_components = 5 \n",
    "    wm = wm[wm.columns[:n_components]].as_matrix()\n",
    "    vcsf = vcsf[vcsf.columns[:n_components]].as_matrix()\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if wm.shape[0] > 176: wm = np.delete(wm, np.arange(176,wm.shape[0]), axis=0)\n",
    "    if vcsf.shape[0] > 176: vcsf = np.delete(vcsf, np.arange(176,vcsf.shape[0]), axis=0)\n",
    "    \n",
    "    ## Load motion regressors.\n",
    "    mc = os.path.join(fast_dir, subject, fsd, run, 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:7]\n",
    "    if mc.shape[0] > 176:\n",
    "        mc = np.delete(mc, np.arange(176,mc.shape[0]), axis=0)\n",
    "\n",
    "    ## Demean/detrend data.\n",
    "    mc = detrend(mc, axis=0, type='constant')\n",
    "    mc = detrend(mc, axis=0, type='linear') ## Keeps breaking\n",
    "\n",
    "    ## Construct basis sets.\n",
    "    mc = np.concatenate([mc, np.roll(mc, -1, 0)], axis=-1)\n",
    "    mc = np.concatenate([mc, np.power(mc,2)], axis=-1)\n",
    "\n",
    "    ## Replace last timepoint with zeros \n",
    "    ## Otherwise you are putting motion from the first frame into the last of the run\n",
    "    mc[-1] = np.zeros((mc[-1].shape))\n",
    "\n",
    "    ## Apply PCA.  \n",
    "    pca = PCA(n_components=24)\n",
    "    mc = pca.fit_transform(mc)  \n",
    "\n",
    "    ## Concatenate nuisance regressors. \n",
    "    ns = np.hstack((mc,wm,vcsf))\n",
    "    \n",
    "    ## Remove first 4 timepoints.\n",
    "    ns = np.delete(ns, np.arange(4), axis=0) ## remove first 4 timepoints\n",
    "    \n",
    "    ## Filter parameters.\n",
    "    tr = 3\n",
    "    Fs = 1. / tr \n",
    "    Fp1 = 0.01\n",
    "    Fp2 = 0.08\n",
    "\n",
    "    ## Filter data and regressors.\n",
    "    mean_dat = band_pass_filter(mean_dat.astype(np.float64), Fs, Fp1, Fp2, filter_length='120s', method='iir')\n",
    "    ns = band_pass_filter(ns.T, Fs, Fp1, Fp2, filter_length='120s', method='iir').T\n",
    "    \n",
    "    ## Regress. \n",
    "    betas,_,_,_ = np.linalg.lstsq(ns,mean_dat.T)\n",
    "    regressed = mean_dat - np.dot(ns, betas).T\n",
    "    \n",
    "    ## Save regressed data.\n",
    "    np.savez_compressed(os.path.join(gt_dir, 'regressed', '%s_regressed_nogsr' %subject), regressed=regressed)\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate SNR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T12:42:53.738694",
     "start_time": "2017-02-02T12:09:17.543179"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "out_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/preproc'\n",
    "    \n",
    "snr_mat = np.zeros((len(subjects)))\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print subject\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load masks.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    mask_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001/masks' %subject\n",
    "    mri_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001' %subject\n",
    "\n",
    "    brainmask = os.path.join(mask_dir,'brain.nii.gz')\n",
    "    brainmask = np.where( nib.load(brainmask).get_data(), 1, 0 ) # Binarize the mask\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load functional data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#    \n",
    "    fmcpr = os.path.join(mri_dir,'fmcpr.nii.gz')\n",
    "    fmcpr = nib.load(fmcpr).get_data()\n",
    "    \n",
    "    fmcpr_mean = np.mean(fmcpr,axis=-1)\n",
    "    brain_mean = np.where(brainmask, fmcpr_mean, 0).mean()\n",
    "    noise_std = np.where(~brainmask, fmcpr_mean, 0).std()\n",
    "    \n",
    "    snr = brain_mean / noise_std\n",
    "    snr_mat[i] = snr\n",
    "    \n",
    "pd.DataFrame(snr_mat, columns=['SNR'], index=subjects).to_csv(os.path.join(out_dir, 'snr.csv'))\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and plot motion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate functional displacement and other motion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T14:35:19.151443",
     "start_time": "2017-02-02T14:35:18.597347"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "def demean(arr): return arr - arr.mean()\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "fmcpr.mcdat file format: \n",
    "1. n      : time point\n",
    "2. roll   : rotation about the I-S axis (degrees CCW)\n",
    "3. pitch  : rotation about the R-L axis (degrees CCW)\n",
    "4. yaw    : rotation about the A-P axis (degrees CCW)\n",
    "5. dS     : displacement in the Superior direction (mm)\n",
    "6. dL     : displacement in the Left direction (mm)\n",
    "7. dP     : displacement in the Posterior direction (mm)\n",
    "8. rmsold : RMS difference between input frame and reference frame\n",
    "9. rmsnew : RMS difference between output frame and reference frame\n",
    "10. trans : translation (mm) = sqrt(dS^2 + dL^2 + dP^2)\n",
    "\n",
    "Translations (displacement) are in mm, rotations are in degrees.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "threshold = 0.5\n",
    "fast_dir = '/space/will/3/users/EMBARC/EMBARC-FAST'\n",
    "fsd = 'rest'\n",
    "run = '001'\n",
    "\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "## Subjects with manual coregistration.\n",
    "# subjects = ['CU0092CUMR1R1', 'CU0087CUMR1R1', 'CU0095CUMR1R1', 'CU0104CUMR1R1', 'CU0131CUMR1R1', 'CU0069CUMR1R1', \n",
    "#             'CU0025CUMR1R1', 'CU0094CUMR1R1', 'CU0067CUMR1R1', 'CU0106CUMR1R1',  'CU0105CUMR1R1', \n",
    "#             'CU0070CUMR1R1', 'CU0129CUMR1R1', 'CU0113CUMR1R1', 'CU0125CUMR1R1', 'CU0133CUMR1R1']\n",
    "\n",
    "motion = np.zeros((len(subjects),6))\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare motion data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Read motion data.\n",
    "    mc = os.path.join(fast_dir, subject, fsd, run, 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:]\n",
    "\n",
    "    ## Remove first four timepoints.\n",
    "    mc = np.delete(mc, np.arange(4),axis=0)\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if mc.shape[0] > 176:\n",
    "        mc = np.delete(mc, np.arange(176,mc.shape[0]), axis=0)\n",
    "\n",
    "    ## Invert angular displacement.\n",
    "    mc[:,:3] = np.deg2rad(mc[:,:3]) \n",
    "    mc[:,:3] *= 50\n",
    "    \n",
    "    abs_mean = mc[8].mean()\n",
    "    abs_max = mc[8].max()\n",
    "    rel_mean = abs(np.diff(mc[9])).mean()\n",
    "    rel_max = abs(np.diff(mc[9])).max()\n",
    "\n",
    "    ## Compute framewise displacement (See Power 2012, 2014).\n",
    "    fd = np.insert( np.abs( np.diff(mc[:,3:], axis=0) ).sum(axis=1), 0, 0 )\n",
    "    \n",
    "    fd_mean = fd.mean()\n",
    "    fd_max = fd.max()\n",
    "    \n",
    "    motion[i] = np.round(np.vstack([abs_mean, abs_max, rel_mean, rel_max, fd_mean, fd_max]).flatten(),2)\n",
    "        \n",
    "pd.DataFrame(motion, columns=['abs_mean', 'abs_max', 'rel_mean', 'rel_max', 'FD_mean', 'FD_max'], index=subjects).to_csv(os.path.join(out_dir, 'fd.csv'))\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask grey and white matter timeseries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T14:49:57.770579",
     "start_time": "2017-02-02T14:43:32.351780"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Please see /space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/wm_masks.csh for instructions on how to make wm masks.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pandas import read_csv\n",
    "from mne.filter import construct_iir_filter, filter_data\n",
    "def demean(arr): return arr - arr.mean()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "decim = 250\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "for subject in subjects[:3]:\n",
    "    \n",
    "    print subject\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and prepare masks.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    mask_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001/masks' %subject\n",
    "    mri_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001' %subject\n",
    "    out_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001/motion' %subject\n",
    "\n",
    "    brainmask = os.path.join(mask_dir,'brain.nii.gz')\n",
    "    brainmask = np.where( nib.load(brainmask).get_data(), 1, 0 ) # Binarize the mask\n",
    "\n",
    "    wm = os.path.join(mask_dir,'wm.mgz')\n",
    "    wm = np.where( nib.load(wm).get_data(), 1, 0 ) # Binarize the mask\n",
    "\n",
    "    gm = brainmask - wm\n",
    "\n",
    "    ## Reduce to indices of interest.\n",
    "    gm = np.vstack(np.where(gm))[:,::decim]\n",
    "    wm = np.vstack(np.where(wm))[:,::decim]\n",
    "    del brainmask\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and slice through EPI image.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load data.\n",
    "    obj = nib.load(os.path.join(mri_dir, 'fmcpr.nii.gz'))\n",
    "    _,_,_,n_acq = obj.shape\n",
    "\n",
    "    ## Preallocoate space for timeseries.\n",
    "    gmts = np.zeros((n_acq, gm.shape[-1]))\n",
    "    wmts = np.zeros((n_acq, wm.shape[-1]))\n",
    "\n",
    "    for n in range(n_acq):\n",
    "\n",
    "        ## Slice image.\n",
    "        acq = obj.dataobj[..., n]\n",
    "\n",
    "        ## Store grey matter.\n",
    "        gmts[n] += acq[gm[0],gm[1],gm[2]]\n",
    "\n",
    "        ## Store white matter.\n",
    "        wmts[n] += acq[wm[0],wm[1],wm[2]]\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Preprocessing data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ## Construct highpass filter.\n",
    "    tr = 3\n",
    "    sfreq = 1. / tr\n",
    "    high_pass = 0.01\n",
    "    iir_params = dict(order=2, ftype='butter', output='sos') # Following Power et al. (2014)\n",
    "    iir_params = construct_iir_filter(iir_params, high_pass, None, sfreq, 'highpass', return_copy=False)  \n",
    "\n",
    "    ## Filter data.\n",
    "    gmts = filter_data(gmts.T, sfreq, high_pass, None, method='iir', iir_params=iir_params, verbose=False)\n",
    "    wmts = filter_data(wmts.T, sfreq, high_pass, None, method='iir', iir_params=iir_params, verbose=False)\n",
    "\n",
    "    ## De-mean.\n",
    "    gmts = np.apply_along_axis(demean, 1, gmts)\n",
    "    wmts = np.apply_along_axis(demean, 1, wmts)\n",
    "\n",
    "    ## Re-organize (center outwards).\n",
    "    gmts = gmts[ np.argsort( np.power( np.apply_along_axis(demean, 1, gm), 2 ).sum(axis=0) ) ]\n",
    "    wmts = gmts[ np.argsort( np.power( np.apply_along_axis(demean, 1, wm), 2 ).sum(axis=0) ) ]\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Save data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    if not os.path.isdir(out_dir): os.makedirs(out_dir)\n",
    "    f = os.path.join(out_dir, '%s_rest_qc_data' %subject)\n",
    "    np.savez_compressed(f, gm=gmts, wm=wmts, iir_params=iir_params )\n",
    "    del gmts, wmts, obj\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare motion and masked fMRI data. Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T15:32:49.355229",
     "start_time": "2017-02-02T15:19:33.986174"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "from scipy.signal import detrend\n",
    "from sklearn.decomposition import PCA\n",
    "def demean(arr): return arr - arr.mean()\n",
    "def rms(arr): return np.sqrt( np.mean( np.power(arr, 2) ) )\n",
    "%matplotlib inline\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "threshold = 0.5\n",
    "fast_dir = '/space/will/3/users/EMBARC/EMBARC-FAST'\n",
    "fsd = 'rest'\n",
    "run = '001'\n",
    "\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "## Subjects with manual coregistration.\n",
    "# subjects = ['CU0092CUMR1R1', 'CU0087CUMR1R1', 'CU0095CUMR1R1', 'CU0104CUMR1R1', 'CU0131CUMR1R1', 'CU0069CUMR1R1', \n",
    "#             'CU0025CUMR1R1', 'CU0094CUMR1R1', 'CU0067CUMR1R1', 'CU0106CUMR1R1',  'CU0105CUMR1R1', \n",
    "#             'CU0070CUMR1R1', 'CU0129CUMR1R1', 'CU0113CUMR1R1', 'CU0125CUMR1R1', 'CU0133CUMR1R1']\n",
    "\n",
    "for subject in subjects[18:]:\n",
    "    \n",
    "    print subject, \n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare motion data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Read motion data.\n",
    "    mc = os.path.join(fast_dir, subject, fsd, run, 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:7]\n",
    "\n",
    "    ## Remove first four timepoints.\n",
    "    mc = np.delete(mc, np.arange(4),axis=0)\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if mc.shape[0] > 176:\n",
    "        mc = np.delete(mc, np.arange(176,mc.shape[0]), axis=0)\n",
    "\n",
    "    ## Invert angular displacement.\n",
    "    copy = mc.copy()\n",
    "    mc[:,:3] = np.deg2rad(mc[:,:3]) \n",
    "    mc[:,:3] *= 50\n",
    "\n",
    "    ## Compute framewise displacement (See Power 2012, 2014).\n",
    "    fd = np.insert( np.abs( np.diff(mc[:,3:], axis=0) ).sum(axis=1), 0, 0)\n",
    "\n",
    "    ## Demean/detrend data.\n",
    "    mc = detrend(copy, axis=0, type='constant')\n",
    "    mc = detrend(mc, axis=0, type='linear')\n",
    "\n",
    "    ## Construct basis sets.\n",
    "    mcreg24 = mc.copy()\n",
    "    mcreg24 = np.concatenate([mcreg24, np.roll(mcreg24, -1, 0)], axis=-1)\n",
    "    mcreg24 = np.concatenate([mcreg24, np.power(mcreg24,2)], axis=-1)\n",
    "\n",
    "    ## Replace last timepoint with zeros.\n",
    "    ## Otherwise you are putting motion from the first frame into the last of the run.\n",
    "    mcreg24[-1] = np.zeros((mcreg24[-1].shape))\n",
    "    \n",
    "    ## Apply PCA.  \n",
    "    pca = PCA(n_components=24)\n",
    "    mcreg24 = pca.fit_transform(mcreg24)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare fMRI data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load gray/white matter timeseries.\n",
    "    npz = np.load(os.path.join(fast_dir, subject, fsd, run, 'motion/%s_rest_qc_data.npz' %subject))\n",
    "    \n",
    "    gm_pre = np.apply_along_axis(demean, 1, np.delete(npz['gm'],np.arange(4),axis=1))\n",
    "    wm_pre = np.apply_along_axis(demean, 1, np.delete(npz['wm'],np.arange(4),axis=1))\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if gm_pre.shape[1] > 176:\n",
    "        gm_pre = np.delete(gm_pre, np.arange(176,gm_pre.shape[1]), axis=1)\n",
    "        wm_pre = np.delete(wm_pre, np.arange(176,wm_pre.shape[1]), axis=1)\n",
    "\n",
    "    gm_beta, _, _, _ = np.linalg.lstsq(mcreg24, gm_pre.T)\n",
    "    wm_beta, _, _, _ = np.linalg.lstsq(mcreg24, wm_pre.T)\n",
    "    gm_post = gm_pre - np.dot(mcreg24, gm_beta).T\n",
    "    wm_post = wm_pre - np.dot(mcreg24, wm_beta).T\n",
    "\n",
    "    ## Compute DVARS.\n",
    "    gm_DVARS = np.zeros((2,fd.shape[0]))\n",
    "    wm_DVARS = np.zeros((2,fd.shape[0]))\n",
    "    for n, arr in enumerate([gm_pre,gm_post]): gm_DVARS[n,1:] += np.apply_along_axis(rms, 0, np.diff(arr, axis=1))\n",
    "    for n, arr in enumerate([wm_pre,wm_post]): wm_DVARS[n,1:] += np.apply_along_axis(rms, 0, np.diff(arr, axis=1))\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plotting\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    fig = plt.figure(figsize=(16,18))\n",
    "    nrow = 16\n",
    "\n",
    "    ## Plot framewise displacement.\n",
    "    ax = plt.subplot2grid((nrow,1),(0,0),rowspan=1)\n",
    "    ax.plot(fd, linewidth=1.5, color='k')\n",
    "    ax.hlines(0.5, 0, fd.shape[0], linestyle='--', color='k', alpha=0.5)\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([0.0,0.5,1.0])\n",
    "    ax.set_yticklabels([0.0,0.5,1.0], rotation=90)\n",
    "    ax.set_ylabel('FD (mm)', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.set_title(subject, fontsize=24)\n",
    "\n",
    "    ## Plot grey matter (pre-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(1,0),rowspan=4)\n",
    "    cbar = ax.imshow(gm_pre, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('Grey Matter\\nPre-Regression', fontsize=12)\n",
    "\n",
    "    # Colobar setup.\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width, box.height])\n",
    "    axColor = plt.axes([0.905, 0.238, 0.015, 0.660])\n",
    "    cbar = plt.colorbar(cbar, cax = axColor, orientation=\"vertical\")\n",
    "    cbar.set_ticks([-50,0,50])\n",
    "    axColor.set_ylabel('BOLD', fontsize=12, rotation=270)\n",
    "\n",
    "    ## Plot grey matter (post-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(5,0),rowspan=4)\n",
    "    cbar = ax.imshow(gm_post, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('Grey Matter\\nPost-Regression', fontsize=12)\n",
    "\n",
    "    ## Plot DVARS.\n",
    "    ax = plt.subplot2grid((nrow,1),(9,0),rowspan=1)\n",
    "    for arr, label in zip(gm_DVARS, ['Pre','Post']): ax.plot(arr, linewidth=1.5, alpha=0.8, label=label)\n",
    "    ax.legend(loc=2, fontsize=8, frameon=False, borderpad=0.0,  handlelength=1.4, handletextpad=0.2)\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_xlabel('Acquisitions', fontsize=12)\n",
    "    ax.set_ylim(20,100)\n",
    "    ax.set_yticks([20,60,100])\n",
    "    ax.set_yticklabels([20,60,100], rotation=90)\n",
    "    ax.set_ylabel('DVARS_gm', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    ## Plot white matter (pre-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(11,0),rowspan=2)\n",
    "    cbar = ax.imshow(wm_pre, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('White Matter\\nPre-Regression', fontsize=12)\n",
    "\n",
    "    ## Plot white matter (post-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(13,0),rowspan=2)\n",
    "    cbar = ax.imshow(wm_post, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('White Matter\\nPost-Regression', fontsize=12)\n",
    "\n",
    "    ## Plot DVARS.\n",
    "    ax = plt.subplot2grid((nrow,1),(15,0))\n",
    "    for arr, label in zip(wm_DVARS, ['Pre','Post']): ax.plot(arr, linewidth=1.5, alpha=0.8, label=label)\n",
    "    ax.legend(loc=2, fontsize=8, frameon=False, borderpad=0.0,  handlelength=1.4, handletextpad=0.2)\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_xlabel('Acquisitions', fontsize=12)\n",
    "    ax.set_ylim(20,100)\n",
    "    ax.set_yticks([20,60,100])\n",
    "    ax.set_yticklabels([20,60,100], rotation=90)\n",
    "    ax.set_ylabel('DVARS_wm', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    plt.subplots_adjust(left=0.05, right=0.90, top=0.95, bottom=0.05, hspace=0.05)\n",
    "#     plt.show()\n",
    "    plt.savefig(os.path.join(fast_dir,subject,fsd,run,'motion/%s_supp_motion.png' %subject), dpi=180)\n",
    "    plt.savefig(os.path.join(fast_dir, 'jan2017', 'motion_plots', '%s_supp_motion.png' %subject), dpi=180)\n",
    "    plt.close('all')\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-15T11:37:34.201953",
     "start_time": "2017-02-15T11:37:27.522550"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "        \n",
    "    ## Load regressed data. \n",
    "    regressed = np.load(os.path.join(gt_dir, 'regressed', '%s_regressed.npz' %subject))['regressed']\n",
    "    \n",
    "    ## Correlate. Remove negative correlations.\n",
    "    corrmat = np.corrcoef(regressed)\n",
    "#     corrmat = np.where(corrmat<=0, np.nan, corrmat)\n",
    "    \n",
    "    # Fisher's r-to-z transform.\n",
    "    zmat = np.arctanh(corrmat)\n",
    "    zmat = np.where(zmat==np.inf,np.nan,zmat)\n",
    "\n",
    "    ## Save correlation.\n",
    "    np.savez(os.path.join(gt_dir, 'correlations', '%s_correlations_unthresholded' %subject), corrmat=corrmat)\n",
    "    np.savez(os.path.join(gt_dir, 'correlations', '%s_zscores_unthresholded' %subject), zmat=zmat)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot z-scores for group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-15T12:06:29.276535",
     "start_time": "2017-02-15T12:06:29.266901"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "## Specify labels.\n",
    "regions = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')])\n",
    "\n",
    "rlh = np.concatenate([regions[:57],regions[114:120]])\n",
    "rrh = np.concatenate([regions[57:114],regions[120:]])\n",
    "\n",
    "regions = np.concatenate([lh,rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-15T13:24:13.908523",
     "start_time": "2017-02-15T13:23:49.665614"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "zmat_master = np.zeros((len(subjects),len(regions),len(regions)))\n",
    "\n",
    "for i, subject in enumerate(subjects): \n",
    "    \n",
    "    ## Load data.\n",
    "    zmat = np.load(os.path.join(gt_dir, 'correlations', '%s_zscores_unthresholded.npz' %subject))['zmat']\n",
    "    \n",
    "    ## Re-arrange.\n",
    "    ztop = np.concatenate([zmat[:57],zmat[114:120]])\n",
    "    zbottom = np.concatenate([zmat[57:114],zmat[120:]])\n",
    "    \n",
    "    zmat = np.concatenate([ztop,zbottom],axis=0)\n",
    "    \n",
    "    zleft = np.concatenate([zmat[:,:57],zmat[:,114:120]],axis=1)\n",
    "    zright = np.concatenate([zmat[:,57:114],zmat[:,120:]],axis=1)\n",
    "    \n",
    "    zmat = np.concatenate([zleft,zright],axis=1)\n",
    "\n",
    "    zmat_master[i] = zmat\n",
    "\n",
    "## Average across subjects. \n",
    "zmat_mean = np.nanmean(zmat_master, axis=0)\n",
    "\n",
    "## Convert to DataFrame.\n",
    "zpd = pd.DataFrame(zmat_mean, columns=regions, index=regions)\n",
    "\n",
    "## Generate a mask for the upper triangle. \n",
    "mask = np.zeros_like(zpd, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask,k=0)] = True\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot heatmap.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "vvals = [0.5,0.6,0.7,0.8,0.9,1.0,zmat_mean[~mask].max()]\n",
    "\n",
    "for vmax in vvals:\n",
    "    f, ax = plt.subplots(figsize=(25,20))\n",
    "    sns.heatmap(zpd, cmap=\"RdYlBu_r\", vmin=zmat_mean[~mask].min(), vmax=vmax, mask=mask, square=True, xticklabels=True, yticklabels=True, linewidth=0.1, \n",
    "                cbar_kws={'shrink':1.0}, ax=ax)\n",
    "\n",
    "    ## Draw lines. \n",
    "    xlines = xlines = [14,26,33,35,46,51,52,57,63,75,87,94,96,109,114,115,120]\n",
    "    ylines = [126-x for x in xlines]\n",
    "    plt.vlines(x=xlines, ymin=0, ymax=[126-x for x in xlines],linestyles='dotted')\n",
    "    plt.hlines(y=ylines, xmin=0, xmax=xlines, linestyles='dotted')\n",
    "\n",
    "    ## Flourishes\n",
    "    plt.title('Z-scores for group (GSR, unthresholded)', fontsize=30)\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    ## Save. \n",
    "    plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/correlations/group_zscores_gsr_unthresh_%.1f.png' %vmax)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Correlation Heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T14:59:27.297601",
     "start_time": "2017-02-14T14:59:26.896770"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "## Specify labels.\n",
    "regions = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')])\n",
    "\n",
    "lh = np.concatenate([regions[:57],regions[114:120]])\n",
    "rh = np.concatenate([regions[57:114],regions[120:]])\n",
    "\n",
    "regions = np.concatenate([lh,rh])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot correlations per subject (with/without GSR).\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for subject in subjects: \n",
    "    \n",
    "    print subject, \n",
    "    \n",
    "    zmat_gsr = np.load(os.path.join(gt_dir, 'correlations', '%s_correlations_unthresholded.npz' %subject))['corrmat']\n",
    "    zmat_nogsr = np.load(os.path.join(gt_dir, 'correlations', '%s_correlations_nogsr_unthresholded.npz' %subject))['corrmat']\n",
    "    \n",
    "    zmat_diff = zmat_gsr - zmat_nogsr\n",
    "    zmat = np.concatenate([np.concatenate([zmat_diff[:57],zmat_diff[114:120]]),np.concatenate([zmat_diff[57:114],zmat_diff[120:]])])\n",
    "    \n",
    "    ## Convert to DataFrame.\n",
    "    zpd = pd.DataFrame(zmat, columns=regions, index=regions)\n",
    "    \n",
    "    ## Generate a mask for the upper triangle. \n",
    "    mask = np.zeros_like(zpd, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plot heatmap.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    f, ax = plt.subplots(figsize=(25,20))\n",
    "    cmap=sns.color_palette(\"RdBu_r\")\n",
    "    sns.heatmap(zpd, mask=mask, cmap=cmap, square=True, xticklabels=True, yticklabels=True, linewidth=0.1, cbar_kws={'shrink':1.0}, ax=ax)\n",
    "    \n",
    "    ## Draw lines. \n",
    "    xlines = [14,26,33,35,46,52,57,63] + [63+x for x in [14,26,33,35,46,52,57]]\n",
    "    ylines = [126-x for x in xlines]\n",
    "    plt.vlines(x=xlines, ymin=0, ymax=[126-x for x in xlines],linestyles='dotted')\n",
    "    plt.hlines(y=ylines, xmin=0, xmax=xlines, linestyles='dotted')\n",
    "        \n",
    "    ## Flourishes.\n",
    "    ax.set_title('%s: Correlations GSR - noGSR, unthresholded' %subject, fontsize=30)\n",
    "    cax = plt.gcf().axes[-1]\n",
    "    cax.tick_params(labelsize=20,labeltop=True,labelbottom=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "#     plt.show()\n",
    "    plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/correlations/%s_corr_unthresh_GSRdiff.png' %subject)\n",
    "    plt.close()\n",
    "    \n",
    "print '\\nDone.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot correlation distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "## Specify regions.\n",
    "regions = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot correlations (with/without GSR).\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for subject in subjects: \n",
    "    \n",
    "    ## Load data.\n",
    "    zmat_gsr = np.load(os.path.join(gt_dir, 'correlations', '%s_correlations_unthresholded.npz' %subject))['corrmat']\n",
    "    zmat_nogsr = np.load(os.path.join(gt_dir, 'correlations', '%s_correlations_nogsr_unthresholded.npz' %subject))['corrmat']\n",
    "    \n",
    "    ## Convert to DataFrame.\n",
    "    zpd_gsr = pd.DataFrame(zmat_gsr, columns=regions, index=regions)\n",
    "    \n",
    "    ## Plotting.\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    sns.set(color_codes=True)\n",
    "    sns.set_palette(sns.color_palette(\"GnBu_d\"))\n",
    "    sns.distplot(zmat_gsr.flatten(),hist=False, kde=True, label='GSR', hist_kws={'alpha':0.7})\n",
    "    sns.distplot(zmat_nogsr.flatten(), hist=False, kde=True, label='No GSR', hist_kws={'alpha':0.7})\n",
    "    \n",
    "    ## Flourishes.\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.title('%s \\nDistribution of Correlations (unthresholded)' %subject, fontsize=16)\n",
    "    \n",
    "    ## Save.\n",
    "    plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/correlations/histograms/%s_corr_GSRhist_unthresh.png' %subject)\n",
    "    plt.close()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis (GSR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-15T13:48:55.265128",
     "start_time": "2017-02-15T13:48:47.903548"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import networkx as nx\n",
    "import scipy.io as sio\n",
    "%matplotlib inline\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define functions.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "def create_network(arr, regions): \n",
    "    \n",
    "    ## Create graph.\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(regions)\n",
    "    \n",
    "    for i,x in enumerate(regions):\n",
    "            for j,y in enumerate(regions):\n",
    "                if not np.isnan(arr[i,j]): \n",
    "                    G.add_edge(x,y)\n",
    "\n",
    "    G.remove_nodes_from(G.nodes_with_selfloops())\n",
    "    return G\n",
    "    \n",
    "    \n",
    "## Specify directories.\n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Load regions.\n",
    "regions = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')]\n",
    "n_regions = len(regions)\n",
    "\n",
    "## Specify subjects.\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "n_subs = len(subjects)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Construct network with GSR.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "Gnets = []\n",
    "\n",
    "for i,subject in enumerate(subjects): \n",
    "    zmat_gsr = np.load(os.path.join(gt_dir, 'correlations', '%s_zscores_unthresholded.npz' %subject))['zmat']\n",
    "    \n",
    "    G = create_network(zmat_gsr,regions)\n",
    "    Gnets.append(G)\n",
    "    \n",
    "Gnets = np.array(Gnets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold network and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-15T15:34:49.019380",
     "start_time": "2017-02-15T15:34:49.012057"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "3570\n",
      "84 4.44265125649\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for i,subject in enumerate(subjects[:1]): \n",
    "    \n",
    "    n_nodes = nx.number_of_nodes(Gnets[i])\n",
    "    n_edges = nx.number_of_edges(Gnets[i])\n",
    "    max_edges = (n_nodes*(n_nodes-1))/2\n",
    "\n",
    "    avg_degree = sum(nx.degree(Gnets[i]).values()) / n_nodes           ## total nodal degree divided by number of nodes\n",
    "    network_cost = n_edges / float(max_edges)                          ## total edges divided by maximum edges\n",
    "\n",
    "    print n_nodes\n",
    "    print n_edges \n",
    "    print avg_degree, np.log(n_nodes)\n",
    "    print network_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-15T13:36:59.975082",
     "start_time": "2017-02-15T13:36:59.972843"
    }
   },
   "source": [
    "### Compute global network metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-15T16:02:30.914519",
     "start_time": "2017-02-15T16:02:25.547318"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = ['n_nodes', 'n_edges', 'avg_degree', 'avg_path', 'density', 'n_components']\n",
    "metrics_dat = np.zeros((len(metrics), n_subs))\n",
    "\n",
    "for i in range(n_subs):\n",
    "    metrics_dat[0,i] = nx.number_of_nodes(Gnets[i])\n",
    "    metrics_dat[1,i] = nx.number_of_edges(Gnets[i])\n",
    "    metrics_dat[2,i] = sum(nx.degree(Gnets[i]).values())/Gnets[i].number_of_nodes()\n",
    "    metrics_dat[3,i] = nx.average_shortest_path_length(Gnets[i])\n",
    "    metrics_dat[4,i] = nx.density(Gnets[i])\n",
    "    metrics_dat[5,i] = len([l for l in nx.connected_components(Gnets[i])])\n",
    "    \n",
    "## Average degree of nodes in G should be >= 2*np.log(G.number_of_nodes()) Bullmore 2006/Drakesmith 2015.\n",
    "if not np.all(metrics_dat[2] > 2*np.log2(metrics_dat[0])): print 'Average degree of nodes not <= 2*log(#nodes)'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-12T13:21:47.400605",
     "start_time": "2017-01-12T13:21:47.398525"
    }
   },
   "source": [
    "### Compute node-specific metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-12T11:39:09.001741",
     "start_time": "2017-01-12T11:38:50.422857"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "node_metrics = ['degree', 'clustering coefficient']\n",
    "\n",
    "## Node-specific metrics.\n",
    "degree_dist = pd.DataFrame(columns=regions,index=subjects)\n",
    "clus_coeff = pd.DataFrame(columns=regions,index=subjects)\n",
    "for i,sub in enumerate(subjects):\n",
    "    degree_dist.loc[sub] = pd.Series(nx.degree(Gnets[i]))\n",
    "    clus_coeff.loc[sub] = pd.Series(nx.clustering(Gnets[i]))\n",
    "degree_dist = degree_dist.fillna(value=0)\n",
    "clus_coeff = clus_coeff.fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Construction (without GSR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import networkx as nx\n",
    "import scipy.io as sio\n",
    "%matplotlib inline\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define functions.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "def create_network(arr, regions): \n",
    "    \n",
    "    ## Create graph.\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(regions)\n",
    "    \n",
    "    for i,x in enumerate(regions):\n",
    "            for j,y in enumerate(regions):\n",
    "                if not np.isnan(arr[i,j]): \n",
    "                    G.add_edge(x,y)\n",
    "\n",
    "    G.remove_nodes_from(G.nodes_with_selfloops())\n",
    "    return G\n",
    "    \n",
    "    \n",
    "## Specify directories.\n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Load regions.\n",
    "regions = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')]\n",
    "n_regions = len(regions)\n",
    "\n",
    "## Specify subjects.\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "n_subs = len(subjects)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Construct network without GSR.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "Gnets_nogsr = []\n",
    "\n",
    "for i,subject in enumerate(subjects): \n",
    "    zmat_nogsr = np.load(os.path.join(gt_dir, 'correlations', '%s_zscores_nogsr_unthresholded.npz' %subject))['zmat']\n",
    "    \n",
    "    G = create_network(zmat_nogsr, regions)\n",
    "    Gnets_nogsr.append(G)\n",
    "    \n",
    "Gnets_nogsr = np.array(Gnets_nogsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute global network metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T14:35:16.645957",
     "start_time": "2017-02-14T14:35:04.991356"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Network metrics without GSR.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "metrics = ['n_nodes', 'n_edges', 'avg_degree', 'avg_path', 'density', 'n_components']\n",
    "metrics_dat_nogsr = np.zeros((len(metrics), n_subs))\n",
    "\n",
    "for i in range(n_subs):\n",
    "    metrics_dat_nogsr[0,i] = nx.number_of_nodes(Gnets_nogsr[i])\n",
    "    metrics_dat_nogsr[1,i] = nx.number_of_edges(Gnets_nogsr[i])\n",
    "    metrics_dat_nogsr[2,i] = sum(nx.degree(Gnets_nogsr[i]).values())/Gnets_nogsr[i].number_of_nodes()\n",
    "    metrics_dat_nogsr[3,i] = nx.average_shortest_path_length(Gnets_nogsr[i])\n",
    "    metrics_dat_nogsr[4,i] = nx.density(Gnets_nogsr[i])\n",
    "    metrics_dat_nogsr[5,i] = len([l for l in nx.connected_components(Gnets_nogsr[i])])\n",
    "    \n",
    "## Average degree of nodes in G should be >= 2*np.log(G.number_of_nodes()) Bullmore 2006/Drakesmith 2015.\n",
    "if not np.all(metrics_dat_nogsr[2] > 2*np.log2(metrics_dat_nogsr[0])): print 'Average degree of nodes not <= 2*log(#nodes)'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-12T13:25:41.974565",
     "start_time": "2017-01-12T13:25:41.972066"
    }
   },
   "source": [
    "### Plot distribution of global metrics. (GSR vs. noGSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T14:35:21.763866",
     "start_time": "2017-02-14T14:35:20.415025"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "## Plotting\n",
    "plt.figure(figsize=(24,18))\n",
    "for r in range(len(metrics)-1):\n",
    "    ax = plt.subplot2grid((2,3),(r/3,r%3),colspan=1,rowspan=1)\n",
    "    sns.distplot(metrics_dat[r], hist=False, label='GSR', hist_kws={'alpha':1})\n",
    "    sns.distplot(metrics_dat_nogsr[r], hist=False, label='No GSR', hist_kws={'alpha':0.5})\n",
    "    ax.set_title(metrics[r], fontsize=20)\n",
    "    ax.legend(fontsize=14)\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "plt.suptitle('Distribution of Graph Metrics (thresholded)', fontsize=24)\n",
    "plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/GSR_graph_metrics_dist_unthresh.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Questionnaires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    return [( float(i) - min(arr) )/( max(arr) - min(arr)) for i in arr] \n",
    "\n",
    "## Load questionnaires. \n",
    "csv = read_csv('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/questionnaires/subtypes_qscores.csv')\n",
    "\n",
    "## Restrict to questionnaires of interest.\n",
    "csv.columns = ['Subject_ID', 'Diagnosis','MASQ_AA', 'SHAPS_Cont', 'AAQ']\n",
    "csv = csv.set_index('Subject_ID', drop=True)\n",
    "\n",
    "## Restrict to psychiatric subjects. \n",
    "csv = csv[csv['Diagnosis'] == 1]\n",
    "\n",
    "## Remove subjects with corrupted data.\n",
    "csv = csv[csv.index != 'CU0068CUMR1R1'] ## need to re-proproc\n",
    "\n",
    "## Remove subjects with missing data.\n",
    "csv = csv.dropna()\n",
    "subjects = list(csv.index)\n",
    "\n",
    "## Create questionnaire matrix. \n",
    "qmat = np.vstack([csv['MASQ_AA'], csv['SHAPS_Cont'], csv['AAQ']])\n",
    "\n",
    "## Normalize questionnaires.\n",
    "for i in np.arange(qmat.shape[0]):\n",
    "    qmat[i] = normalize(qmat[i])\n",
    "    \n",
    "## Add intercept column. \n",
    "intercept = np.array([1]*len(subjects))\n",
    "qmat = np.vstack([intercept,qmat])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "930px",
   "left": "0px",
   "right": "1708px",
   "top": "106px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
