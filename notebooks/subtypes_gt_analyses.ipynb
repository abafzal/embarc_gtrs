{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Preprocess.\" data-toc-modified-id=\"Preprocess.-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preprocess.</a></div><div class=\"lev2 toc-item\"><a href=\"#Load-Labels.\" data-toc-modified-id=\"Load-Labels.-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Load Labels.</a></div><div class=\"lev2 toc-item\"><a href=\"#Load-Data.-Average-by-Labels.-(DONE)\" data-toc-modified-id=\"Load-Data.-Average-by-Labels.-(DONE)-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Load Data. Average by Labels. (DONE)</a></div><div class=\"lev2 toc-item\"><a href=\"#Plot-average-signal-by-label.\" data-toc-modified-id=\"Plot-average-signal-by-label.-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Plot average signal by label.</a></div><div class=\"lev3 toc-item\"><a href=\"#Save-out-ratio-signal.-(DONE)\" data-toc-modified-id=\"Save-out-ratio-signal.-(DONE)-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Save out ratio signal. (DONE)</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-ratio-signal.\" data-toc-modified-id=\"Plot-ratio-signal.-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Plot ratio signal.</a></div><div class=\"lev2 toc-item\"><a href=\"#Filter-and-Nuisance-Regress.\" data-toc-modified-id=\"Filter-and-Nuisance-Regress.-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Filter and Nuisance Regress.</a></div><div class=\"lev1 toc-item\"><a href=\"#Calculate-SNR.\" data-toc-modified-id=\"Calculate-SNR.-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Calculate SNR.</a></div><div class=\"lev1 toc-item\"><a href=\"#Calculate-and-plot-motion.\" data-toc-modified-id=\"Calculate-and-plot-motion.-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Calculate and plot motion.</a></div><div class=\"lev2 toc-item\"><a href=\"#Calculate-functional-displacement-and-other-motion-parameters.\" data-toc-modified-id=\"Calculate-functional-displacement-and-other-motion-parameters.-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Calculate functional displacement and other motion parameters.</a></div><div class=\"lev2 toc-item\"><a href=\"#Mask-grey-and-white-matter-timeseries.\" data-toc-modified-id=\"Mask-grey-and-white-matter-timeseries.-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Mask grey and white matter timeseries.</a></div><div class=\"lev2 toc-item\"><a href=\"#Prepare-motion-and-masked-fMRI-data.-Plot.\" data-toc-modified-id=\"Prepare-motion-and-masked-fMRI-data.-Plot.-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Prepare motion and masked fMRI data. Plot.</a></div><div class=\"lev1 toc-item\"><a href=\"#Correlate.\" data-toc-modified-id=\"Correlate.-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Correlate.</a></div><div class=\"lev2 toc-item\"><a href=\"#Calculate-Correlations-and-r-to-z-transform.\" data-toc-modified-id=\"Calculate-Correlations-and-r-to-z-transform.-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Calculate Correlations and r-to-z transform.</a></div><div class=\"lev2 toc-item\"><a href=\"#Calculate-ratio-of-negative-correlations.\" data-toc-modified-id=\"Calculate-ratio-of-negative-correlations.-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Calculate ratio of negative correlations.</a></div><div class=\"lev2 toc-item\"><a href=\"#Plot-correlations.\" data-toc-modified-id=\"Plot-correlations.-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Plot correlations.</a></div><div class=\"lev3 toc-item\"><a href=\"#Group-specific-r-to-z-distributions-and-heatmaps.\" data-toc-modified-id=\"Group-specific-r-to-z-distributions-and-heatmaps.-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Group specific r-to-z distributions and heatmaps.</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-Correlation-heatmap-with/without-GSR.\" data-toc-modified-id=\"Plot-Correlation-heatmap-with/without-GSR.-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Plot Correlation heatmap with/without GSR.</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-correlation-and-r-to-z-distribution-by-site.\" data-toc-modified-id=\"Plot-correlation-and-r-to-z-distribution-by-site.-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>Plot correlation and r-to-z distribution by site.</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-correlation-distribution-(GSR-vs.-noGSR).\" data-toc-modified-id=\"Plot-correlation-distribution-(GSR-vs.-noGSR).-4.3.4\"><span class=\"toc-item-num\">4.3.4&nbsp;&nbsp;</span>Plot correlation distribution (GSR vs. noGSR).</a></div><div class=\"lev1 toc-item\"><a href=\"#Network-Analysis\" data-toc-modified-id=\"Network-Analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Network Analysis</a></div><div class=\"lev2 toc-item\"><a href=\"#Compute-network-thresholds.\" data-toc-modified-id=\"Compute-network-thresholds.-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Compute network thresholds.</a></div><div class=\"lev2 toc-item\"><a href=\"#Compute-global-graph-metrics-by-density.\" data-toc-modified-id=\"Compute-global-graph-metrics-by-density.-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Compute global graph metrics by density.</a></div><div class=\"lev2 toc-item\"><a href=\"#Compute-degree-distribution.\" data-toc-modified-id=\"Compute-degree-distribution.-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Compute degree distribution.</a></div><div class=\"lev2 toc-item\"><a href=\"#Visualize-global-graph-metrics-by-density.\" data-toc-modified-id=\"Visualize-global-graph-metrics-by-density.-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Visualize global graph metrics by density.</a></div><div class=\"lev2 toc-item\"><a href=\"#Create-thresholded-networks.\" data-toc-modified-id=\"Create-thresholded-networks.-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Create thresholded networks.</a></div><div class=\"lev1 toc-item\"><a href=\"#Load-Questionnaires.\" data-toc-modified-id=\"Load-Questionnaires.-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Load Questionnaires.</a></div><div class=\"lev1 toc-item\"><a href=\"#Load-Demographics.\" data-toc-modified-id=\"Load-Demographics.-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Load Demographics.</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T14:25:59.217460",
     "start_time": "2017-03-01T14:25:56.990743"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from mne import read_label\n",
    "from pandas import read_csv\n",
    "from pandas import read_table\n",
    "\n",
    "## Specify directories and parameters.\n",
    "fast_dir = '/space/will/3/users/EMBARC/EMBARC-FAST'\n",
    "fsd = 'rest'\n",
    "run = '001'\n",
    "\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "### Load cortical labels.\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "\n",
    "parc_dir = '/space/lilli/1/users/DARPA-Recons/fscopy/label/yeo114_orig'\n",
    "lh_labels = [read_label(os.path.join(parc_dir, l)) for l in sorted(os.listdir(parc_dir)) if 'LH' in l]\n",
    "rh_labels = [read_label(os.path.join(parc_dir, l)) for l in sorted(os.listdir(parc_dir)) if 'RH' in l]\n",
    "\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "### Load subcortical labels.\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "\n",
    "## Read Freesurfer Color Lookup Table. \n",
    "lh_subcort = dict.fromkeys(['Left-Accumbens-area', 'Left-Thalamus-Proper', 'Left-Caudate', 'Left-Putamen', 'Left-Hippocampus', 'Left-Amygdala'])\n",
    "rh_subcort = dict.fromkeys(['Right-Accumbens-area', 'Right-Thalamus-Proper', 'Right-Caudate', 'Right-Putamen', 'Right-Hippocampus', 'Right-Amygdala'])\n",
    "\n",
    "clut = read_table(os.path.join('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/labels', 'FreeSurferColorLUT_truncated.txt'), \n",
    "                                              sep=' *', skiprows=4, names=['No', 'Label','R','G','B','A'], engine='python')\n",
    "clut = clut.loc[np.where(np.in1d(clut.Label,lh_subcort.keys() + rh_subcort.keys()))]\n",
    "\n",
    "## Load subcortical segmentation.\n",
    "aseg_obj = nib.load('/space/lilli/1/users/DARPA-Recons/fsaverage/mri.2mm/aseg.mgz')\n",
    "aseg_dat = aseg_obj.get_data()\n",
    "\n",
    "for l in lh_subcort.keys(): lh_subcort[l] = np.where(aseg_dat == int(clut.No[clut.Label == l]))\n",
    "for l in rh_subcort.keys(): rh_subcort[l] = np.where(aseg_dat == int(clut.No[clut.Label == l]))\n",
    "    \n",
    "print 'cortical lh: %d,' %len(lh_labels), 'rh: %d\\n' %len(rh_labels), 'subcortical lh: %d' %len(lh_subcort), 'subcortical rh: %d' %len(rh_subcort)\n",
    "\n",
    "## Save list of regions. \n",
    "regions = np.array([l.name for l in lh_labels] + lh_subcort.keys() + [l.name for l in rh_labels] + rh_subcort.keys())\n",
    "np.savetxt(os.path.join(fast_dir, 'scripts', 'subtypes_gt', 'gt_regions'), regions, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data. Average by Labels. (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T16:48:41.871137",
     "start_time": "2017-03-01T16:42:16.611747"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from pandas import DataFrame\n",
    "from scipy.stats import pearsonr\n",
    "from mne.filter import band_pass_filter\n",
    "\n",
    "## Define functions and directories.\n",
    "def zscore(arr): return (arr - arr.mean()) / arr.std()\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "## Specify regions. \n",
    "regions = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')]\n",
    "n_regions = len(regions)\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print i, subject, \n",
    "\n",
    "    ## Load cortical/subcortical data.\n",
    "    lh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'lh'))).get_data().squeeze()\n",
    "    rh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'rh'))).get_data().squeeze()\n",
    "    sc_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.%s.2mm.nii.gz' %('mni305')))).get_data()\n",
    "    \n",
    "    ## Drop first 4 volumes. \n",
    "    lh_dat = np.delete(lh_dat, np.arange(4), axis=1)\n",
    "    rh_dat = np.delete(rh_dat, np.arange(4), axis=1)\n",
    "    sc_dat = np.delete(sc_dat, np.arange(4), axis=3)\n",
    "\n",
    "    n_times = sc_dat.shape[3]\n",
    "    n_verts = lh_dat.shape[0]\n",
    "    \n",
    "    ## Remove any extra timepoints collected.\n",
    "    if n_times > 176:\n",
    "        print 'Removing last %d timepoints for %s' %(n_times-176,subject)\n",
    "        lh_dat = np.delete(lh_dat, np.arange(176,n_times), axis=1)\n",
    "        rh_dat = np.delete(rh_dat, np.arange(176,n_times), axis=1)\n",
    "        sc_dat = np.delete(sc_dat, np.arange(176,n_times), axis=3)\n",
    "        n_times = sc_dat.shape[3]\n",
    "            \n",
    "    elif n_times < 176: \n",
    "        print 'Mismatch in number of timepoints. %s, n_times: %d. Missing %d timepoints.' %(subject, n_times, 176-n_times)\n",
    "        break\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Average by labels.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "    mean_dat = np.zeros((n_regions,n_times))\n",
    "    \n",
    "    for idx in np.arange(len(lh_labels)):\n",
    "        mean_dat[idx] = lh_dat[lh_labels[idx].vertices].mean(axis=0)\n",
    "\n",
    "    for idx,k in zip(np.arange(len(lh_labels), len(lh_labels) + len(lh_subcort)),lh_subcort.keys()):\n",
    "        x,y,z = lh_subcort[k]\n",
    "        mean_dat[idx] = sc_dat[x,y,z].mean(axis=0)\n",
    "        \n",
    "    for rh_idx, idx in zip(np.arange(len(rh_labels)), np.arange(len(lh_labels) + len(lh_subcort), len(lh_labels) + len(lh_subcort) + len(rh_labels))):\n",
    "        mean_dat[idx] = rh_dat[rh_labels[rh_idx].vertices].mean(axis=0)\n",
    "\n",
    "    for idx,k in zip(np.arange(len(lh_labels) + len(lh_subcort) + len(rh_labels), n_regions),rh_subcort.keys()):\n",
    "        x,y,z = rh_subcort[k]\n",
    "        mean_dat[idx] = sc_dat[x,y,z].mean(axis=0)\n",
    "        \n",
    "    ## Save data averaged by labels. \n",
    "    np.savez_compressed(os.path.join(gt_dir, 'raw', '%s_raw' %subject), mean_dat=mean_dat)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T16:09:31.472985",
     "start_time": "2017-02-02T16:09:31.470878"
    }
   },
   "source": [
    "## Plot average signal by label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save out ratio signal. (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T13:41:56.527190",
     "start_time": "2017-02-21T13:41:44.469311"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "\n",
    "def get_signal_ratio(arr, idx): return np.count_nonzero(arr[idx]) / float(arr[idx].shape[0] * arr[idx].shape[1])\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "n_regions = len(labels) + len(subcort_labels)\n",
    "sig_ratio = np.zeros((len(subjects), n_regions))\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print i, subject, \n",
    "\n",
    "    ## Load cortical/subcortical data.\n",
    "    lh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'lh'))).get_data().squeeze()\n",
    "    rh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'rh'))).get_data().squeeze()\n",
    "    sc_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.%s.2mm.nii.gz' %('mni305')))).get_data()\n",
    "    \n",
    "    ## Drop first 4 volumes. \n",
    "    lh_dat = np.delete(lh_dat, np.arange(4), axis=1)\n",
    "    rh_dat = np.delete(rh_dat, np.arange(4), axis=1)\n",
    "    sc_dat = np.delete(sc_dat, np.arange(4), axis=3)\n",
    "\n",
    "    n_times = lh_dat.shape[1]\n",
    "    n_verts = lh_dat.shape[0]\n",
    "    \n",
    "    ## Remove any extra timepoints collected.\n",
    "    if n_times > 176:\n",
    "        print 'Removing last %d timepoints for %s' %(n_times-176,subject)\n",
    "        lh_dat = np.delete(lh_dat, np.arange(176,n_times), axis=1)\n",
    "        rh_dat = np.delete(rh_dat, np.arange(176,n_times), axis=1)\n",
    "        sc_dat = np.delete(sc_dat, np.arange(176,n_times), axis=3)\n",
    "        n_times = lh_dat.shape[1]\n",
    "            \n",
    "    elif n_times < 176: \n",
    "        print 'Mismatch in number of timepoints. %s, n_times: %d. Missing %d timepoints.' %(subject, n_times, 176-n_times)\n",
    "        break\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Calcualate label signal ratio.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "    \n",
    "    sub_ratio = np.zeros((n_regions))\n",
    "    \n",
    "    for idx, label in enumerate(lh_labels):\n",
    "        sub_ratio[idx] = get_signal_ratio(lh_dat, label.vertices)\n",
    "        \n",
    "    for idx, label in enumerate(rh_labels): \n",
    "        sub_ratio[idx+len(lh_labels)] = get_signal_ratio(rh_dat, label.vertices)\n",
    "            \n",
    "    for idx,k in zip(np.arange(len(labels), n_regions),subcort_labels.keys()):\n",
    "        x,y,z = subcort_labels[k]\n",
    "        sub_ratio[idx] = get_signal_ratio(sc_dat, [x,y,z])\n",
    "        \n",
    "    sig_ratio[i] = sub_ratio\n",
    "\n",
    "## Save. \n",
    "np.save(os.path.join(gt_dir, 'signal_ratio'), sig_ratio)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ratio signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T13:40:36.470080",
     "start_time": "2017-02-21T13:40:35.684000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pylab as plt \n",
    "\n",
    "rs = np.load(os.path.join(gt_dir, 'signal_ratio.npy')).T\n",
    "rs = rs * 100\n",
    "rs_mean = np.mean(rs, axis=1)\n",
    "\n",
    "label_names = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_cortical_labels\", 'r')])\n",
    "\n",
    "## Scatterplot for percent signal across subjects. \n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "y = (rs_mean)\n",
    "mask = np.where(y<99.9)[0]\n",
    "colors = np.where(y<99.9, '#d7191c', '#2b83ba')\n",
    "\n",
    "plt.scatter(np.arange(rs_mean.shape[0]),y,c=colors)\n",
    "\n",
    "for i,j,k in zip(label_names[mask], mask, y[mask]): \n",
    "    plt.annotate('%s: %.2f' %('_'.join(i.split('_')[2:4]),y[j]), (j,k))\n",
    "\n",
    "plt.title('Percent label signal', fontsize=18)\n",
    "plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/percent_signal_labels.png')\n",
    "plt.close()\n",
    "\n",
    "## Plot histogram for each label. \n",
    "for r in mask: \n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    \n",
    "    plt.hist(rs[r]*100, normed=True)\n",
    "    plt.title(label_names[r])\n",
    "    plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/%s.png' %label_names[r])\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and Nuisance Regress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T09:50:12.072879",
     "start_time": "2017-03-02T09:49:04.844716"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from pandas import DataFrame\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import detrend\n",
    "from sklearn.decomposition import PCA\n",
    "from mne.filter import band_pass_filter\n",
    "\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print i, subject\n",
    "    \n",
    "    ## Load data.\n",
    "    mean_dat = np.load(os.path.join(gt_dir, 'raw', '%s_raw.npz' %subject))['mean_dat']\n",
    "    \n",
    "    ## Remove first 4 timepoints from data.\n",
    "    mean_dat = np.delete(mean_dat, np.arange(4), axis=1)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Perform Nuissance Regression and Filtering.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    n_components = 5        # No. of components to use from PCA.\n",
    "\n",
    "    ## Load global signal regressors. \n",
    "    gs = os.path.join(fast_dir, subject,fsd, run, 'global.waveform.dat')\n",
    "    gs = read_table(gs, sep=' *', header=None, engine='python').as_matrix()\n",
    "    \n",
    "    ## Load wm and csf regressors. \n",
    "    wm = read_table(os.path.join(fast_dir, subject,fsd, run, 'wm.dat'), sep=' *', header=None, engine='python')\n",
    "    vcsf = read_table(os.path.join(fast_dir, subject,fsd, run, 'vcsf.dat'), sep=' *', header=None, engine='python')\n",
    "    \n",
    "    ## Keep only top 5 components.\n",
    "    n_components = 5 \n",
    "    wm = wm[wm.columns[:n_components]].as_matrix()\n",
    "    vcsf = vcsf[vcsf.columns[:n_components]].as_matrix()\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if gs.shape[0] > 176: gs = np.delete(gs, np.arange(176,gs.shape[0]), axis=0)\n",
    "    if wm.shape[0] > 176: wm = np.delete(wm, np.arange(176,wm.shape[0]), axis=0)\n",
    "    if vcsf.shape[0] > 176: vcsf = np.delete(vcsf, np.arange(176,vcsf.shape[0]), axis=0)\n",
    "    \n",
    "    ## Load motion regressors.\n",
    "    mc = os.path.join(fast_dir, subject, fsd, run, 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:7]\n",
    "    if mc.shape[0] > 176:\n",
    "        mc = np.delete(mc, np.arange(176,mc.shape[0]), axis=0)\n",
    "\n",
    "    ## Demean/detrend data.\n",
    "    mc = detrend(mc, axis=0, type='constant')\n",
    "    mc = detrend(mc, axis=0, type='linear') \n",
    "\n",
    "    ## Construct basis sets.\n",
    "    mc = np.concatenate([mc, np.roll(mc, -1, 0)], axis=-1)\n",
    "    mc = np.concatenate([mc, np.power(mc,2)], axis=-1)\n",
    "\n",
    "    ## Replace last timepoint with zeros \n",
    "    ## Otherwise you are putting motion from the first frame into the last of the run\n",
    "    mc[-1] = np.zeros((mc[-1].shape))\n",
    "\n",
    "    ## Apply PCA.  \n",
    "    pca = PCA(n_components=24)\n",
    "    mc = pca.fit_transform(mc)  \n",
    "\n",
    "    ## Concatenate nuisance regressors. \n",
    "    ns = np.hstack((mc,gs,wm,vcsf))\n",
    "    \n",
    "    ## Remove first 4 timepoints.\n",
    "    ns = np.delete(ns, np.arange(4), axis=0) ## remove first 4 timepoints\n",
    "    \n",
    "    ## Filter parameters.\n",
    "    tr = 3\n",
    "    Fs = 1. / tr \n",
    "    Fp1 = 0.01\n",
    "    Fp2 = 0.08\n",
    "\n",
    "    ## Filter data and regressors.\n",
    "    mean_dat = band_pass_filter(mean_dat.astype(np.float64), Fs, Fp1, Fp2, filter_length='120s', method='iir')\n",
    "    ns = band_pass_filter(ns.T, Fs, Fp1, Fp2, filter_length='120s', method='iir').T\n",
    "    \n",
    "    ## Regress. \n",
    "    betas,_,_,_ = np.linalg.lstsq(ns,mean_dat.T)\n",
    "    regressed = mean_dat - np.dot(ns, betas).T\n",
    "    \n",
    "    ## Save regressed data.\n",
    "    np.savez_compressed(os.path.join(gt_dir, 'regressed', '%s_regressed' %subject), regressed=regressed)\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate SNR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T12:42:53.738694",
     "start_time": "2017-02-02T12:09:17.543179"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "out_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/preproc'\n",
    "    \n",
    "snr_mat = np.zeros((len(subjects)))\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print subject\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load masks.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    mask_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001/masks' %subject\n",
    "    mri_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001' %subject\n",
    "\n",
    "    brainmask = os.path.join(mask_dir,'brain.nii.gz')\n",
    "    brainmask = np.where( nib.load(brainmask).get_data(), 1, 0 ) # Binarize the mask\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load functional data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#    \n",
    "    fmcpr = os.path.join(mri_dir,'fmcpr.nii.gz')\n",
    "    fmcpr = nib.load(fmcpr).get_data()\n",
    "    \n",
    "    fmcpr_mean = np.mean(fmcpr,axis=-1)\n",
    "    brain_mean = np.where(brainmask, fmcpr_mean, 0).mean()\n",
    "    noise_std = np.where(~brainmask, fmcpr_mean, 0).std()\n",
    "    \n",
    "    snr = brain_mean / noise_std\n",
    "    snr_mat[i] = snr\n",
    "    \n",
    "pd.DataFrame(snr_mat, columns=['SNR'], index=subjects).to_csv(os.path.join(out_dir, 'snr.csv'))\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate and plot motion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate functional displacement and other motion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T12:11:43.903704",
     "start_time": "2017-02-21T12:11:43.165691"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "def demean(arr): return arr - arr.mean()\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "fmcpr.mcdat file format: \n",
    "1. n      : time point\n",
    "2. roll   : rotation about the I-S axis (degrees CCW)\n",
    "3. pitch  : rotation about the R-L axis (degrees CCW)\n",
    "4. yaw    : rotation about the A-P axis (degrees CCW)\n",
    "5. dS     : displacement in the Superior direction (mm)\n",
    "6. dL     : displacement in the Left direction (mm)\n",
    "7. dP     : displacement in the Posterior direction (mm)\n",
    "8. rmsold : RMS difference between input frame and reference frame\n",
    "9. rmsnew : RMS difference between output frame and reference frame\n",
    "10. trans : translation (mm) = sqrt(dS^2 + dL^2 + dP^2)\n",
    "\n",
    "Translations (displacement) are in mm, rotations are in degrees.\n",
    "\n",
    "'''\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "threshold = 0.5\n",
    "fast_dir = '/space/will/3/users/EMBARC/EMBARC-FAST'\n",
    "fsd = 'rest'\n",
    "run = '001'\n",
    "\n",
    "out_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/preproc'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "## Subjects with manual coregistration.\n",
    "# subjects = ['CU0092CUMR1R1', 'CU0087CUMR1R1', 'CU0095CUMR1R1', 'CU0104CUMR1R1', 'CU0131CUMR1R1', 'CU0069CUMR1R1', \n",
    "#             'CU0025CUMR1R1', 'CU0094CUMR1R1', 'CU0067CUMR1R1', 'CU0106CUMR1R1',  'CU0105CUMR1R1', \n",
    "#             'CU0070CUMR1R1', 'CU0129CUMR1R1', 'CU0113CUMR1R1', 'CU0125CUMR1R1', 'CU0133CUMR1R1']\n",
    "\n",
    "motion = np.zeros((len(subjects),6))\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare motion data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Read motion data.\n",
    "    mc = os.path.join(fast_dir, subject, fsd, run, 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:]\n",
    "\n",
    "    ## Remove first four timepoints.\n",
    "    mc = np.delete(mc, np.arange(4),axis=0)\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if mc.shape[0] > 176:\n",
    "        mc = np.delete(mc, np.arange(176,mc.shape[0]), axis=0)\n",
    "\n",
    "    ## Invert angular displacement.\n",
    "    mc[:,:3] = np.deg2rad(mc[:,:3]) \n",
    "    mc[:,:3] *= 50\n",
    "    \n",
    "    abs_mean = mc[8].mean()\n",
    "    abs_max = mc[8].max()\n",
    "    rel_mean = abs(np.diff(mc[9])).mean()\n",
    "    rel_max = abs(np.diff(mc[9])).max()\n",
    "\n",
    "    ## Compute framewise displacement (See Power 2012, 2014).\n",
    "    fd = np.insert( np.abs( np.diff(mc[:,3:], axis=0) ).sum(axis=1), 0, 0 )\n",
    "    \n",
    "    fd_mean = fd.mean()\n",
    "    fd_max = fd.max()\n",
    "    \n",
    "    motion[i] = np.round(np.vstack([abs_mean, abs_max, rel_mean, rel_max, fd_mean, fd_max]).flatten(),2)\n",
    "        \n",
    "pd.DataFrame(motion, columns=['abs_mean', 'abs_max', 'rel_mean', 'rel_max', 'FD_mean', 'FD_max'], index=subjects).to_csv(os.path.join(out_dir, 'fd.csv'))\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask grey and white matter timeseries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T14:49:57.770579",
     "start_time": "2017-02-02T14:43:32.351780"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Please see /space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/wm_masks.csh for instructions on how to make wm masks.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pandas import read_csv\n",
    "from mne.filter import construct_iir_filter, filter_data\n",
    "def demean(arr): return arr - arr.mean()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "decim = 250\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "for subject in subjects[:3]:\n",
    "    \n",
    "    print subject\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and prepare masks.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    mask_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001/masks' %subject\n",
    "    mri_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001' %subject\n",
    "    out_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001/motion' %subject\n",
    "\n",
    "    brainmask = os.path.join(mask_dir,'brain.nii.gz')\n",
    "    brainmask = np.where( nib.load(brainmask).get_data(), 1, 0 ) # Binarize the mask\n",
    "\n",
    "    wm = os.path.join(mask_dir,'wm.mgz')\n",
    "    wm = np.where( nib.load(wm).get_data(), 1, 0 ) # Binarize the mask\n",
    "\n",
    "    gm = brainmask - wm\n",
    "\n",
    "    ## Reduce to indices of interest.\n",
    "    gm = np.vstack(np.where(gm))[:,::decim]\n",
    "    wm = np.vstack(np.where(wm))[:,::decim]\n",
    "    del brainmask\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and slice through EPI image.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load data.\n",
    "    obj = nib.load(os.path.join(mri_dir, 'fmcpr.nii.gz'))\n",
    "    _,_,_,n_acq = obj.shape\n",
    "\n",
    "    ## Preallocoate space for timeseries.\n",
    "    gmts = np.zeros((n_acq, gm.shape[-1]))\n",
    "    wmts = np.zeros((n_acq, wm.shape[-1]))\n",
    "\n",
    "    for n in range(n_acq):\n",
    "\n",
    "        ## Slice image.\n",
    "        acq = obj.dataobj[..., n]\n",
    "\n",
    "        ## Store grey matter.\n",
    "        gmts[n] += acq[gm[0],gm[1],gm[2]]\n",
    "\n",
    "        ## Store white matter.\n",
    "        wmts[n] += acq[wm[0],wm[1],wm[2]]\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Preprocessing data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ## Construct highpass filter.\n",
    "    tr = 3\n",
    "    sfreq = 1. / tr\n",
    "    high_pass = 0.01\n",
    "    iir_params = dict(order=2, ftype='butter', output='sos') # Following Power et al. (2014)\n",
    "    iir_params = construct_iir_filter(iir_params, high_pass, None, sfreq, 'highpass', return_copy=False)  \n",
    "\n",
    "    ## Filter data.\n",
    "    gmts = filter_data(gmts.T, sfreq, high_pass, None, method='iir', iir_params=iir_params, verbose=False)\n",
    "    wmts = filter_data(wmts.T, sfreq, high_pass, None, method='iir', iir_params=iir_params, verbose=False)\n",
    "\n",
    "    ## De-mean.\n",
    "    gmts = np.apply_along_axis(demean, 1, gmts)\n",
    "    wmts = np.apply_along_axis(demean, 1, wmts)\n",
    "\n",
    "    ## Re-organize (center outwards).\n",
    "    gmts = gmts[ np.argsort( np.power( np.apply_along_axis(demean, 1, gm), 2 ).sum(axis=0) ) ]\n",
    "    wmts = gmts[ np.argsort( np.power( np.apply_along_axis(demean, 1, wm), 2 ).sum(axis=0) ) ]\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Save data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    if not os.path.isdir(out_dir): os.makedirs(out_dir)\n",
    "    f = os.path.join(out_dir, '%s_rest_qc_data' %subject)\n",
    "    np.savez_compressed(f, gm=gmts, wm=wmts, iir_params=iir_params )\n",
    "    del gmts, wmts, obj\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare motion and masked fMRI data. Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T15:32:49.355229",
     "start_time": "2017-02-02T15:19:33.986174"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "from scipy.signal import detrend\n",
    "from sklearn.decomposition import PCA\n",
    "def demean(arr): return arr - arr.mean()\n",
    "def rms(arr): return np.sqrt( np.mean( np.power(arr, 2) ) )\n",
    "%matplotlib inline\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "threshold = 0.5\n",
    "fast_dir = '/space/will/3/users/EMBARC/EMBARC-FAST'\n",
    "fsd = 'rest'\n",
    "run = '001'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "## Subjects with manual coregistration.\n",
    "# subjects = ['CU0092CUMR1R1', 'CU0087CUMR1R1', 'CU0095CUMR1R1', 'CU0104CUMR1R1', 'CU0131CUMR1R1', 'CU0069CUMR1R1', \n",
    "#             'CU0025CUMR1R1', 'CU0094CUMR1R1', 'CU0067CUMR1R1', 'CU0106CUMR1R1',  'CU0105CUMR1R1', \n",
    "#             'CU0070CUMR1R1', 'CU0129CUMR1R1', 'CU0113CUMR1R1', 'CU0125CUMR1R1', 'CU0133CUMR1R1']\n",
    "\n",
    "for subject in subjects[18:]:\n",
    "    \n",
    "    print subject, \n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare motion data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Read motion data.\n",
    "    mc = os.path.join(fast_dir, subject, fsd, run, 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:7]\n",
    "\n",
    "    ## Remove first four timepoints.\n",
    "    mc = np.delete(mc, np.arange(4),axis=0)\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if mc.shape[0] > 176:\n",
    "        mc = np.delete(mc, np.arange(176,mc.shape[0]), axis=0)\n",
    "\n",
    "    ## Invert angular displacement.\n",
    "    copy = mc.copy()\n",
    "    mc[:,:3] = np.deg2rad(mc[:,:3]) \n",
    "    mc[:,:3] *= 50\n",
    "\n",
    "    ## Compute framewise displacement (See Power 2012, 2014).\n",
    "    fd = np.insert( np.abs( np.diff(mc[:,3:], axis=0) ).sum(axis=1), 0, 0)\n",
    "\n",
    "    ## Demean/detrend data.\n",
    "    mc = detrend(copy, axis=0, type='constant')\n",
    "    mc = detrend(mc, axis=0, type='linear')\n",
    "\n",
    "    ## Construct basis sets.\n",
    "    mcreg24 = mc.copy()\n",
    "    mcreg24 = np.concatenate([mcreg24, np.roll(mcreg24, -1, 0)], axis=-1)\n",
    "    mcreg24 = np.concatenate([mcreg24, np.power(mcreg24,2)], axis=-1)\n",
    "\n",
    "    ## Replace last timepoint with zeros.\n",
    "    ## Otherwise you are putting motion from the first frame into the last of the run.\n",
    "    mcreg24[-1] = np.zeros((mcreg24[-1].shape))\n",
    "    \n",
    "    ## Apply PCA.  \n",
    "    pca = PCA(n_components=24)\n",
    "    mcreg24 = pca.fit_transform(mcreg24)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare fMRI data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load gray/white matter timeseries.\n",
    "    npz = np.load(os.path.join(fast_dir, subject, fsd, run, 'motion/%s_rest_qc_data.npz' %subject))\n",
    "    \n",
    "    gm_pre = np.apply_along_axis(demean, 1, np.delete(npz['gm'],np.arange(4),axis=1))\n",
    "    wm_pre = np.apply_along_axis(demean, 1, np.delete(npz['wm'],np.arange(4),axis=1))\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if gm_pre.shape[1] > 176:\n",
    "        gm_pre = np.delete(gm_pre, np.arange(176,gm_pre.shape[1]), axis=1)\n",
    "        wm_pre = np.delete(wm_pre, np.arange(176,wm_pre.shape[1]), axis=1)\n",
    "\n",
    "    gm_beta, _, _, _ = np.linalg.lstsq(mcreg24, gm_pre.T)\n",
    "    wm_beta, _, _, _ = np.linalg.lstsq(mcreg24, wm_pre.T)\n",
    "    gm_post = gm_pre - np.dot(mcreg24, gm_beta).T\n",
    "    wm_post = wm_pre - np.dot(mcreg24, wm_beta).T\n",
    "\n",
    "    ## Compute DVARS.\n",
    "    gm_DVARS = np.zeros((2,fd.shape[0]))\n",
    "    wm_DVARS = np.zeros((2,fd.shape[0]))\n",
    "    for n, arr in enumerate([gm_pre,gm_post]): gm_DVARS[n,1:] += np.apply_along_axis(rms, 0, np.diff(arr, axis=1))\n",
    "    for n, arr in enumerate([wm_pre,wm_post]): wm_DVARS[n,1:] += np.apply_along_axis(rms, 0, np.diff(arr, axis=1))\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plotting\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    fig = plt.figure(figsize=(16,18))\n",
    "    nrow = 16\n",
    "\n",
    "    ## Plot framewise displacement.\n",
    "    ax = plt.subplot2grid((nrow,1),(0,0),rowspan=1)\n",
    "    ax.plot(fd, linewidth=1.5, color='k')\n",
    "    ax.hlines(0.5, 0, fd.shape[0], linestyle='--', color='k', alpha=0.5)\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([0.0,0.5,1.0])\n",
    "    ax.set_yticklabels([0.0,0.5,1.0], rotation=90)\n",
    "    ax.set_ylabel('FD (mm)', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.set_title(subject, fontsize=24)\n",
    "\n",
    "    ## Plot grey matter (pre-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(1,0),rowspan=4)\n",
    "    cbar = ax.imshow(gm_pre, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('Grey Matter\\nPre-Regression', fontsize=12)\n",
    "\n",
    "    # Colobar setup.\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width, box.height])\n",
    "    axColor = plt.axes([0.905, 0.238, 0.015, 0.660])\n",
    "    cbar = plt.colorbar(cbar, cax = axColor, orientation=\"vertical\")\n",
    "    cbar.set_ticks([-50,0,50])\n",
    "    axColor.set_ylabel('BOLD', fontsize=12, rotation=270)\n",
    "\n",
    "    ## Plot grey matter (post-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(5,0),rowspan=4)\n",
    "    cbar = ax.imshow(gm_post, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('Grey Matter\\nPost-Regression', fontsize=12)\n",
    "\n",
    "    ## Plot DVARS.\n",
    "    ax = plt.subplot2grid((nrow,1),(9,0),rowspan=1)\n",
    "    for arr, label in zip(gm_DVARS, ['Pre','Post']): ax.plot(arr, linewidth=1.5, alpha=0.8, label=label)\n",
    "    ax.legend(loc=2, fontsize=8, frameon=False, borderpad=0.0,  handlelength=1.4, handletextpad=0.2)\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_xlabel('Acquisitions', fontsize=12)\n",
    "    ax.set_ylim(20,100)\n",
    "    ax.set_yticks([20,60,100])\n",
    "    ax.set_yticklabels([20,60,100], rotation=90)\n",
    "    ax.set_ylabel('DVARS_gm', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    ## Plot white matter (pre-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(11,0),rowspan=2)\n",
    "    cbar = ax.imshow(wm_pre, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('White Matter\\nPre-Regression', fontsize=12)\n",
    "\n",
    "    ## Plot white matter (post-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(13,0),rowspan=2)\n",
    "    cbar = ax.imshow(wm_post, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('White Matter\\nPost-Regression', fontsize=12)\n",
    "\n",
    "    ## Plot DVARS.\n",
    "    ax = plt.subplot2grid((nrow,1),(15,0))\n",
    "    for arr, label in zip(wm_DVARS, ['Pre','Post']): ax.plot(arr, linewidth=1.5, alpha=0.8, label=label)\n",
    "    ax.legend(loc=2, fontsize=8, frameon=False, borderpad=0.0,  handlelength=1.4, handletextpad=0.2)\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_xlabel('Acquisitions', fontsize=12)\n",
    "    ax.set_ylim(20,100)\n",
    "    ax.set_yticks([20,60,100])\n",
    "    ax.set_yticklabels([20,60,100], rotation=90)\n",
    "    ax.set_ylabel('DVARS_wm', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    plt.subplots_adjust(left=0.05, right=0.90, top=0.95, bottom=0.05, hspace=0.05)\n",
    "#     plt.show()\n",
    "    plt.savefig(os.path.join(fast_dir,subject,fsd,run,'motion/%s_supp_motion.png' %subject), dpi=180)\n",
    "    plt.savefig(os.path.join(fast_dir, 'jan2017', 'motion_plots', '%s_supp_motion.png' %subject), dpi=180)\n",
    "    plt.close('all')\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T14:52:28.312842",
     "start_time": "2017-02-23T14:52:28.310448"
    }
   },
   "source": [
    "## Calculate Correlations and r-to-z transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T09:51:14.745052",
     "start_time": "2017-03-02T09:51:09.212971"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np  \n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = \"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt\"\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs \n",
    "\n",
    "## Specify labels.\n",
    "regions = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')])\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "        \n",
    "    ## Load regressed data. \n",
    "    regressed = np.load(os.path.join(gt_dir, 'regressed', '%s_regressed.npz' %subject))['regressed']\n",
    "    \n",
    "    ## Correlate. \n",
    "    corrmat = np.corrcoef(regressed)\n",
    "    \n",
    "    ## Remove negative correlations.\n",
    "    corrmat = np.where(corrmat<=0, 0, corrmat)\n",
    "\n",
    "    ## Restrict to lower triangular matrix. \n",
    "    corrmat = corrmat[np.tril_indices_from(corrmat,k=-1)]\n",
    "    \n",
    "    # Fisher's r-to-z transform.\n",
    "    zmat = np.arctanh(corrmat)\n",
    "    \n",
    "    ## Remove inf created by correlation values == 1.\n",
    "    zmat = np.where(zmat==np.inf,np.nan,zmat)\n",
    "    \n",
    "    ## Normalize correlations.\n",
    "#     zmat = (zmat - np.nanmin(zmat)) / (np.nanmax(zmat) - np.nanmin(zmat))\n",
    "    \n",
    "    ## Save correlation.\n",
    "    np.savez(os.path.join(gt_dir, 'correlations', '%s_corr' %subject), corrmat=corrmat)\n",
    "    np.savez(os.path.join(gt_dir, 'correlations', '%s_rtoz' %subject), zmat=zmat)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate ratio of negative correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T14:57:01.849124",
     "start_time": "2017-02-23T14:57:01.311477"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "pps_rtoz = np.zeros((len(pps),np.triu_indices(126,k=1)[0].shape[0]))\n",
    "hcs_rtoz = np.zeros((len(hcs),np.triu_indices(126,k=1)[0].shape[0]))\n",
    "\n",
    "for i, subject in enumerate(pps): \n",
    "    pps_rtoz[i] = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']\n",
    "    \n",
    "for i, subject in enumerate(hcs): \n",
    "    hcs_rtoz[i] = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']    \n",
    "\n",
    "## Concatenate all subjects' rtoz.\n",
    "rtoz_mat = np.concatenate([pps_rtoz,hcs_rtoz],axis=0)\n",
    "    \n",
    "ncorrs = 7875.0\n",
    "\n",
    "for subgroup,mat,name in zip([subjects,pps,hcs],[rtoz_mat,pps_rtoz,hcs_rtoz],['all','psychiatric','healthy']):\n",
    "    \n",
    "    ratio_neg = np.zeros((len(subgroup)))\n",
    "\n",
    "    for i in np.arange(ratio_neg.shape[0]):\n",
    "        ratio_neg[i] = np.where(mat[i]<0)[0].shape[0] / ncorrs\n",
    "\n",
    "    print('Negative correlations, (%s), mean pct: %.2f' %(name,np.mean(ratio_neg)*100))\n",
    "    print('Negative correlations, (%s), std: %.3f' %(name,np.std(ratio_neg)))\n",
    "    \n",
    "    print('Min: ', np.min(mat[np.where(mat<0)]))\n",
    "    print('Max: ', np.max(mat[np.where(mat<0)]))\n",
    "    \n",
    "    plt.hist(mat[np.where(mat<0)],normed=True,alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group specific r-to-z distributions and heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T09:57:47.528184",
     "start_time": "2017-03-02T09:57:35.895749"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "## Specify regions.\n",
    "regions = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')])\n",
    "\n",
    "## Initialize arrays. \n",
    "pps_rtoz = np.zeros((len(pps),np.triu_indices(126,k=1)[0].shape[0]))\n",
    "hcs_rtoz = np.zeros((len(hcs),np.triu_indices(126,k=1)[0].shape[0]))\n",
    "\n",
    "for i, subject in enumerate(pps): \n",
    "    pps_rtoz[i] = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']\n",
    "    \n",
    "for i, subject in enumerate(hcs): \n",
    "    hcs_rtoz[i] = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']    \n",
    "    \n",
    "## Concatenate all subjects' rtoz.\n",
    "rtoz_mat = np.concatenate([pps_rtoz,hcs_rtoz],axis=0)\n",
    "\n",
    "###~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot rtoz distribution by group.\n",
    "###~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "# sns.distplot(np.mean(rtoz_mat,axis=0),hist=False,hist_kws={'alpha':0.7},label='All subjects',norm_hist=True)\n",
    "sns.distplot(np.mean(pps_rtoz,axis=0),hist=False,hist_kws={'alpha':0.7},label='Psychiatric',norm_hist=True)\n",
    "sns.distplot(np.mean(hcs_rtoz,axis=0),hist=False,hist_kws={'alpha':0.5},label='Healthy',norm_hist=True)\n",
    "plt.legend(fontsize=14)\n",
    "plt.title('r-to-z distribution by group',fontsize=18)\n",
    "plt.xlabel('r-to-z',fontsize=14)\n",
    "plt.ylabel('frequency (normed)',fontsize=14)\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig('/autofs/space/will_003/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/correlations/distributions/rtoz_dist_by_group.png')\n",
    "plt.close()\n",
    "\n",
    "###~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot rtoz heatmap by group.\n",
    "###~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for subgroup,zvals,name in zip([subjects,pps,hcs],[rtoz_mat,pps_rtoz,hcs_rtoz],['all','psychiatric','healthy']):\n",
    "    \n",
    "    ## Convert to square lower-triangular matrix.\n",
    "    zmat = np.zeros((126,126))\n",
    "    zmat[np.tril_indices(126,k=-1)] = np.mean(zvals,axis=0)\n",
    "    zmat[np.triu_indices(126,k=0)] = np.nan\n",
    "\n",
    "    ## Remove negative correlations. \n",
    "    zmat = np.where(zmat<0,np.nan,zmat)\n",
    "   \n",
    "    ## Plotting.\n",
    "    fig = plt.figure(figsize=(30,30))\n",
    "    sns.heatmap(zmat, cmap=\"YlOrRd_r\", vmax=np.nanmax(zmat), vmin=np.nanmin(zmat), square=True, xticklabels=regions, yticklabels=regions, linewidth=0.1, \n",
    "                    cbar_kws={'shrink':0.8},label=name)\n",
    "\n",
    "    ## Draw lines. \n",
    "    xlines = xlines = [14,26,33,35,46,51,52,57,63,75,87,94,96,109,114,115,120]\n",
    "    ylines = [126-x for x in xlines]\n",
    "    plt.vlines(x=xlines, ymin=0, ymax=[126-x for x in xlines],linestyles='dotted')\n",
    "    plt.hlines(y=ylines, xmin=0, xmax=xlines, linestyles='dotted')\n",
    "    \n",
    "    plt.title('Average r-to-z for %s subjects' %name,fontsize=24)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/autofs/space/will_003/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/correlations/heatmaps/%s_average_rtoz.png' %name)\n",
    "    plt.close()\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Correlation heatmap with/without GSR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T14:08:10.554779",
     "start_time": "2017-02-21T14:08:10.387390"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "## Specify labels.\n",
    "regions = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')])\n",
    "\n",
    "lh = np.concatenate([regions[:57],regions[114:120]])\n",
    "rh = np.concatenate([regions[57:114],regions[120:]])\n",
    "\n",
    "regions = np.concatenate([lh,rh])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot correlations per subject (with/without GSR).\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for subject in subjects: \n",
    "    \n",
    "    print subject, \n",
    "    \n",
    "    zmat_gsr = np.load(os.path.join(gt_dir, 'correlations', '%s_correlations_unthresholded.npz' %subject))['corrmat']\n",
    "    zmat_nogsr = np.load(os.path.join(gt_dir, 'correlations', '%s_correlations_nogsr_unthresholded.npz' %subject))['corrmat']\n",
    "    \n",
    "    zmat_diff = zmat_gsr - zmat_nogsr\n",
    "    zmat = np.concatenate([np.concatenate([zmat_diff[:57],zmat_diff[114:120]]),np.concatenate([zmat_diff[57:114],zmat_diff[120:]])])\n",
    "    \n",
    "    ## Convert to DataFrame.\n",
    "    zpd = pd.DataFrame(zmat, columns=regions, index=regions)\n",
    "    \n",
    "    ## Generate a mask for the upper triangle. \n",
    "    mask = np.zeros_like(zpd, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plot heatmap.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    f, ax = plt.subplots(figsize=(25,20))\n",
    "    cmap=sns.color_palette(\"RdBu_r\")\n",
    "    sns.heatmap(zpd, mask=mask, cmap=cmap, square=True, xticklabels=True, yticklabels=True, linewidth=0.1, cbar_kws={'shrink':1.0}, ax=ax)\n",
    "    \n",
    "    ## Draw lines. \n",
    "    xlines = [14,26,33,35,46,52,57,63] + [63+x for x in [14,26,33,35,46,52,57]]\n",
    "    ylines = [126-x for x in xlines]\n",
    "    plt.vlines(x=xlines, ymin=0, ymax=[126-x for x in xlines],linestyles='dotted')\n",
    "    plt.hlines(y=ylines, xmin=0, xmax=xlines, linestyles='dotted')\n",
    "        \n",
    "    ## Flourishes.\n",
    "    ax.set_title('%s: Correlations GSR - noGSR, unthresholded' %subject, fontsize=30)\n",
    "    cax = plt.gcf().axes[-1]\n",
    "    cax.tick_params(labelsize=20,labeltop=True,labelbottom=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "#     plt.show()\n",
    "    plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/correlations/hcs/%s_corr_unthresh_GSRdiff.png' %subject)\n",
    "    plt.close()\n",
    "    \n",
    "print '\\nDone.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot correlation and r-to-z distribution by site. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T09:59:31.773242",
     "start_time": "2017-03-02T09:59:30.942954"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs \n",
    "\n",
    "## Specify regions.\n",
    "regions = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')])\n",
    "ncorrs = 7875\n",
    "\n",
    "## Initialize matrix to store correlations.\n",
    "corrmat_master = np.zeros((len(subjects),ncorrs))\n",
    "    \n",
    "## Load correlations. \n",
    "for i,subject in enumerate(subjects): \n",
    "\n",
    "    corrmat_master[i] = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']\n",
    "\n",
    "## Remove negative correlations. \n",
    "corrmat_master = np.where(corrmat_master<0, np.nan, corrmat_master)\n",
    "\n",
    "## Remove perfect correlations (r==1). \n",
    "# corrmat_master = np.where(corrmat_master==1, np.nan, corrmat_master)\n",
    "\n",
    "## Plotting.\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.set(color_codes=True)\n",
    "sns.set_palette(sns.color_palette(\"deep\"))\n",
    "\n",
    "for site in ['CU','MG','TX','UM']:\n",
    "    \n",
    "    site_corrmat = corrmat_master[np.nonzero([1 if s.startswith(site) else 0 for s in subjects])]\n",
    "    \n",
    "    ## Plotting.\n",
    "    sns.distplot(site_corrmat.flatten(),hist=False, kde=True, label=site, hist_kws={'alpha':0.7})\n",
    "\n",
    "## Flourishes.\n",
    "plt.legend(fontsize=14)\n",
    "plt.title('Distribution of r-to-z', fontsize=16)\n",
    "\n",
    "## Save.\n",
    "plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/correlations/distributions/rtoz_by_site.png')\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot correlation distribution (GSR vs. noGSR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "## Specify regions.\n",
    "regions = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot correlations (with/without GSR).\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for subject in subjects: \n",
    "    \n",
    "    ## Load data.\n",
    "    zmat_gsr = np.load(os.path.join(gt_dir, 'correlations', '%s_correlations_unthresholded.npz' %subject))['corrmat']\n",
    "    zmat_nogsr = np.load(os.path.join(gt_dir, 'correlations', '%s_correlations_nogsr_unthresholded.npz' %subject))['corrmat']\n",
    "    \n",
    "    ## Convert to DataFrame.\n",
    "    zpd_gsr = pd.DataFrame(zmat_gsr, columns=regions, index=regions)\n",
    "    \n",
    "    ## Plotting.\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    sns.set(color_codes=True)\n",
    "    sns.set_palette(sns.color_palette(\"GnBu_d\"))\n",
    "    sns.distplot(zmat_gsr.flatten(),hist=False, kde=True, label='GSR', hist_kws={'alpha':0.7})\n",
    "    sns.distplot(zmat_nogsr.flatten(), hist=False, kde=True, label='No GSR', hist_kws={'alpha':0.7})\n",
    "    \n",
    "    ## Flourishes.\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.title('%s \\nDistribution of Correlations (unthresholded)' %subject, fontsize=16)\n",
    "    \n",
    "    ## Save.\n",
    "    plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/correlations/histograms/%s_corr_GSRhist_unthresh.png' %subject)\n",
    "    plt.close()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute network thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T10:31:10.344462",
     "start_time": "2017-03-02T10:05:53.937226"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import networkx as nx\n",
    "import scipy.io as sio\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "%matplotlib inline\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Construct network with GSR.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Specify directories.\n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Load regions.\n",
    "regions = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')]\n",
    "n_regions = len(regions)\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "n_subs = len(subjects)\n",
    "\n",
    "## Specify parameters. \n",
    "thresholds = np.arange(0,1.75,0.01)  ## len(thresholds) = 175\n",
    "metrics = ['threshold', 'n_nodes', 'n_edges', 'avg_degree', 'n_components', 'cc_node_ratio', 'cc_edge_ratio', 'density']\n",
    "metrics_dat = np.zeros((len(subjects),thresholds.shape[0],len(metrics)))\n",
    "\n",
    "## Iterate over subjects.\n",
    "for s,subject in enumerate(subjects):\n",
    "                \n",
    "    ## Load data. \n",
    "    rtoz = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']\n",
    "    \n",
    "    ## Iterate over thresholds. \n",
    "    for t,thresh in enumerate(thresholds): \n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Create graph.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(regions)\n",
    "\n",
    "        for q,r,val in zip(np.tril_indices(len(regions),k=-1)[0], np.tril_indices(len(regions),k=-1)[1], rtoz):\n",
    "            if not np.isnan(val) and val >= thresh:\n",
    "                G.add_edge(regions[q],regions[r])\n",
    "\n",
    "        G.remove_nodes_from(G.nodes_with_selfloops())\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute graph metrics.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        n_nodes = nx.number_of_nodes(G)\n",
    "        n_edges = nx.number_of_edges(G)\n",
    "\n",
    "        max_edges = (n_nodes*(n_nodes-1))/2\n",
    "        avg_degree = sum(nx.degree(G).values()) / n_nodes  \n",
    "\n",
    "        n_components = nx.number_connected_components(G)\n",
    "        largest_cc = max(nx.connected_component_subgraphs(G), key=len)\n",
    "        cc_nodes = nx.number_of_nodes(max(nx.connected_component_subgraphs(G), key=len))\n",
    "        cc_edges = nx.number_of_edges(max(nx.connected_component_subgraphs(G), key=len))\n",
    "\n",
    "        cc_node_ratio = float(cc_nodes) / n_nodes\n",
    "        \n",
    "        if n_edges > 0: \n",
    "            cc_edge_ratio = float(cc_edges) / n_edges\n",
    "        else: \n",
    "            cc_edge_ratio = 0\n",
    "\n",
    "        density = np.round(nx.density(G),3)\n",
    "\n",
    "        ## Store metrics. \n",
    "        metrics_dat[s,t] = np.array([thresh, n_nodes, n_edges, avg_degree, n_components, cc_node_ratio, cc_edge_ratio, density])\n",
    "\n",
    "## Flatten. \n",
    "metrics_dat = metrics_dat.reshape((len(subjects)*len(thresholds),len(metrics)),order='C')\n",
    "\n",
    "## Convert to csv. Save.\n",
    "subject_ids = np.expand_dims(np.repeat(np.array(subjects),len(thresholds)),axis=1)\n",
    "metrics_pd = pd.DataFrame(np.concatenate([subject_ids, metrics_dat],axis=1),columns=['Subject_ID'] + metrics)\n",
    "metrics_pd.to_csv(os.path.join(gt_dir, 'graph_metrics', 'metrics_by_rtoz_threshold.csv'))\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-28T13:40:34.499555",
     "start_time": "2017-02-28T13:40:34.496291"
    }
   },
   "source": [
    "## Compute global graph metrics by density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T12:53:27.965113",
     "start_time": "2017-03-02T11:19:35.149969"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy.io as sio\n",
    "from itertools import permutations\n",
    "\n",
    "## Define functions. \n",
    "def efficiency(G, u, v):\n",
    "    return 1. / nx.shortest_path_length(G, u, v)\n",
    "\n",
    "def global_efficiency(G):\n",
    "    n = len(G)\n",
    "    denom = n * (n - 1)\n",
    "    return sum(efficiency(G, u, v) for u, v in permutations(G, 2)) / denom\n",
    "\n",
    "def local_efficiency(G):\n",
    "    return sum(global_efficiency(nx.ego_graph(G, v)) for v in G) / len(G)\n",
    "\n",
    "## Specify directories.\n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Construct network.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Specify directories.\n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Load regions.\n",
    "regions = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')]\n",
    "n_regions = len(regions)\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "n_subs = len(subjects)\n",
    "\n",
    "## Specify parameters. \n",
    "density_range = [0.3,0.1]\n",
    "thresholds = np.arange(0.10,2.50,0.05)  ## len(thresholds) = 48\n",
    "metrics = ['threshold', 'n_nodes', 'n_edges', 'avg_degree', 'n_components', 'cc_node_ratio', 'cc_edge_ratio', 'density', \n",
    "           'clustering_coefficient', 'avg_path_length', 'glob_efficiency', 'loc_efficiency']\n",
    "\n",
    "## Load graph metrics.\n",
    "metrics_pd = pd.read_csv(os.path.join(gt_dir, 'graph_metrics', 'metrics_by_rtoz_threshold.csv'), \n",
    "                         usecols=['Subject_ID', 'threshold', 'n_nodes', 'n_edges', 'avg_degree', 'n_components', 'cc_node_ratio', 'cc_edge_ratio', 'density'])\n",
    "metrics_pd['cohort'] = [1 if subject in pps else 2 for subject in metrics_pd.Subject_ID]\n",
    "\n",
    "## Reset index. \n",
    "metrics_pd = metrics_pd.set_index('Subject_ID', drop=True)\n",
    "\n",
    "## Threshold by average degree. \n",
    "metrics_pd = metrics_pd.where(metrics_pd.avg_degree > (2 * np.log(metrics_pd.n_nodes)))\n",
    "\n",
    "## Threshold by ratio of nodes and edges in largest connected component. \n",
    "metrics_pd = metrics_pd.where(metrics_pd.cc_node_ratio > 0.9)\n",
    "metrics_pd = metrics_pd.where(metrics_pd.cc_node_ratio > 0.9)\n",
    "\n",
    "## Threshold by density.\n",
    "metrics_pd.density = np.round(metrics_pd.density,2)\n",
    "metrics_pd = metrics_pd.where(((metrics_pd.density<=density_range[0])&(metrics_pd.density>=density_range[1])))\n",
    "\n",
    "## Remove NaNs. \n",
    "metrics_pd = metrics_pd.dropna()\n",
    "\n",
    "## Matrix to store data. \n",
    "metrics_dat = np.zeros((metrics_pd.index.shape[0],len(metrics)))\n",
    "\n",
    "## Iterate over subjects.\n",
    "for idx,subject,thresh in zip(np.arange(metrics_dat.shape[0]),metrics_pd.index,metrics_pd.threshold):\n",
    "                \n",
    "    ## Load data. \n",
    "    rtoz = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Create graph.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(regions)\n",
    "\n",
    "    for q,r,val in zip(np.tril_indices(126,k=-1)[0], np.tril_indices(126,k=-1)[1], rtoz):\n",
    "        if not np.isnan(val) and val >= thresh:\n",
    "            G.add_edge(regions[q],regions[r])\n",
    "\n",
    "    G.remove_nodes_from(G.nodes_with_selfloops())\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute graph metrics.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    n_nodes = nx.number_of_nodes(G)\n",
    "    n_edges = nx.number_of_edges(G)\n",
    "\n",
    "    max_edges = (n_nodes*(n_nodes-1))/2\n",
    "    avg_degree = sum(nx.degree(G).values()) / n_nodes  \n",
    "\n",
    "    n_components = nx.number_connected_components(G)\n",
    "    largest_cc = max(nx.connected_component_subgraphs(G), key=len)\n",
    "    cc_nodes = nx.number_of_nodes(max(nx.connected_component_subgraphs(G), key=len))\n",
    "    cc_edges = nx.number_of_edges(max(nx.connected_component_subgraphs(G), key=len))\n",
    "\n",
    "    cc_node_ratio = float(cc_nodes) / n_nodes\n",
    "    cc_edge_ratio = float(cc_edges) / n_edges\n",
    "\n",
    "    density = np.round(nx.density(G),3)\n",
    "\n",
    "    ## Global metrics.\n",
    "    clustering_coeff = np.round(nx.average_clustering(G),3)\n",
    "\n",
    "    if n_components >1: \n",
    "        avg_path_length = np.round(nx.average_shortest_path_length(largest_cc),3)\n",
    "        glob_efficiency = np.round(global_efficiency(largest_cc),3)\n",
    "        loc_efficiency = np.round(local_efficiency(largest_cc),3)\n",
    "    else: \n",
    "        avg_path_length = np.round(nx.average_shortest_path_length(G),3)\n",
    "        glob_efficiency = np.round(global_efficiency(G),3)\n",
    "        loc_efficiency = np.round(local_efficiency(G),3)\n",
    "\n",
    "    ## Store metrics. \n",
    "    metrics_dat[idx] = np.array([thresh, n_nodes, n_edges, avg_degree, n_components, cc_node_ratio, cc_edge_ratio, \n",
    "    density, clustering_coeff, avg_path_length, glob_efficiency, loc_efficiency])\n",
    "\n",
    "## Convert to csv. Save.\n",
    "metrics_updated = pd.DataFrame(np.concatenate([np.expand_dims(metrics_pd.index,1), metrics_dat],axis=1),columns=['Subject_ID'] + metrics)\n",
    "\n",
    "## Save. \n",
    "metrics_updated.to_csv(os.path.join(gt_dir, 'graph_metrics', 'metrics_by_density.csv'))\n",
    "\n",
    "print 'Done.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute degree distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T13:35:58.013100",
     "start_time": "2017-03-02T13:35:57.162435"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy.io as sio\n",
    "import pylab as plt\n",
    "from itertools import permutations\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "metrics = ['threshold', 'n_nodes', 'n_edges', 'avg_degree', 'n_components', 'cc_node_ratio', 'cc_edge_ratio', 'density', \n",
    "           'clustering_coefficient', 'avg_path_length', 'glob_efficiency', 'loc_efficiency']\n",
    "\n",
    "metrics_pd = pd.read_csv(os.path.join(gt_dir, 'graph_metrics', 'metrics_by_density.csv'), usecols=['Subject_ID'] + metrics)\n",
    "metrics_pd.density = np.round(metrics_pd.density,2)\n",
    "\n",
    "Gnets = []\n",
    "\n",
    "## Iterate over subjects.\n",
    "for subject,thresh in zip(metrics_density.Subject_ID,metrics_density.threshold):\n",
    "\n",
    "    ## Load data. \n",
    "    rtoz = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Create graph.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(regions)\n",
    "\n",
    "    for q,r,val in zip(np.tril_indices(126,k=-1)[0], np.tril_indices(126,k=-1)[1], rtoz):\n",
    "        if not np.isnan(val) and val >= thresh:\n",
    "            G.add_edge(regions[q],regions[r])\n",
    "\n",
    "    G.remove_nodes_from(G.nodes_with_selfloops())\n",
    "\n",
    "    Gnets.append(G)\n",
    "\n",
    "degree_dist = np.zeros((len(regions)))\n",
    "\n",
    "for i,g in enumerate(Gnets):\n",
    "    degrees,counts = np.unique(nx.degree(g).values(),return_counts=True)\n",
    "    degree_dist[degrees] += counts\n",
    "\n",
    "degree_dist = degree_dist / len(Gnets)\n",
    "\n",
    "## Plotting. \n",
    "fig = plt.figure(figsize=(16,6))\n",
    "\n",
    "ax1 = plt.subplot2grid((1,3),(0,0))\n",
    "ax1.bar(np.arange(len(regions)),degree_dist)\n",
    "ax1.set_xlabel('Degree')\n",
    "ax1.set_ylabel('Number of regions')\n",
    "\n",
    "ax2 = plt.subplot2grid((1,3),(0,1))\n",
    "ax2.scatter(np.log(np.arange(len(regions))),np.log(degree_dist))\n",
    "ax2.plot(np.log(np.arange(len(regions))),np.log(degree_dist))\n",
    "ax2.set_xlabel('log2(Degree)')\n",
    "ax2.set_ylabel('log2(Number of regions)')\n",
    "\n",
    "ax2 = plt.subplot2grid((1,3),(0,2))\n",
    "ax2.scatter(np.log10(np.arange(len(regions))),np.log10(degree_dist))\n",
    "ax2.plot(np.log10(np.arange(len(regions))),np.log10(degree_dist))\n",
    "ax2.set_xlabel('Rank (10^x)')\n",
    "ax2.set_ylabel('log10(Number of regions)')\n",
    "\n",
    "plt.suptitle('Degree Distribution',fontsize=18)\n",
    "\n",
    "#plt.show()\n",
    "plt.savefig('/autofs/space/will_003/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/graph_metrics/degree_dist.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize global graph metrics by density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T16:33:16.287718",
     "start_time": "2017-03-06T16:33:16.101004"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import pylab as plt\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "n_subs = len(subjects)\n",
    "\n",
    "## Specify parameters. \n",
    "qc_metrics = ['threshold', 'n_nodes', 'n_edges','n_components', 'cc_node_ratio', 'cc_edge_ratio']\n",
    "global_metrics = ['avg_degree','clustering_coefficient', 'avg_path_length', 'glob_efficiency', 'loc_efficiency']\n",
    "\n",
    "metrics_pd = pd.read_csv(os.path.join(gt_dir, 'graph_metrics', 'metrics_by_density.csv'), usecols=['Subject_ID'] + qc_metrics + global_metrics + ['density'])\n",
    "metrics_pd.density = np.round(metrics_pd.density,2)\n",
    "\n",
    "density_gb = metrics_pd.groupby('density')\n",
    "\n",
    "## Separate by cohort.\n",
    "metrics_pd['cohort'] = [1 if subject in pps else 2 for subject in metrics_pd.Subject_ID]\n",
    "\n",
    "density_pp = (metrics_pd.where(metrics_pd.cohort == 1)).groupby('density')\n",
    "density_hc = (metrics_pd.where(metrics_pd.cohort == 2)).groupby('density')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T16:42:43.089033",
     "start_time": "2017-03-06T16:42:42.431602"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import statsmodels.api as sm \n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "## Specify directories. \n",
    "behav_dir = '/autofs/space/will_003/users/EMBARC/behavior/assessments_raw'\n",
    "\n",
    "## Load data. \n",
    "dem = pd.read_csv(os.path.join(behav_dir, 'Demographics.csv'), usecols=['sample','ProjectSpecificID','SiteName','sex','hispa','race','age_evaluation'])\n",
    "dem = dem.sort_values(by='ProjectSpecificID')\n",
    "\n",
    "## Restrict to subjects of interest. \n",
    "dem = dem.where(dem.ProjectSpecificID[np.in1d(dem.ProjectSpecificID, [s[:6] for s in subjects])])\n",
    "dem = dem.dropna()\n",
    "\n",
    "dem['site0'] = np.where(dem.SiteName == 'Columbia',1,0)\n",
    "dem['site1'] = np.where(dem.SiteName == 'Harvard',1,0)\n",
    "dem['site2'] = np.where(dem.SiteName == 'Michigan',1,0)\n",
    "dem['site3'] = np.where(dem.SiteName == 'UTSW',1,0)\n",
    "\n",
    "dem = dem.set_index(np.unique(metrics_pd.Subject_ID))\n",
    "metrics_pd = metrics_pd.set_index(metrics_pd.Subject_ID)\n",
    "\n",
    "merged = pd.merge(metrics_pd,dem,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T16:54:31.690946",
     "start_time": "2017-03-06T16:54:30.186779"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Mixed Linear Model Regression Results\n",
      "=============================================================\n",
      "Model:            MixedLM Dependent Variable: glob_efficiency\n",
      "No. Observations: 5700    Method:             REML           \n",
      "No. Groups:       245     Scale:              0.0037         \n",
      "Min. group size:  8       Likelihood:         7792.3982      \n",
      "Max. group size:  40      Converged:          Yes            \n",
      "Mean group size:  23.3                                       \n",
      "--------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z     P>|z|  [0.025  0.975]\n",
      "--------------------------------------------------------------\n",
      "Intercept      0.540     0.002  321.782  0.000   0.536   0.543\n",
      "sex           -0.006     0.002   -2.861  0.004  -0.010  -0.002\n",
      "Intercept RE   0.000     0.000                                \n",
      "=============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/lilli/1/users/aafzal/software/Anaconda2.7/lib/python2.7/site-packages/statsmodels/regression/mixed_linear_model.py:1717: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "md = smf.mixedlm(formula='glob_efficiency ~ sex', data=merged, groups=merged['Subject_ID'])\n",
    "mdf = md.fit()\n",
    "\n",
    "print(mdf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T16:58:18.748647",
     "start_time": "2017-03-06T16:57:52.275532"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Mixed Linear Model Regression Results\n",
      "====================================================================\n",
      "Model:               MixedLM   Dependent Variable:   glob_efficiency\n",
      "No. Observations:    5700      Method:               REML           \n",
      "No. Groups:          245       Scale:                0.0036         \n",
      "Min. group size:     8         Likelihood:           7567.8218      \n",
      "Max. group size:     40        Converged:            No             \n",
      "Mean group size:     23.3                                           \n",
      "--------------------------------------------------------------------\n",
      "               Coef.   Std.Err.    z    P>|z|    [0.025     0.975]  \n",
      "--------------------------------------------------------------------\n",
      "Intercept       0.429 112186.435  0.000 1.000 -219880.943 219881.802\n",
      "sex            -0.006      0.008 -0.700 0.484      -0.022      0.010\n",
      "age_evaluation  0.000      0.000  0.273 0.785      -0.000      0.001\n",
      "site0           0.106 112186.435  0.000 1.000 -219881.267 219881.478\n",
      "site1           0.111 112186.435  0.000 1.000 -219881.262 219881.484\n",
      "site2           0.109 112186.435  0.000 1.000 -219881.263 219881.482\n",
      "site3           0.104 112186.435  0.000 1.000 -219881.269 219881.476\n",
      "Intercept RE    0.004                                               \n",
      "====================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/lilli/1/users/aafzal/software/Anaconda2.7/lib/python2.7/site-packages/statsmodels/regression/mixed_linear_model.py:1705: ConvergenceWarning: Gradient optimization failed.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/space/lilli/1/users/aafzal/software/Anaconda2.7/lib/python2.7/site-packages/statsmodels/regression/mixed_linear_model.py:1717: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "md = smf.mixedlm(formula='glob_efficiency ~ sex + age_evaluation + site0 + site1 + site2 + site3', data=merged, groups=merged['Subject_ID'])\n",
    "mdf = md.fit()\n",
    "\n",
    "print(mdf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Compare global metrics across cohorts. \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "   \n",
    "for metrics, name in zip([qc_metrics, global_metrics],['QC','Global']): \n",
    "    \n",
    "    fig = plt.figure(figsize=(30,24))\n",
    "        \n",
    "    for i,m in enumerate(sorted(metrics)):\n",
    "\n",
    "        ax = plt.subplot2grid((2,3),(i/3,i%3), rowspan=1, colspan=1)\n",
    "\n",
    "        ax.scatter(density_pp.mean().index, density_pp.mean()[m])\n",
    "        ax.plot(density_pp.mean().index, density_pp.mean()[m])\n",
    "\n",
    "        ax.scatter(density_hc.mean().index, density_hc.mean()[m])\n",
    "        ax.plot(density_hc.mean().index, density_hc.mean()[m])\n",
    "        \n",
    "        tval,pval = ttest_ind((metrics_pd[m][metrics_pd.cohort == 1]),(metrics_pd[m][metrics_pd.cohort == 2]))\n",
    "        \n",
    "        ax.set_xlabel('density', fontsize=18)\n",
    "        ax.set_ylabel(m, fontsize=18)\n",
    "\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(16)  \n",
    "        for tick in ax.yaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(16)     \n",
    "        \n",
    "        if pval<0.05: \n",
    "            plt.legend(['Psychiatric***','Healthy'], fontsize=18)\n",
    "        else: \n",
    "            plt.legend(['Psychiatric','Healthy'], fontsize=18)\n",
    "        plt.title(m, fontsize=22)\n",
    "        \n",
    "\n",
    "    plt.suptitle('%s Graph Metrics' %name, fontsize=50)\n",
    "\n",
    "#     plt.show()\n",
    "    plt.savefig('/autofs/space/will_003/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/graph_metrics/compare_%s_by_density.png' %name)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T16:27:29.221405",
     "start_time": "2017-03-02T16:27:29.200438"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Compute small-world metrics. \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "gamma = np.zeros((np.unique(metrics_pd.density).shape[0]))\n",
    "_lambda = np.zeros((np.unique(metrics_pd.density).shape[0]))\n",
    "sigma = np.zeros((np.unique(metrics_pd.density).shape[0]))\n",
    "\n",
    "for (idx, pp), (_, hc) in zip(enumerate(density_pp),enumerate(density_hc)): \n",
    "    gamma[idx] = np.mean(pp[1]['clustering_coefficient']) / np.mean(hc[1]['clustering_coefficient'])\n",
    "    _lambda[idx] = np.mean(pp[1]['avg_path_length']) / np.mean(hc[1]['avg_path_length'])\n",
    "    sigma[idx] = gamma[idx] / _lambda[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T16:36:05.488385",
     "start_time": "2017-03-02T16:36:05.123896"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plotting. \n",
    "fig, axes = plt.subplots(3, sharex=True, sharey=True, figsize=(12,12))\n",
    "\n",
    "axes[0].scatter(np.unique(metrics_pd.density),gamma)\n",
    "axes[0].plot(np.unique(metrics_pd.density),gamma)\n",
    "axes[0].set_ylabel('gamma', fontsize=16)\n",
    "axes[0].set_title('Clustering Coefficient Ratio',fontsize=20)\n",
    "\n",
    "axes[1].scatter(np.unique(metrics_pd.density),_lambda)\n",
    "axes[1].plot(np.unique(metrics_pd.density),_lambda)\n",
    "axes[1].set_ylabel('lambda', fontsize=16)\n",
    "axes[1].set_title('Average Shorted Path Length Ratio',fontsize=20)\n",
    "\n",
    "axes[2].scatter(np.unique(metrics_pd.density),sigma)\n",
    "axes[2].plot(np.unique(metrics_pd.density),sigma)\n",
    "\n",
    "axes[2].axhline(y=1, xmin=0, xmax=1, linewidth=1, color = 'k', linestyle='--')\n",
    "plt.xlabel('Density', fontsize=16)\n",
    "axes[2].set_ylabel('sigma', fontsize=16)\n",
    "axes[2].set_title('Small-world Index (Psychiatric vs. Healthy)',fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/autofs/space/will_003/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/graph_metrics/small_world.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create thresholded networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-24T16:21:23.078362",
     "start_time": "2017-02-24T16:21:23.006827"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Compute node-specific metrics. \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "node_degrees[i] = np.array([deg[region] if region in deg.keys() else 0 for region in regions])\n",
    "degree_centrality[i] = np.array([deg_cent[region] if region in deg_cent.keys() else 0 for region in regions])\n",
    "betweenness_centrality[i] = np.array([bet_cent[region] if region in bet_cent.keys() else 0 for region in regions])\n",
    "eigenvector_centrality[i] = np.array([eig_cent[region] if region in eig_cent.keys() else 0 for region in regions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Questionnaires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    return [( float(i) - min(arr) )/( max(arr) - min(arr)) for i in arr] \n",
    "\n",
    "## Load questionnaires. \n",
    "csv = read_csv('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/questionnaires/subtypes_qscores.csv')\n",
    "\n",
    "## Restrict to questionnaires of interest.\n",
    "csv.columns = ['Subject_ID', 'Diagnosis','MASQ_AA', 'SHAPS_Cont', 'AAQ']\n",
    "csv = csv.set_index('Subject_ID', drop=True)\n",
    "\n",
    "## Restrict to psychiatric subjects. \n",
    "csv = csv[csv['Diagnosis'] == 1]\n",
    "\n",
    "## Remove subjects with corrupted data.\n",
    "csv = csv[csv.index != 'CU0068CUMR1R1'] ## need to re-proproc\n",
    "\n",
    "## Remove subjects with missing data.\n",
    "csv = csv.dropna()\n",
    "subjects = list(csv.index)\n",
    "\n",
    "## Create questionnaire matrix. \n",
    "qmat = np.vstack([csv['MASQ_AA'], csv['SHAPS_Cont'], csv['AAQ']])\n",
    "\n",
    "## Normalize questionnaires.\n",
    "for i in np.arange(qmat.shape[0]):\n",
    "    qmat[i] = normalize(qmat[i])\n",
    "    \n",
    "## Add intercept column. \n",
    "intercept = np.array([1]*len(subjects))\n",
    "qmat = np.vstack([intercept,qmat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T16:11:07.033964",
     "start_time": "2017-03-06T16:11:06.983502"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-0.18141732651583653, pvalue=0.85619125040904631)\n",
      "Power_divergenceResult(statistic=array([ 5.34385396]), pvalue=array([ 0.02079545]))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from scipy.stats import chi2 \n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "''' \n",
    "Sex: 0 male, 1 female\n",
    "\n",
    "Race: \n",
    "0, no, White_x000D_\n",
    "1, no, Black or African American_x000D_\n",
    "2, no, Asian_x000D_\n",
    "3, no, American Indian or Alaska Native_x000D_\n",
    "4, no, Native Hawaiian or Other Pacific Islander_x000D_\n",
    "5, no, Other_x000D_\n",
    "6, no, Unknown \n",
    "\n",
    "Sites: {'Columbia':1, 'Harvard':2, 'Michigan': 3, 'UTSW': 4}\n",
    "'''\n",
    "\n",
    "## Specify directories. \n",
    "behav_dir = '/autofs/space/will_003/users/EMBARC/behavior/assessments_raw'\n",
    "\n",
    "## Load data. \n",
    "dem = pd.read_csv(os.path.join(behav_dir, 'Demographics.csv'), usecols=['sample','ProjectSpecificID','SiteName','sex','hispa','race','age_evaluation'])\n",
    "dem = dem.sort_values(by='ProjectSpecificID')\n",
    "\n",
    "## Restrict to subjects of interest. \n",
    "dem = dem.where(dem.ProjectSpecificID[np.in1d(dem.ProjectSpecificID, [s[:6] for s in subjects])])\n",
    "dem = dem.dropna()\n",
    "\n",
    "## Separate by group. \n",
    "pps = (dem.where(dem['sample'] == 1)).dropna()\n",
    "hcs = (dem.where(dem['sample'] == 2)).dropna()\n",
    "\n",
    "## T-test to compare age across cohorts\n",
    "print ttest_ind(hcs.age_evaluation, pps.age_evaluation)\n",
    "\n",
    "## Chi-squared Goodness-of-fit test for sex across cohorts\n",
    "observed = pd.crosstab(index=pps.sex, columns=\"count\")\n",
    "expected = (pd.crosstab(index=hcs.sex, columns=\"count\") / len(hcs)) * len(pps)\n",
    "\n",
    "print chisquare(f_obs = observed, f_exp = expected)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "929px",
   "left": "0px",
   "right": "1708px",
   "top": "106px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
