{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Preprocess.\" data-toc-modified-id=\"Preprocess.-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preprocess.</a></div><div class=\"lev2 toc-item\"><a href=\"#Load-Labels.\" data-toc-modified-id=\"Load-Labels.-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Load Labels.</a></div><div class=\"lev2 toc-item\"><a href=\"#Load-Data.-Average-by-Labels.-(DONE)\" data-toc-modified-id=\"Load-Data.-Average-by-Labels.-(DONE)-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Load Data. Average by Labels. (DONE)</a></div><div class=\"lev2 toc-item\"><a href=\"#Plot-average-signal-by-label.\" data-toc-modified-id=\"Plot-average-signal-by-label.-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Plot average signal by label.</a></div><div class=\"lev3 toc-item\"><a href=\"#Save-out-ratio-signal.-(DONE)\" data-toc-modified-id=\"Save-out-ratio-signal.-(DONE)-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Save out ratio signal. (DONE)</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-ratio-signal.\" data-toc-modified-id=\"Plot-ratio-signal.-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Plot ratio signal.</a></div><div class=\"lev2 toc-item\"><a href=\"#Filter-and-Nuisance-Regress.\" data-toc-modified-id=\"Filter-and-Nuisance-Regress.-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Filter and Nuisance Regress.</a></div><div class=\"lev1 toc-item\"><a href=\"#Calculate-SNR.\" data-toc-modified-id=\"Calculate-SNR.-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Calculate SNR.</a></div><div class=\"lev1 toc-item\"><a href=\"#Calculate-and-plot-motion.\" data-toc-modified-id=\"Calculate-and-plot-motion.-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Calculate and plot motion.</a></div><div class=\"lev2 toc-item\"><a href=\"#Calculate-functional-displacement-and-other-motion-parameters.\" data-toc-modified-id=\"Calculate-functional-displacement-and-other-motion-parameters.-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Calculate functional displacement and other motion parameters.</a></div><div class=\"lev2 toc-item\"><a href=\"#Mask-grey-and-white-matter-timeseries.\" data-toc-modified-id=\"Mask-grey-and-white-matter-timeseries.-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Mask grey and white matter timeseries.</a></div><div class=\"lev2 toc-item\"><a href=\"#Prepare-motion-and-masked-fMRI-data.-Plot.\" data-toc-modified-id=\"Prepare-motion-and-masked-fMRI-data.-Plot.-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Prepare motion and masked fMRI data. Plot.</a></div><div class=\"lev1 toc-item\"><a href=\"#Correlate.\" data-toc-modified-id=\"Correlate.-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Correlate.</a></div><div class=\"lev2 toc-item\"><a href=\"#Calculate-Correlations-and-r-to-z-transform.\" data-toc-modified-id=\"Calculate-Correlations-and-r-to-z-transform.-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Calculate Correlations and r-to-z transform.</a></div><div class=\"lev2 toc-item\"><a href=\"#Calculate-ratio-of-negative-correlations.\" data-toc-modified-id=\"Calculate-ratio-of-negative-correlations.-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Calculate ratio of negative correlations.</a></div><div class=\"lev2 toc-item\"><a href=\"#Plot-correlations.\" data-toc-modified-id=\"Plot-correlations.-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Plot correlations.</a></div><div class=\"lev3 toc-item\"><a href=\"#Group-specific-r-to-z-distributions-and-heatmaps.\" data-toc-modified-id=\"Group-specific-r-to-z-distributions-and-heatmaps.-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Group specific r-to-z distributions and heatmaps.</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-Correlation-heatmap-with/without-GSR.\" data-toc-modified-id=\"Plot-Correlation-heatmap-with/without-GSR.-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Plot Correlation heatmap with/without GSR.</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-correlation-and-r-to-z-distribution-by-site.\" data-toc-modified-id=\"Plot-correlation-and-r-to-z-distribution-by-site.-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>Plot correlation and r-to-z distribution by site.</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-correlation-distribution-(GSR-vs.-noGSR).\" data-toc-modified-id=\"Plot-correlation-distribution-(GSR-vs.-noGSR).-4.3.4\"><span class=\"toc-item-num\">4.3.4&nbsp;&nbsp;</span>Plot correlation distribution (GSR vs. noGSR).</a></div><div class=\"lev1 toc-item\"><a href=\"#Network-Analysis\" data-toc-modified-id=\"Network-Analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Network Analysis</a></div><div class=\"lev2 toc-item\"><a href=\"#Compute-network-thresholds.\" data-toc-modified-id=\"Compute-network-thresholds.-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Compute network thresholds.</a></div><div class=\"lev2 toc-item\"><a href=\"#Compute-global-graph-metrics-by-density.\" data-toc-modified-id=\"Compute-global-graph-metrics-by-density.-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Compute global graph metrics by density.</a></div><div class=\"lev3 toc-item\"><a href=\"#Load-data.\" data-toc-modified-id=\"Load-data.-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Load data.</a></div><div class=\"lev3 toc-item\"><a href=\"#Compute-metrics-part-1.\" data-toc-modified-id=\"Compute-metrics-part-1.-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Compute metrics part 1.</a></div><div class=\"lev3 toc-item\"><a href=\"#Compute-modularity.\" data-toc-modified-id=\"Compute-modularity.-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Compute modularity.</a></div><div class=\"lev4 toc-item\"><a href=\"#Visualize-modules.\" data-toc-modified-id=\"Visualize-modules.-5.2.3.1\"><span class=\"toc-item-num\">5.2.3.1&nbsp;&nbsp;</span>Visualize modules.</a></div><div class=\"lev2 toc-item\"><a href=\"#Visualize-global-graph-metrics-by-density.\" data-toc-modified-id=\"Visualize-global-graph-metrics-by-density.-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Visualize global graph metrics by density.</a></div><div class=\"lev2 toc-item\"><a href=\"#Compute-degree-distribution.\" data-toc-modified-id=\"Compute-degree-distribution.-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Compute degree distribution.</a></div><div class=\"lev1 toc-item\"><a href=\"#Regress-against-site-and-age-variables.\" data-toc-modified-id=\"Regress-against-site-and-age-variables.-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Regress against site and age variables.</a></div><div class=\"lev2 toc-item\"><a href=\"#Load-Questionnaires.\" data-toc-modified-id=\"Load-Questionnaires.-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Load Questionnaires.</a></div><div class=\"lev2 toc-item\"><a href=\"#Correlate-global-metrics-with-questionnaires.\" data-toc-modified-id=\"Correlate-global-metrics-with-questionnaires.-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Correlate global metrics with questionnaires.</a></div><div class=\"lev1 toc-item\"><a href=\"#Compute-node-specific-metrics.\" data-toc-modified-id=\"Compute-node-specific-metrics.-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Compute node-specific metrics.</a></div><div class=\"lev2 toc-item\"><a href=\"#Compute-differences-between-node-specific-metrics.\" data-toc-modified-id=\"Compute-differences-between-node-specific-metrics.-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Compute differences between node-specific metrics.</a></div><div class=\"lev2 toc-item\"><a href=\"#Correlate-node-specific-metrics-with-questionnaires.\" data-toc-modified-id=\"Correlate-node-specific-metrics-with-questionnaires.-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Correlate node-specific metrics with questionnaires.</a></div><div class=\"lev2 toc-item\"><a href=\"#Visualize-node-specific-metrics.\" data-toc-modified-id=\"Visualize-node-specific-metrics.-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Visualize node-specific metrics.</a></div><div class=\"lev1 toc-item\"><a href=\"#Create-null-networks.\" data-toc-modified-id=\"Create-null-networks.-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Create null networks.</a></div><div class=\"lev2 toc-item\"><a href=\"#Calculate-small-world-index.-(deprecated)\" data-toc-modified-id=\"Calculate-small-world-index.-(deprecated)-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Calculate small-world index. (deprecated)</a></div><div class=\"lev1 toc-item\"><a href=\"#Load-Demographics.\" data-toc-modified-id=\"Load-Demographics.-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Load Demographics.</a></div><div class=\"lev1 toc-item\"><a href=\"#Load-Questionnaires.\" data-toc-modified-id=\"Load-Questionnaires.-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Load Questionnaires.</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T14:25:59.217460",
     "start_time": "2017-03-01T14:25:56.990743"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from mne import read_label\n",
    "from pandas import read_csv\n",
    "from pandas import read_table\n",
    "\n",
    "## Specify directories and parameters.\n",
    "fast_dir = '/space/will/3/users/EMBARC/EMBARC-FAST'\n",
    "fsd = 'rest'\n",
    "run = '001'\n",
    "\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "### Load cortical labels.\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "\n",
    "parc_dir = '/space/lilli/1/users/DARPA-Recons/fscopy/label/yeo114_orig'\n",
    "lh_labels = [read_label(os.path.join(parc_dir, l)) for l in sorted(os.listdir(parc_dir)) if 'LH' in l]\n",
    "rh_labels = [read_label(os.path.join(parc_dir, l)) for l in sorted(os.listdir(parc_dir)) if 'RH' in l]\n",
    "\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "### Load subcortical labels.\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "\n",
    "## Read Freesurfer Color Lookup Table. \n",
    "lh_subcort = dict.fromkeys(['Left-Accumbens-area', 'Left-Thalamus-Proper', 'Left-Caudate', 'Left-Putamen', 'Left-Hippocampus', 'Left-Amygdala'])\n",
    "rh_subcort = dict.fromkeys(['Right-Accumbens-area', 'Right-Thalamus-Proper', 'Right-Caudate', 'Right-Putamen', 'Right-Hippocampus', 'Right-Amygdala'])\n",
    "\n",
    "clut = read_table(os.path.join('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/labels', 'FreeSurferColorLUT_truncated.txt'), \n",
    "                                              sep=' *', skiprows=4, names=['No', 'Label','R','G','B','A'], engine='python')\n",
    "clut = clut.loc[np.where(np.in1d(clut.Label,lh_subcort.keys() + rh_subcort.keys()))]\n",
    "\n",
    "## Load subcortical segmentation.\n",
    "aseg_obj = nib.load('/space/lilli/1/users/DARPA-Recons/fsaverage/mri.2mm/aseg.mgz')\n",
    "aseg_dat = aseg_obj.get_data()\n",
    "\n",
    "for l in lh_subcort.keys(): lh_subcort[l] = np.where(aseg_dat == int(clut.No[clut.Label == l]))\n",
    "for l in rh_subcort.keys(): rh_subcort[l] = np.where(aseg_dat == int(clut.No[clut.Label == l]))\n",
    "    \n",
    "print 'cortical lh: %d,' %len(lh_labels), 'rh: %d\\n' %len(rh_labels), 'subcortical lh: %d' %len(lh_subcort), 'subcortical rh: %d' %len(rh_subcort)\n",
    "\n",
    "## Save list of regions. \n",
    "regions = np.array([l.name for l in lh_labels] + lh_subcort.keys() + [l.name for l in rh_labels] + rh_subcort.keys())\n",
    "np.savetxt(os.path.join(fast_dir, 'scripts', 'subtypes_gt', 'gt_regions'), regions, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data. Average by Labels. (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T16:48:41.871137",
     "start_time": "2017-03-01T16:42:16.611747"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from pandas import DataFrame\n",
    "from scipy.stats import pearsonr\n",
    "from mne.filter import band_pass_filter\n",
    "\n",
    "## Define functions and directories.\n",
    "def zscore(arr): return (arr - arr.mean()) / arr.std()\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "## Specify regions. \n",
    "regions = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')]\n",
    "n_regions = len(regions)\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print i, subject, \n",
    "\n",
    "    ## Load cortical/subcortical data.\n",
    "    lh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'lh'))).get_data().squeeze()\n",
    "    rh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'rh'))).get_data().squeeze()\n",
    "    sc_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.%s.2mm.nii.gz' %('mni305')))).get_data()\n",
    "    \n",
    "    ## Drop first 4 volumes. \n",
    "    lh_dat = np.delete(lh_dat, np.arange(4), axis=1)\n",
    "    rh_dat = np.delete(rh_dat, np.arange(4), axis=1)\n",
    "    sc_dat = np.delete(sc_dat, np.arange(4), axis=3)\n",
    "\n",
    "    n_times = sc_dat.shape[3]\n",
    "    n_verts = lh_dat.shape[0]\n",
    "    \n",
    "    ## Remove any extra timepoints collected.\n",
    "    if n_times > 176:\n",
    "        print 'Removing last %d timepoints for %s' %(n_times-176,subject)\n",
    "        lh_dat = np.delete(lh_dat, np.arange(176,n_times), axis=1)\n",
    "        rh_dat = np.delete(rh_dat, np.arange(176,n_times), axis=1)\n",
    "        sc_dat = np.delete(sc_dat, np.arange(176,n_times), axis=3)\n",
    "        n_times = sc_dat.shape[3]\n",
    "            \n",
    "    elif n_times < 176: \n",
    "        print 'Mismatch in number of timepoints. %s, n_times: %d. Missing %d timepoints.' %(subject, n_times, 176-n_times)\n",
    "        break\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Average by labels.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "    mean_dat = np.zeros((n_regions,n_times))\n",
    "    \n",
    "    for idx in np.arange(len(lh_labels)):\n",
    "        mean_dat[idx] = lh_dat[lh_labels[idx].vertices].mean(axis=0)\n",
    "\n",
    "    for idx,k in zip(np.arange(len(lh_labels), len(lh_labels) + len(lh_subcort)),lh_subcort.keys()):\n",
    "        x,y,z = lh_subcort[k]\n",
    "        mean_dat[idx] = sc_dat[x,y,z].mean(axis=0)\n",
    "        \n",
    "    for rh_idx, idx in zip(np.arange(len(rh_labels)), np.arange(len(lh_labels) + len(lh_subcort), len(lh_labels) + len(lh_subcort) + len(rh_labels))):\n",
    "        mean_dat[idx] = rh_dat[rh_labels[rh_idx].vertices].mean(axis=0)\n",
    "\n",
    "    for idx,k in zip(np.arange(len(lh_labels) + len(lh_subcort) + len(rh_labels), n_regions),rh_subcort.keys()):\n",
    "        x,y,z = rh_subcort[k]\n",
    "        mean_dat[idx] = sc_dat[x,y,z].mean(axis=0)\n",
    "        \n",
    "    ## Save data averaged by labels. \n",
    "    np.savez_compressed(os.path.join(gt_dir, 'raw', '%s_raw' %subject), mean_dat=mean_dat)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T16:09:31.472985",
     "start_time": "2017-02-02T16:09:31.470878"
    }
   },
   "source": [
    "## Plot average signal by label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save out ratio signal. (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T13:41:56.527190",
     "start_time": "2017-02-21T13:41:44.469311"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "\n",
    "def get_signal_ratio(arr, idx): return np.count_nonzero(arr[idx]) / float(arr[idx].shape[0] * arr[idx].shape[1])\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "n_regions = len(labels) + len(subcort_labels)\n",
    "sig_ratio = np.zeros((len(subjects), n_regions))\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print i, subject, \n",
    "\n",
    "    ## Load cortical/subcortical data.\n",
    "    lh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'lh'))).get_data().squeeze()\n",
    "    rh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'rh'))).get_data().squeeze()\n",
    "    sc_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.%s.2mm.nii.gz' %('mni305')))).get_data()\n",
    "    \n",
    "    ## Drop first 4 volumes. \n",
    "    lh_dat = np.delete(lh_dat, np.arange(4), axis=1)\n",
    "    rh_dat = np.delete(rh_dat, np.arange(4), axis=1)\n",
    "    sc_dat = np.delete(sc_dat, np.arange(4), axis=3)\n",
    "\n",
    "    n_times = lh_dat.shape[1]\n",
    "    n_verts = lh_dat.shape[0]\n",
    "    \n",
    "    ## Remove any extra timepoints collected.\n",
    "    if n_times > 176:\n",
    "        print 'Removing last %d timepoints for %s' %(n_times-176,subject)\n",
    "        lh_dat = np.delete(lh_dat, np.arange(176,n_times), axis=1)\n",
    "        rh_dat = np.delete(rh_dat, np.arange(176,n_times), axis=1)\n",
    "        sc_dat = np.delete(sc_dat, np.arange(176,n_times), axis=3)\n",
    "        n_times = lh_dat.shape[1]\n",
    "            \n",
    "    elif n_times < 176: \n",
    "        print 'Mismatch in number of timepoints. %s, n_times: %d. Missing %d timepoints.' %(subject, n_times, 176-n_times)\n",
    "        break\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Calcualate label signal ratio.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "    \n",
    "    sub_ratio = np.zeros((n_regions))\n",
    "    \n",
    "    for idx, label in enumerate(lh_labels):\n",
    "        sub_ratio[idx] = get_signal_ratio(lh_dat, label.vertices)\n",
    "        \n",
    "    for idx, label in enumerate(rh_labels): \n",
    "        sub_ratio[idx+len(lh_labels)] = get_signal_ratio(rh_dat, label.vertices)\n",
    "            \n",
    "    for idx,k in zip(np.arange(len(labels), n_regions),subcort_labels.keys()):\n",
    "        x,y,z = subcort_labels[k]\n",
    "        sub_ratio[idx] = get_signal_ratio(sc_dat, [x,y,z])\n",
    "        \n",
    "    sig_ratio[i] = sub_ratio\n",
    "\n",
    "## Save. \n",
    "np.save(os.path.join(gt_dir, 'signal_ratio'), sig_ratio)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ratio signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T13:40:36.470080",
     "start_time": "2017-02-21T13:40:35.684000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pylab as plt \n",
    "\n",
    "rs = np.load(os.path.join(gt_dir, 'signal_ratio.npy')).T\n",
    "rs = rs * 100\n",
    "rs_mean = np.mean(rs, axis=1)\n",
    "\n",
    "label_names = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_cortical_labels\", 'r')])\n",
    "\n",
    "## Scatterplot for percent signal across subjects. \n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "y = (rs_mean)\n",
    "mask = np.where(y<99.9)[0]\n",
    "colors = np.where(y<99.9, '#d7191c', '#2b83ba')\n",
    "\n",
    "plt.scatter(np.arange(rs_mean.shape[0]),y,c=colors)\n",
    "\n",
    "for i,j,k in zip(label_names[mask], mask, y[mask]): \n",
    "    plt.annotate('%s: %.2f' %('_'.join(i.split('_')[2:4]),y[j]), (j,k))\n",
    "\n",
    "plt.title('Percent label signal', fontsize=18)\n",
    "plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/percent_signal_labels.png')\n",
    "plt.close()\n",
    "\n",
    "## Plot histogram for each label. \n",
    "for r in mask: \n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    \n",
    "    plt.hist(rs[r]*100, normed=True)\n",
    "    plt.title(label_names[r])\n",
    "    plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/%s.png' %label_names[r])\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and Nuisance Regress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T09:50:12.072879",
     "start_time": "2017-03-02T09:49:04.844716"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from pandas import DataFrame\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import detrend\n",
    "from sklearn.decomposition import PCA\n",
    "from mne.filter import band_pass_filter\n",
    "\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print i, subject\n",
    "    \n",
    "    ## Load data.\n",
    "    mean_dat = np.load(os.path.join(gt_dir, 'raw', '%s_raw.npz' %subject))['mean_dat']\n",
    "    \n",
    "    ## Remove first 4 timepoints from data.\n",
    "    mean_dat = np.delete(mean_dat, np.arange(4), axis=1)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Perform Nuissance Regression and Filtering.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    n_components = 5        # No. of components to use from PCA.\n",
    "\n",
    "    ## Load global signal regressors. \n",
    "    gs = os.path.join(fast_dir, subject,fsd, run, 'global.waveform.dat')\n",
    "    gs = read_table(gs, sep=' *', header=None, engine='python').as_matrix()\n",
    "    \n",
    "    ## Load wm and csf regressors. \n",
    "    wm = read_table(os.path.join(fast_dir, subject,fsd, run, 'wm.dat'), sep=' *', header=None, engine='python')\n",
    "    vcsf = read_table(os.path.join(fast_dir, subject,fsd, run, 'vcsf.dat'), sep=' *', header=None, engine='python')\n",
    "    \n",
    "    ## Keep only top 5 components.\n",
    "    n_components = 5 \n",
    "    wm = wm[wm.columns[:n_components]].as_matrix()\n",
    "    vcsf = vcsf[vcsf.columns[:n_components]].as_matrix()\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if gs.shape[0] > 176: gs = np.delete(gs, np.arange(176,gs.shape[0]), axis=0)\n",
    "    if wm.shape[0] > 176: wm = np.delete(wm, np.arange(176,wm.shape[0]), axis=0)\n",
    "    if vcsf.shape[0] > 176: vcsf = np.delete(vcsf, np.arange(176,vcsf.shape[0]), axis=0)\n",
    "    \n",
    "    ## Load motion regressors.\n",
    "    mc = os.path.join(fast_dir, subject, fsd, run, 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:7]\n",
    "    if mc.shape[0] > 176:\n",
    "        mc = np.delete(mc, np.arange(176,mc.shape[0]), axis=0)\n",
    "\n",
    "    ## Demean/detrend data.\n",
    "    mc = detrend(mc, axis=0, type='constant')\n",
    "    mc = detrend(mc, axis=0, type='linear') \n",
    "\n",
    "    ## Construct basis sets.\n",
    "    mc = np.concatenate([mc, np.roll(mc, -1, 0)], axis=-1)\n",
    "    mc = np.concatenate([mc, np.power(mc,2)], axis=-1)\n",
    "\n",
    "    ## Replace last timepoint with zeros \n",
    "    ## Otherwise you are putting motion from the first frame into the last of the run\n",
    "    mc[-1] = np.zeros((mc[-1].shape))\n",
    "\n",
    "    ## Apply PCA.  \n",
    "    pca = PCA(n_components=24)\n",
    "    mc = pca.fit_transform(mc)  \n",
    "\n",
    "    ## Concatenate nuisance regressors. \n",
    "    ns = np.hstack((mc,gs,wm,vcsf))\n",
    "    \n",
    "    ## Remove first 4 timepoints.\n",
    "    ns = np.delete(ns, np.arange(4), axis=0) ## remove first 4 timepoints\n",
    "    \n",
    "    ## Filter parameters.\n",
    "    tr = 3\n",
    "    Fs = 1. / tr \n",
    "    Fp1 = 0.01\n",
    "    Fp2 = 0.08\n",
    "\n",
    "    ## Filter data and regressors.\n",
    "    mean_dat = band_pass_filter(mean_dat.astype(np.float64), Fs, Fp1, Fp2, filter_length='120s', method='iir')\n",
    "    ns = band_pass_filter(ns.T, Fs, Fp1, Fp2, filter_length='120s', method='iir').T\n",
    "    \n",
    "    ## Regress. \n",
    "    betas,_,_,_ = np.linalg.lstsq(ns,mean_dat.T)\n",
    "    regressed = mean_dat - np.dot(ns, betas).T\n",
    "    \n",
    "    ## Save regressed data.\n",
    "    np.savez_compressed(os.path.join(gt_dir, 'regressed', '%s_regressed' %subject), regressed=regressed)\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate SNR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T12:42:53.738694",
     "start_time": "2017-02-02T12:09:17.543179"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "out_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/preproc'\n",
    "    \n",
    "snr_mat = np.zeros((len(subjects)))\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print subject\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load masks.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    mask_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001/masks' %subject\n",
    "    mri_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001' %subject\n",
    "\n",
    "    brainmask = os.path.join(mask_dir,'brain.nii.gz')\n",
    "    brainmask = np.where( nib.load(brainmask).get_data(), 1, 0 ) # Binarize the mask\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load functional data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#    \n",
    "    fmcpr = os.path.join(mri_dir,'fmcpr.nii.gz')\n",
    "    fmcpr = nib.load(fmcpr).get_data()\n",
    "    \n",
    "    fmcpr_mean = np.mean(fmcpr,axis=-1)\n",
    "    brain_mean = np.where(brainmask, fmcpr_mean, 0).mean()\n",
    "    noise_std = np.where(~brainmask, fmcpr_mean, 0).std()\n",
    "    \n",
    "    snr = brain_mean / noise_std\n",
    "    snr_mat[i] = snr\n",
    "    \n",
    "pd.DataFrame(snr_mat, columns=['SNR'], index=subjects).to_csv(os.path.join(out_dir, 'snr.csv'))\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate and plot motion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate functional displacement and other motion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T12:11:43.903704",
     "start_time": "2017-02-21T12:11:43.165691"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "def demean(arr): return arr - arr.mean()\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "fmcpr.mcdat file format: \n",
    "1. n      : time point\n",
    "2. roll   : rotation about the I-S axis (degrees CCW)\n",
    "3. pitch  : rotation about the R-L axis (degrees CCW)\n",
    "4. yaw    : rotation about the A-P axis (degrees CCW)\n",
    "5. dS     : displacement in the Superior direction (mm)\n",
    "6. dL     : displacement in the Left direction (mm)\n",
    "7. dP     : displacement in the Posterior direction (mm)\n",
    "8. rmsold : RMS difference between input frame and reference frame\n",
    "9. rmsnew : RMS difference between output frame and reference frame\n",
    "10. trans : translation (mm) = sqrt(dS^2 + dL^2 + dP^2)\n",
    "\n",
    "Translations (displacement) are in mm, rotations are in degrees.\n",
    "\n",
    "'''\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "threshold = 0.5\n",
    "fast_dir = '/space/will/3/users/EMBARC/EMBARC-FAST'\n",
    "fsd = 'rest'\n",
    "run = '001'\n",
    "\n",
    "out_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/preproc'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "## Subjects with manual coregistration.\n",
    "# subjects = ['CU0092CUMR1R1', 'CU0087CUMR1R1', 'CU0095CUMR1R1', 'CU0104CUMR1R1', 'CU0131CUMR1R1', 'CU0069CUMR1R1', \n",
    "#             'CU0025CUMR1R1', 'CU0094CUMR1R1', 'CU0067CUMR1R1', 'CU0106CUMR1R1',  'CU0105CUMR1R1', \n",
    "#             'CU0070CUMR1R1', 'CU0129CUMR1R1', 'CU0113CUMR1R1', 'CU0125CUMR1R1', 'CU0133CUMR1R1']\n",
    "\n",
    "motion = np.zeros((len(subjects),6))\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare motion data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Read motion data.\n",
    "    mc = os.path.join(fast_dir, subject, fsd, run, 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:]\n",
    "\n",
    "    ## Remove first four timepoints.\n",
    "    mc = np.delete(mc, np.arange(4),axis=0)\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if mc.shape[0] > 176:\n",
    "        mc = np.delete(mc, np.arange(176,mc.shape[0]), axis=0)\n",
    "\n",
    "    ## Invert angular displacement.\n",
    "    mc[:,:3] = np.deg2rad(mc[:,:3]) \n",
    "    mc[:,:3] *= 50\n",
    "    \n",
    "    abs_mean = mc[8].mean()\n",
    "    abs_max = mc[8].max()\n",
    "    rel_mean = abs(np.diff(mc[9])).mean()\n",
    "    rel_max = abs(np.diff(mc[9])).max()\n",
    "\n",
    "    ## Compute framewise displacement (See Power 2012, 2014).\n",
    "    fd = np.insert( np.abs( np.diff(mc[:,3:], axis=0) ).sum(axis=1), 0, 0 )\n",
    "    \n",
    "    fd_mean = fd.mean()\n",
    "    fd_max = fd.max()\n",
    "    \n",
    "    motion[i] = np.round(np.vstack([abs_mean, abs_max, rel_mean, rel_max, fd_mean, fd_max]).flatten(),2)\n",
    "        \n",
    "pd.DataFrame(motion, columns=['abs_mean', 'abs_max', 'rel_mean', 'rel_max', 'FD_mean', 'FD_max'], index=subjects).to_csv(os.path.join(out_dir, 'fd.csv'))\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask grey and white matter timeseries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T14:49:57.770579",
     "start_time": "2017-02-02T14:43:32.351780"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Please see /space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/wm_masks.csh for instructions on how to make wm masks.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pandas import read_csv\n",
    "from mne.filter import construct_iir_filter, filter_data\n",
    "def demean(arr): return arr - arr.mean()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "decim = 250\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "for subject in subjects[:3]:\n",
    "    \n",
    "    print subject\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and prepare masks.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    mask_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001/masks' %subject\n",
    "    mri_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001' %subject\n",
    "    out_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001/motion' %subject\n",
    "\n",
    "    brainmask = os.path.join(mask_dir,'brain.nii.gz')\n",
    "    brainmask = np.where( nib.load(brainmask).get_data(), 1, 0 ) # Binarize the mask\n",
    "\n",
    "    wm = os.path.join(mask_dir,'wm.mgz')\n",
    "    wm = np.where( nib.load(wm).get_data(), 1, 0 ) # Binarize the mask\n",
    "\n",
    "    gm = brainmask - wm\n",
    "\n",
    "    ## Reduce to indices of interest.\n",
    "    gm = np.vstack(np.where(gm))[:,::decim]\n",
    "    wm = np.vstack(np.where(wm))[:,::decim]\n",
    "    del brainmask\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and slice through EPI image.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load data.\n",
    "    obj = nib.load(os.path.join(mri_dir, 'fmcpr.nii.gz'))\n",
    "    _,_,_,n_acq = obj.shape\n",
    "\n",
    "    ## Preallocoate space for timeseries.\n",
    "    gmts = np.zeros((n_acq, gm.shape[-1]))\n",
    "    wmts = np.zeros((n_acq, wm.shape[-1]))\n",
    "\n",
    "    for n in range(n_acq):\n",
    "\n",
    "        ## Slice image.\n",
    "        acq = obj.dataobj[..., n]\n",
    "\n",
    "        ## Store grey matter.\n",
    "        gmts[n] += acq[gm[0],gm[1],gm[2]]\n",
    "\n",
    "        ## Store white matter.\n",
    "        wmts[n] += acq[wm[0],wm[1],wm[2]]\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Preprocessing data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ## Construct highpass filter.\n",
    "    tr = 3\n",
    "    sfreq = 1. / tr\n",
    "    high_pass = 0.01\n",
    "    iir_params = dict(order=2, ftype='butter', output='sos') # Following Power et al. (2014)\n",
    "    iir_params = construct_iir_filter(iir_params, high_pass, None, sfreq, 'highpass', return_copy=False)  \n",
    "\n",
    "    ## Filter data.\n",
    "    gmts = filter_data(gmts.T, sfreq, high_pass, None, method='iir', iir_params=iir_params, verbose=False)\n",
    "    wmts = filter_data(wmts.T, sfreq, high_pass, None, method='iir', iir_params=iir_params, verbose=False)\n",
    "\n",
    "    ## De-mean.\n",
    "    gmts = np.apply_along_axis(demean, 1, gmts)\n",
    "    wmts = np.apply_along_axis(demean, 1, wmts)\n",
    "\n",
    "    ## Re-organize (center outwards).\n",
    "    gmts = gmts[ np.argsort( np.power( np.apply_along_axis(demean, 1, gm), 2 ).sum(axis=0) ) ]\n",
    "    wmts = gmts[ np.argsort( np.power( np.apply_along_axis(demean, 1, wm), 2 ).sum(axis=0) ) ]\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Save data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    if not os.path.isdir(out_dir): os.makedirs(out_dir)\n",
    "    f = os.path.join(out_dir, '%s_rest_qc_data' %subject)\n",
    "    np.savez_compressed(f, gm=gmts, wm=wmts, iir_params=iir_params )\n",
    "    del gmts, wmts, obj\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare motion and masked fMRI data. Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T15:32:49.355229",
     "start_time": "2017-02-02T15:19:33.986174"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "from scipy.signal import detrend\n",
    "from sklearn.decomposition import PCA\n",
    "def demean(arr): return arr - arr.mean()\n",
    "def rms(arr): return np.sqrt( np.mean( np.power(arr, 2) ) )\n",
    "%matplotlib inline\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "threshold = 0.5\n",
    "fast_dir = '/space/will/3/users/EMBARC/EMBARC-FAST'\n",
    "fsd = 'rest'\n",
    "run = '001'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "## Subjects with manual coregistration.\n",
    "# subjects = ['CU0092CUMR1R1', 'CU0087CUMR1R1', 'CU0095CUMR1R1', 'CU0104CUMR1R1', 'CU0131CUMR1R1', 'CU0069CUMR1R1', \n",
    "#             'CU0025CUMR1R1', 'CU0094CUMR1R1', 'CU0067CUMR1R1', 'CU0106CUMR1R1',  'CU0105CUMR1R1', \n",
    "#             'CU0070CUMR1R1', 'CU0129CUMR1R1', 'CU0113CUMR1R1', 'CU0125CUMR1R1', 'CU0133CUMR1R1']\n",
    "\n",
    "for subject in subjects[18:]:\n",
    "    \n",
    "    print subject, \n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare motion data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Read motion data.\n",
    "    mc = os.path.join(fast_dir, subject, fsd, run, 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:7]\n",
    "\n",
    "    ## Remove first four timepoints.\n",
    "    mc = np.delete(mc, np.arange(4),axis=0)\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if mc.shape[0] > 176:\n",
    "        mc = np.delete(mc, np.arange(176,mc.shape[0]), axis=0)\n",
    "\n",
    "    ## Invert angular displacement.\n",
    "    copy = mc.copy()\n",
    "    mc[:,:3] = np.deg2rad(mc[:,:3]) \n",
    "    mc[:,:3] *= 50\n",
    "\n",
    "    ## Compute framewise displacement (See Power 2012, 2014).\n",
    "    fd = np.insert( np.abs( np.diff(mc[:,3:], axis=0) ).sum(axis=1), 0, 0)\n",
    "\n",
    "    ## Demean/detrend data.\n",
    "    mc = detrend(copy, axis=0, type='constant')\n",
    "    mc = detrend(mc, axis=0, type='linear')\n",
    "\n",
    "    ## Construct basis sets.\n",
    "    mcreg24 = mc.copy()\n",
    "    mcreg24 = np.concatenate([mcreg24, np.roll(mcreg24, -1, 0)], axis=-1)\n",
    "    mcreg24 = np.concatenate([mcreg24, np.power(mcreg24,2)], axis=-1)\n",
    "\n",
    "    ## Replace last timepoint with zeros.\n",
    "    ## Otherwise you are putting motion from the first frame into the last of the run.\n",
    "    mcreg24[-1] = np.zeros((mcreg24[-1].shape))\n",
    "    \n",
    "    ## Apply PCA.  \n",
    "    pca = PCA(n_components=24)\n",
    "    mcreg24 = pca.fit_transform(mcreg24)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare fMRI data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load gray/white matter timeseries.\n",
    "    npz = np.load(os.path.join(fast_dir, subject, fsd, run, 'motion/%s_rest_qc_data.npz' %subject))\n",
    "    \n",
    "    gm_pre = np.apply_along_axis(demean, 1, np.delete(npz['gm'],np.arange(4),axis=1))\n",
    "    wm_pre = np.apply_along_axis(demean, 1, np.delete(npz['wm'],np.arange(4),axis=1))\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if gm_pre.shape[1] > 176:\n",
    "        gm_pre = np.delete(gm_pre, np.arange(176,gm_pre.shape[1]), axis=1)\n",
    "        wm_pre = np.delete(wm_pre, np.arange(176,wm_pre.shape[1]), axis=1)\n",
    "\n",
    "    gm_beta, _, _, _ = np.linalg.lstsq(mcreg24, gm_pre.T)\n",
    "    wm_beta, _, _, _ = np.linalg.lstsq(mcreg24, wm_pre.T)\n",
    "    gm_post = gm_pre - np.dot(mcreg24, gm_beta).T\n",
    "    wm_post = wm_pre - np.dot(mcreg24, wm_beta).T\n",
    "\n",
    "    ## Compute DVARS.\n",
    "    gm_DVARS = np.zeros((2,fd.shape[0]))\n",
    "    wm_DVARS = np.zeros((2,fd.shape[0]))\n",
    "    for n, arr in enumerate([gm_pre,gm_post]): gm_DVARS[n,1:] += np.apply_along_axis(rms, 0, np.diff(arr, axis=1))\n",
    "    for n, arr in enumerate([wm_pre,wm_post]): wm_DVARS[n,1:] += np.apply_along_axis(rms, 0, np.diff(arr, axis=1))\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plotting\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    fig = plt.figure(figsize=(16,18))\n",
    "    nrow = 16\n",
    "\n",
    "    ## Plot framewise displacement.\n",
    "    ax = plt.subplot2grid((nrow,1),(0,0),rowspan=1)\n",
    "    ax.plot(fd, linewidth=1.5, color='k')\n",
    "    ax.hlines(0.5, 0, fd.shape[0], linestyle='--', color='k', alpha=0.5)\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([0.0,0.5,1.0])\n",
    "    ax.set_yticklabels([0.0,0.5,1.0], rotation=90)\n",
    "    ax.set_ylabel('FD (mm)', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.set_title(subject, fontsize=24)\n",
    "\n",
    "    ## Plot grey matter (pre-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(1,0),rowspan=4)\n",
    "    cbar = ax.imshow(gm_pre, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('Grey Matter\\nPre-Regression', fontsize=12)\n",
    "\n",
    "    # Colobar setup.\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width, box.height])\n",
    "    axColor = plt.axes([0.905, 0.238, 0.015, 0.660])\n",
    "    cbar = plt.colorbar(cbar, cax = axColor, orientation=\"vertical\")\n",
    "    cbar.set_ticks([-50,0,50])\n",
    "    axColor.set_ylabel('BOLD', fontsize=12, rotation=270)\n",
    "\n",
    "    ## Plot grey matter (post-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(5,0),rowspan=4)\n",
    "    cbar = ax.imshow(gm_post, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('Grey Matter\\nPost-Regression', fontsize=12)\n",
    "\n",
    "    ## Plot DVARS.\n",
    "    ax = plt.subplot2grid((nrow,1),(9,0),rowspan=1)\n",
    "    for arr, label in zip(gm_DVARS, ['Pre','Post']): ax.plot(arr, linewidth=1.5, alpha=0.8, label=label)\n",
    "    ax.legend(loc=2, fontsize=8, frameon=False, borderpad=0.0,  handlelength=1.4, handletextpad=0.2)\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_xlabel('Acquisitions', fontsize=12)\n",
    "    ax.set_ylim(20,100)\n",
    "    ax.set_yticks([20,60,100])\n",
    "    ax.set_yticklabels([20,60,100], rotation=90)\n",
    "    ax.set_ylabel('DVARS_gm', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    ## Plot white matter (pre-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(11,0),rowspan=2)\n",
    "    cbar = ax.imshow(wm_pre, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('White Matter\\nPre-Regression', fontsize=12)\n",
    "\n",
    "    ## Plot white matter (post-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(13,0),rowspan=2)\n",
    "    cbar = ax.imshow(wm_post, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('White Matter\\nPost-Regression', fontsize=12)\n",
    "\n",
    "    ## Plot DVARS.\n",
    "    ax = plt.subplot2grid((nrow,1),(15,0))\n",
    "    for arr, label in zip(wm_DVARS, ['Pre','Post']): ax.plot(arr, linewidth=1.5, alpha=0.8, label=label)\n",
    "    ax.legend(loc=2, fontsize=8, frameon=False, borderpad=0.0,  handlelength=1.4, handletextpad=0.2)\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_xlabel('Acquisitions', fontsize=12)\n",
    "    ax.set_ylim(20,100)\n",
    "    ax.set_yticks([20,60,100])\n",
    "    ax.set_yticklabels([20,60,100], rotation=90)\n",
    "    ax.set_ylabel('DVARS_wm', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    plt.subplots_adjust(left=0.05, right=0.90, top=0.95, bottom=0.05, hspace=0.05)\n",
    "#     plt.show()\n",
    "    plt.savefig(os.path.join(fast_dir,subject,fsd,run,'motion/%s_supp_motion.png' %subject), dpi=180)\n",
    "    plt.savefig(os.path.join(fast_dir, 'jan2017', 'motion_plots', '%s_supp_motion.png' %subject), dpi=180)\n",
    "    plt.close('all')\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T14:52:28.312842",
     "start_time": "2017-02-23T14:52:28.310448"
    }
   },
   "source": [
    "## Calculate Correlations and r-to-z transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T09:51:14.745052",
     "start_time": "2017-03-02T09:51:09.212971"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np  \n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = \"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt\"\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs \n",
    "\n",
    "## Specify labels.\n",
    "regions = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')])\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "        \n",
    "    ## Load regressed data. \n",
    "    regressed = np.load(os.path.join(gt_dir, 'regressed', '%s_regressed.npz' %subject))['regressed']\n",
    "    \n",
    "    ## Correlate. \n",
    "    corrmat = np.corrcoef(regressed)\n",
    "    \n",
    "    ## Remove negative correlations.\n",
    "    corrmat = np.where(corrmat<=0, 0, corrmat)\n",
    "\n",
    "    ## Restrict to lower triangular matrix. \n",
    "    corrmat = corrmat[np.tril_indices_from(corrmat,k=-1)]\n",
    "    \n",
    "    # Fisher's r-to-z transform.\n",
    "    zmat = np.arctanh(corrmat)\n",
    "    \n",
    "    ## Remove inf created by correlation values == 1.\n",
    "    zmat = np.where(zmat==np.inf,np.nan,zmat)\n",
    "    \n",
    "    ## Normalize correlations.\n",
    "#     zmat = (zmat - np.nanmin(zmat)) / (np.nanmax(zmat) - np.nanmin(zmat))\n",
    "    \n",
    "    ## Save correlation.\n",
    "    np.savez(os.path.join(gt_dir, 'correlations', '%s_corr' %subject), corrmat=corrmat)\n",
    "    np.savez(os.path.join(gt_dir, 'correlations', '%s_rtoz' %subject), zmat=zmat)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate ratio of negative correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T14:57:01.849124",
     "start_time": "2017-02-23T14:57:01.311477"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "pps_rtoz = np.zeros((len(pps),np.triu_indices(126,k=1)[0].shape[0]))\n",
    "hcs_rtoz = np.zeros((len(hcs),np.triu_indices(126,k=1)[0].shape[0]))\n",
    "\n",
    "for i, subject in enumerate(pps): \n",
    "    pps_rtoz[i] = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']\n",
    "    \n",
    "for i, subject in enumerate(hcs): \n",
    "    hcs_rtoz[i] = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']    \n",
    "\n",
    "## Concatenate all subjects' rtoz.\n",
    "rtoz_mat = np.concatenate([pps_rtoz,hcs_rtoz],axis=0)\n",
    "    \n",
    "ncorrs = 7875.0\n",
    "\n",
    "for subgroup,mat,name in zip([subjects,pps,hcs],[rtoz_mat,pps_rtoz,hcs_rtoz],['all','psychiatric','healthy']):\n",
    "    \n",
    "    ratio_neg = np.zeros((len(subgroup)))\n",
    "\n",
    "    for i in np.arange(ratio_neg.shape[0]):\n",
    "        ratio_neg[i] = np.where(mat[i]<0)[0].shape[0] / ncorrs\n",
    "\n",
    "    print('Negative correlations, (%s), mean pct: %.2f' %(name,np.mean(ratio_neg)*100))\n",
    "    print('Negative correlations, (%s), std: %.3f' %(name,np.std(ratio_neg)))\n",
    "    \n",
    "    print('Min: ', np.min(mat[np.where(mat<0)]))\n",
    "    print('Max: ', np.max(mat[np.where(mat<0)]))\n",
    "    \n",
    "    plt.hist(mat[np.where(mat<0)],normed=True,alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group specific r-to-z distributions and heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T09:57:47.528184",
     "start_time": "2017-03-02T09:57:35.895749"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "## Specify regions.\n",
    "regions = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')])\n",
    "\n",
    "## Initialize arrays. \n",
    "pps_rtoz = np.zeros((len(pps),np.triu_indices(126,k=1)[0].shape[0]))\n",
    "hcs_rtoz = np.zeros((len(hcs),np.triu_indices(126,k=1)[0].shape[0]))\n",
    "\n",
    "for i, subject in enumerate(pps): \n",
    "    pps_rtoz[i] = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']\n",
    "    \n",
    "for i, subject in enumerate(hcs): \n",
    "    hcs_rtoz[i] = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']    \n",
    "    \n",
    "## Concatenate all subjects' rtoz.\n",
    "rtoz_mat = np.concatenate([pps_rtoz,hcs_rtoz],axis=0)\n",
    "\n",
    "###~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot rtoz distribution by group.\n",
    "###~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "# sns.distplot(np.mean(rtoz_mat,axis=0),hist=False,hist_kws={'alpha':0.7},label='All subjects',norm_hist=True)\n",
    "sns.distplot(np.mean(pps_rtoz,axis=0),hist=False,hist_kws={'alpha':0.7},label='Psychiatric',norm_hist=True)\n",
    "sns.distplot(np.mean(hcs_rtoz,axis=0),hist=False,hist_kws={'alpha':0.5},label='Healthy',norm_hist=True)\n",
    "plt.legend(fontsize=14)\n",
    "plt.title('r-to-z distribution by group',fontsize=18)\n",
    "plt.xlabel('r-to-z',fontsize=14)\n",
    "plt.ylabel('frequency (normed)',fontsize=14)\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig('/autofs/space/will_003/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/correlations/distributions/rtoz_dist_by_group.png')\n",
    "plt.close()\n",
    "\n",
    "###~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot rtoz heatmap by group.\n",
    "###~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for subgroup,zvals,name in zip([subjects,pps,hcs],[rtoz_mat,pps_rtoz,hcs_rtoz],['all','psychiatric','healthy']):\n",
    "    \n",
    "    ## Convert to square lower-triangular matrix.\n",
    "    zmat = np.zeros((126,126))\n",
    "    zmat[np.tril_indices(126,k=-1)] = np.mean(zvals,axis=0)\n",
    "    zmat[np.triu_indices(126,k=0)] = np.nan\n",
    "\n",
    "    ## Remove negative correlations. \n",
    "    zmat = np.where(zmat<0,np.nan,zmat)\n",
    "   \n",
    "    ## Plotting.\n",
    "    fig = plt.figure(figsize=(30,30))\n",
    "    sns.heatmap(zmat, cmap=\"YlOrRd_r\", vmax=np.nanmax(zmat), vmin=np.nanmin(zmat), square=True, xticklabels=regions, yticklabels=regions, linewidth=0.1, \n",
    "                    cbar_kws={'shrink':0.8},label=name)\n",
    "\n",
    "    ## Draw lines. \n",
    "    xlines = xlines = [14,26,33,35,46,51,52,57,63,75,87,94,96,109,114,115,120]\n",
    "    ylines = [126-x for x in xlines]\n",
    "    plt.vlines(x=xlines, ymin=0, ymax=[126-x for x in xlines],linestyles='dotted')\n",
    "    plt.hlines(y=ylines, xmin=0, xmax=xlines, linestyles='dotted')\n",
    "    \n",
    "    plt.title('Average r-to-z for %s subjects' %name,fontsize=24)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/autofs/space/will_003/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/correlations/heatmaps/%s_average_rtoz.png' %name)\n",
    "    plt.close()\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Correlation heatmap with/without GSR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T14:08:10.554779",
     "start_time": "2017-02-21T14:08:10.387390"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "## Specify labels.\n",
    "regions = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')])\n",
    "\n",
    "lh = np.concatenate([regions[:57],regions[114:120]])\n",
    "rh = np.concatenate([regions[57:114],regions[120:]])\n",
    "\n",
    "regions = np.concatenate([lh,rh])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot correlations per subject (with/without GSR).\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for subject in subjects: \n",
    "    \n",
    "    print subject, \n",
    "    \n",
    "    zmat_gsr = np.load(os.path.join(gt_dir, 'correlations', '%s_correlations_unthresholded.npz' %subject))['corrmat']\n",
    "    zmat_nogsr = np.load(os.path.join(gt_dir, 'correlations', '%s_correlations_nogsr_unthresholded.npz' %subject))['corrmat']\n",
    "    \n",
    "    zmat_diff = zmat_gsr - zmat_nogsr\n",
    "    zmat = np.concatenate([np.concatenate([zmat_diff[:57],zmat_diff[114:120]]),np.concatenate([zmat_diff[57:114],zmat_diff[120:]])])\n",
    "    \n",
    "    ## Convert to DataFrame.\n",
    "    zpd = pd.DataFrame(zmat, columns=regions, index=regions)\n",
    "    \n",
    "    ## Generate a mask for the upper triangle. \n",
    "    mask = np.zeros_like(zpd, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plot heatmap.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    f, ax = plt.subplots(figsize=(25,20))\n",
    "    cmap=sns.color_palette(\"RdBu_r\")\n",
    "    sns.heatmap(zpd, mask=mask, cmap=cmap, square=True, xticklabels=True, yticklabels=True, linewidth=0.1, cbar_kws={'shrink':1.0}, ax=ax)\n",
    "    \n",
    "    ## Draw lines. \n",
    "    xlines = [14,26,33,35,46,52,57,63] + [63+x for x in [14,26,33,35,46,52,57]]\n",
    "    ylines = [126-x for x in xlines]\n",
    "    plt.vlines(x=xlines, ymin=0, ymax=[126-x for x in xlines],linestyles='dotted')\n",
    "    plt.hlines(y=ylines, xmin=0, xmax=xlines, linestyles='dotted')\n",
    "        \n",
    "    ## Flourishes.\n",
    "    ax.set_title('%s: Correlations GSR - noGSR, unthresholded' %subject, fontsize=30)\n",
    "    cax = plt.gcf().axes[-1]\n",
    "    cax.tick_params(labelsize=20,labeltop=True,labelbottom=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "#     plt.show()\n",
    "    plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/correlations/hcs/%s_corr_unthresh_GSRdiff.png' %subject)\n",
    "    plt.close()\n",
    "    \n",
    "print '\\nDone.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot correlation and r-to-z distribution by site. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T09:59:31.773242",
     "start_time": "2017-03-02T09:59:30.942954"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs \n",
    "\n",
    "## Specify regions.\n",
    "regions = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')])\n",
    "ncorrs = 7875\n",
    "\n",
    "## Initialize matrix to store correlations.\n",
    "corrmat_master = np.zeros((len(subjects),ncorrs))\n",
    "    \n",
    "## Load correlations. \n",
    "for i,subject in enumerate(subjects): \n",
    "\n",
    "    corrmat_master[i] = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']\n",
    "\n",
    "## Remove negative correlations. \n",
    "corrmat_master = np.where(corrmat_master<0, np.nan, corrmat_master)\n",
    "\n",
    "## Remove perfect correlations (r==1). \n",
    "# corrmat_master = np.where(corrmat_master==1, np.nan, corrmat_master)\n",
    "\n",
    "## Plotting.\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.set(color_codes=True)\n",
    "sns.set_palette(sns.color_palette(\"deep\"))\n",
    "\n",
    "for site in ['CU','MG','TX','UM']:\n",
    "    \n",
    "    site_corrmat = corrmat_master[np.nonzero([1 if s.startswith(site) else 0 for s in subjects])]\n",
    "    \n",
    "    ## Plotting.\n",
    "    sns.distplot(site_corrmat.flatten(),hist=False, kde=True, label=site, hist_kws={'alpha':0.7})\n",
    "\n",
    "## Flourishes.\n",
    "plt.legend(fontsize=14)\n",
    "plt.title('Distribution of r-to-z', fontsize=16)\n",
    "\n",
    "## Save.\n",
    "plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/correlations/distributions/rtoz_by_site.png')\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot correlation distribution (GSR vs. noGSR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "## Specify regions.\n",
    "regions = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot correlations (with/without GSR).\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for subject in subjects: \n",
    "    \n",
    "    ## Load data.\n",
    "    zmat_gsr = np.load(os.path.join(gt_dir, 'correlations', '%s_correlations_unthresholded.npz' %subject))['corrmat']\n",
    "    zmat_nogsr = np.load(os.path.join(gt_dir, 'correlations', '%s_correlations_nogsr_unthresholded.npz' %subject))['corrmat']\n",
    "    \n",
    "    ## Convert to DataFrame.\n",
    "    zpd_gsr = pd.DataFrame(zmat_gsr, columns=regions, index=regions)\n",
    "    \n",
    "    ## Plotting.\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    sns.set(color_codes=True)\n",
    "    sns.set_palette(sns.color_palette(\"GnBu_d\"))\n",
    "    sns.distplot(zmat_gsr.flatten(),hist=False, kde=True, label='GSR', hist_kws={'alpha':0.7})\n",
    "    sns.distplot(zmat_nogsr.flatten(), hist=False, kde=True, label='No GSR', hist_kws={'alpha':0.7})\n",
    "    \n",
    "    ## Flourishes.\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.title('%s \\nDistribution of Correlations (unthresholded)' %subject, fontsize=16)\n",
    "    \n",
    "    ## Save.\n",
    "    plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/correlations/histograms/%s_corr_GSRhist_unthresh.png' %subject)\n",
    "    plt.close()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute network thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T10:31:10.344462",
     "start_time": "2017-03-02T10:05:53.937226"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import networkx as nx\n",
    "import scipy.io as sio\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "%matplotlib inline\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Construct network with GSR.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Specify directories.\n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Load regions.\n",
    "regions = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')]\n",
    "n_regions = len(regions)\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "n_subs = len(subjects)\n",
    "\n",
    "## Specify parameters. \n",
    "thresholds = np.arange(0,1.75,0.01)  ## len(thresholds) = 175\n",
    "metrics = ['threshold', 'n_nodes', 'n_edges', 'avg_degree', 'n_components', 'cc_node_ratio', 'cc_edge_ratio', 'density']\n",
    "metrics_dat = np.zeros((len(subjects),thresholds.shape[0],len(metrics)))\n",
    "\n",
    "## Iterate over subjects.\n",
    "for s,subject in enumerate(subjects):\n",
    "                \n",
    "    ## Load data. \n",
    "    rtoz = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']\n",
    "    \n",
    "    ## Iterate over thresholds. \n",
    "    for t,thresh in enumerate(thresholds): \n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Create graph.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(regions)\n",
    "\n",
    "        for q,r,val in zip(np.tril_indices(len(regions),k=-1)[0], np.tril_indices(len(regions),k=-1)[1], rtoz):\n",
    "            if not np.isnan(val) and val >= thresh:\n",
    "                G.add_edge(regions[q],regions[r])\n",
    "\n",
    "        G.remove_nodes_from(G.nodes_with_selfloops())\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute graph metrics.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        n_nodes = nx.number_of_nodes(G)\n",
    "        n_edges = nx.number_of_edges(G)\n",
    "\n",
    "        max_edges = (n_nodes*(n_nodes-1))/2\n",
    "        avg_degree = sum(nx.degree(G).values()) / n_nodes  \n",
    "\n",
    "        n_components = nx.number_connected_components(G)\n",
    "        largest_cc = max(nx.connected_component_subgraphs(G), key=len)\n",
    "        cc_nodes = nx.number_of_nodes(max(nx.connected_component_subgraphs(G), key=len))\n",
    "        cc_edges = nx.number_of_edges(max(nx.connected_component_subgraphs(G), key=len))\n",
    "\n",
    "        cc_node_ratio = float(cc_nodes) / n_nodes\n",
    "        \n",
    "        if n_edges > 0: \n",
    "            cc_edge_ratio = float(cc_edges) / n_edges\n",
    "        else: \n",
    "            cc_edge_ratio = 0\n",
    "\n",
    "        density = np.round(nx.density(G),3)\n",
    "\n",
    "        ## Store metrics. \n",
    "        metrics_dat[s,t] = np.array([thresh, n_nodes, n_edges, avg_degree, n_components, cc_node_ratio, cc_edge_ratio, density])\n",
    "\n",
    "## Flatten. \n",
    "metrics_dat = metrics_dat.reshape((len(subjects)*len(thresholds),len(metrics)),order='C')\n",
    "\n",
    "## Convert to csv. Save.\n",
    "subject_ids = np.expand_dims(np.repeat(np.array(subjects),len(thresholds)),axis=1)\n",
    "metrics_pd = pd.DataFrame(np.concatenate([subject_ids, metrics_dat],axis=1),columns=['Subject_ID'] + metrics)\n",
    "metrics_pd.to_csv(os.path.join(gt_dir, 'graph_metrics', 'metrics_by_rtoz_threshold.csv'))\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-28T13:40:34.499555",
     "start_time": "2017-02-28T13:40:34.496291"
    }
   },
   "source": [
    "## Compute global graph metrics by density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-20T11:39:46.873692",
     "start_time": "2017-03-20T11:39:46.224031"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy.io as sio\n",
    "from itertools import permutations\n",
    "\n",
    "## Define functions. \n",
    "def efficiency(G, u, v):\n",
    "    return 1. / nx.shortest_path_length(G, u, v)\n",
    "\n",
    "def global_efficiency(G):\n",
    "    n = len(G)\n",
    "    denom = n * (n - 1)\n",
    "    return sum(efficiency(G, u, v) for u, v in permutations(G, 2)) / denom\n",
    "\n",
    "def local_efficiency(G):\n",
    "    return sum(global_efficiency(nx.ego_graph(G, v)) for v in G) / len(G)\n",
    "\n",
    "## Specify directories.\n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Construct network.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Specify directories.\n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Load regions.\n",
    "regions = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')]\n",
    "n_regions = len(regions)\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "n_subs = len(subjects)\n",
    "\n",
    "## Specify parameters. \n",
    "density_range = [0.3,0.1]\n",
    "thresholds = np.arange(0.10,2.50,0.05)  ## len(thresholds) = 48\n",
    "metrics = ['threshold', 'n_nodes', 'n_edges', 'avg_degree', 'n_components', 'cc_node_ratio', 'cc_edge_ratio', 'density', \n",
    "           'clustering_coefficient', 'avg_path_length', 'glob_efficiency', 'loc_efficiency']\n",
    "\n",
    "## Load graph metrics.\n",
    "metrics_pd = pd.read_csv(os.path.join(gt_dir, 'graph_metrics', 'metrics_by_rtoz_threshold.csv'), \n",
    "                         usecols=['Subject_ID', 'threshold', 'n_nodes', 'n_edges', 'avg_degree', 'n_components', 'cc_node_ratio', 'cc_edge_ratio', 'density'])\n",
    "metrics_pd['cohort'] = [1 if subject in pps else 2 for subject in metrics_pd.Subject_ID]\n",
    "\n",
    "## Reset index. \n",
    "metrics_pd = metrics_pd.set_index('Subject_ID', drop=True)\n",
    "\n",
    "## Threshold by average degree. \n",
    "metrics_pd = metrics_pd.where(metrics_pd.avg_degree > (2 * np.log(metrics_pd.n_nodes)))\n",
    "\n",
    "## Threshold by ratio of nodes and edges in largest connected component. \n",
    "metrics_pd = metrics_pd.where(metrics_pd.cc_node_ratio > 0.9)\n",
    "metrics_pd = metrics_pd.where(metrics_pd.cc_node_ratio > 0.9)\n",
    "\n",
    "## Threshold by density.\n",
    "metrics_pd.density = np.round(metrics_pd.density,2)\n",
    "metrics_pd = metrics_pd.where(((metrics_pd.density<=density_range[0])&(metrics_pd.density>=density_range[1])))\n",
    "\n",
    "## Remove NaNs. \n",
    "metrics_pd = metrics_pd.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T12:53:27.965113",
     "start_time": "2017-03-02T11:19:35.149969"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Matrix to store data. \n",
    "metrics_dat = np.zeros((metrics_pd.index.shape[0],len(metrics)))\n",
    "\n",
    "## Iterate over subjects.\n",
    "for idx,subject,thresh in zip(np.arange(metrics_dat.shape[0]),metrics_pd.index,metrics_pd.threshold):\n",
    "                \n",
    "    ## Load data. \n",
    "    rtoz = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Create graph.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(regions)\n",
    "\n",
    "    for q,r,val in zip(np.tril_indices(126,k=-1)[0], np.tril_indices(126,k=-1)[1], rtoz):\n",
    "        if not np.isnan(val) and val >= thresh:\n",
    "            G.add_edge(regions[q],regions[r])\n",
    "\n",
    "    G.remove_nodes_from(G.nodes_with_selfloops())\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute graph metrics.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    n_nodes = nx.number_of_nodes(G)\n",
    "    n_edges = nx.number_of_edges(G)\n",
    "\n",
    "    max_edges = (n_nodes*(n_nodes-1))/2\n",
    "    avg_degree = sum(nx.degree(G).values()) / n_nodes  \n",
    "\n",
    "    n_components = nx.number_connected_components(G)\n",
    "    largest_cc = max(nx.connected_component_subgraphs(G), key=len)\n",
    "    cc_nodes = nx.number_of_nodes(max(nx.connected_component_subgraphs(G), key=len))\n",
    "    cc_edges = nx.number_of_edges(max(nx.connected_component_subgraphs(G), key=len))\n",
    "\n",
    "    cc_node_ratio = float(cc_nodes) / n_nodes\n",
    "    cc_edge_ratio = float(cc_edges) / n_edges\n",
    "\n",
    "    density = np.round(nx.density(G),3)\n",
    "\n",
    "    ## Global metrics.\n",
    "    clustering_coeff = np.round(nx.average_clustering(G),3)\n",
    "\n",
    "    if n_components >1: \n",
    "        avg_path_length = np.round(nx.average_shortest_path_length(largest_cc),3)\n",
    "        glob_efficiency = np.round(global_efficiency(largest_cc),3)\n",
    "        loc_efficiency = np.round(local_efficiency(largest_cc),3)\n",
    "    else: \n",
    "        avg_path_length = np.round(nx.average_shortest_path_length(G),3)\n",
    "        glob_efficiency = np.round(global_efficiency(G),3)\n",
    "        loc_efficiency = np.round(local_efficiency(G),3)\n",
    "\n",
    "    ## Store metrics. \n",
    "    metrics_dat[idx] = np.array([thresh, n_nodes, n_edges, avg_degree, n_components, cc_node_ratio, cc_edge_ratio, \n",
    "    density, clustering_coeff, avg_path_length, glob_efficiency, loc_efficiency])\n",
    "\n",
    "## Convert to csv. Save.\n",
    "metrics_updated = pd.DataFrame(np.concatenate([np.expand_dims(metrics_pd.index,1), metrics_dat],axis=1),columns=['Subject_ID'] + metrics)\n",
    "\n",
    "## Save. \n",
    "metrics_updated.to_csv(os.path.join(gt_dir, 'graph_metrics', 'metrics_by_density.csv'))\n",
    "\n",
    "print 'Done.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute modularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-20T11:49:48.753890",
     "start_time": "2017-03-20T11:49:48.629050"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run 5.2.1. \n",
    "## Source code: http://perso.crans.org/aynaud/communities/api.html#community.generate_dendrogram \n",
    "\n",
    "import community\n",
    "import networkx as nx\n",
    "\n",
    "modularities = np.zeros((metrics_pd.index.shape[0]))\n",
    "\n",
    "## Iterate over subjects.\n",
    "for idx,subject,thresh in zip(np.arange(metrics_pd.shape[0]),metrics_pd.index,metrics_pd.threshold):\n",
    "                \n",
    "    ## Load data. \n",
    "    rtoz = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Create graph.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(regions)\n",
    "\n",
    "    for q,r,val in zip(np.tril_indices(126,k=-1)[0], np.tril_indices(126,k=-1)[1], rtoz):\n",
    "        if not np.isnan(val) and val >= thresh:\n",
    "            G.add_edge(regions[q],regions[r])\n",
    "\n",
    "    G.remove_nodes_from(G.nodes_with_selfloops())\n",
    "\n",
    "    part = community.best_partition(G)\n",
    "    modularities[idx] = community.modularity(part,G)\n",
    "    \n",
    "## Add results to dataframe. \n",
    "metrics_pd['modularity'] = modularities\n",
    "\n",
    "## Save. \n",
    "metrics_pd.to_csv(os.path.join(gt_dir, 'graph_metrics', 'metrics_by_density_updated.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-20T11:35:39.963007",
     "start_time": "2017-03-20T11:35:39.959551"
    }
   },
   "source": [
    "#### Visualize modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-17T16:33:05.742572",
     "start_time": "2017-03-17T16:33:05.464687"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab as plt \n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "size = float(len(set(part.values())))\n",
    "pos = nx.spring_layout(largest_cc)\n",
    "count = 0\n",
    "for com in set(part.values()):\n",
    "    count += 1\n",
    "    list_nodes = [nodes for nodes in part.keys() if part[nodes] == com ]\n",
    "    nx.draw_networkx_nodes(largest_cc, pos, list_nodes, node_size = 60, node_color = str(count / size))\n",
    "nx.draw_networkx_edges(largest_cc, pos, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize global graph metrics by density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-20T13:13:28.990445",
     "start_time": "2017-03-20T13:13:26.709947"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import pylab as plt\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "n_subs = len(subjects)\n",
    "\n",
    "## Specify parameters. \n",
    "qc_metrics = ['threshold', 'n_nodes', 'n_edges','n_components', 'cc_node_ratio', 'cc_edge_ratio']\n",
    "global_metrics = ['avg_degree','clustering_coefficient', 'avg_path_length', 'glob_efficiency', 'loc_efficiency', 'modularity']\n",
    "\n",
    "metrics_pd = pd.read_csv(os.path.join(gt_dir, 'graph_metrics', 'metrics_by_density.csv'), usecols=['Subject_ID'] + qc_metrics + global_metrics + ['density'])\n",
    "metrics_pd.density = np.round(metrics_pd.density,2)\n",
    "\n",
    "density_gb = metrics_pd.groupby('density')\n",
    "\n",
    "## Separate by cohort.\n",
    "metrics_pd['cohort'] = [1 if subject in pps else 2 for subject in metrics_pd.Subject_ID]\n",
    "\n",
    "density_pp = (metrics_pd.where(metrics_pd.cohort == 1)).groupby('density')\n",
    "density_hc = (metrics_pd.where(metrics_pd.cohort == 2)).groupby('density')\n",
    "  \n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Compare global metrics across cohorts. \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "   \n",
    "for metrics, name in zip([qc_metrics, global_metrics],['QC','Global']): \n",
    "    \n",
    "    fig = plt.figure(figsize=(30,24))\n",
    "        \n",
    "    for i,m in enumerate(sorted(metrics)):\n",
    "\n",
    "        ax = plt.subplot2grid((2,3),(i/3,i%3), rowspan=1, colspan=1)\n",
    "\n",
    "        ax.scatter(density_pp.mean().index, density_pp.mean()[m])\n",
    "        ax.plot(density_pp.mean().index, density_pp.mean()[m])\n",
    "\n",
    "        ax.scatter(density_hc.mean().index, density_hc.mean()[m])\n",
    "        ax.plot(density_hc.mean().index, density_hc.mean()[m])\n",
    "                \n",
    "        ax.set_xlabel('density', fontsize=18)\n",
    "        ax.set_ylabel(m, fontsize=18)\n",
    "\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(16)  \n",
    "        for tick in ax.yaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(16)     \n",
    "        \n",
    "        plt.legend(['Psychiatric','Healthy'], fontsize=18)\n",
    "        plt.title(m, fontsize=22)\n",
    "        \n",
    "\n",
    "    plt.suptitle('%s Graph Metrics' %name, fontsize=50)\n",
    "\n",
    "#     plt.show()\n",
    "    plt.savefig('/autofs/space/will_003/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/graph_metrics/compare_%s_by_density.png' %name)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute degree distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T13:35:58.013100",
     "start_time": "2017-03-02T13:35:57.162435"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy.io as sio\n",
    "import pylab as plt\n",
    "from itertools import permutations\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "metrics = ['threshold', 'n_nodes', 'n_edges', 'avg_degree', 'n_components', 'cc_node_ratio', 'cc_edge_ratio', 'density', \n",
    "           'clustering_coefficient', 'avg_path_length', 'glob_efficiency', 'loc_efficiency']\n",
    "\n",
    "metrics_pd = pd.read_csv(os.path.join(gt_dir, 'graph_metrics', 'metrics_by_density.csv'), usecols=['Subject_ID'] + metrics)\n",
    "metrics_pd.density = np.round(metrics_pd.density,2)\n",
    "\n",
    "Gnets = []\n",
    "\n",
    "## Iterate over subjects.\n",
    "for subject,thresh in zip(metrics_density.Subject_ID,metrics_density.threshold):\n",
    "\n",
    "    ## Load data. \n",
    "    rtoz = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Create graph.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(regions)\n",
    "\n",
    "    for q,r,val in zip(np.tril_indices(126,k=-1)[0], np.tril_indices(126,k=-1)[1], rtoz):\n",
    "        if not np.isnan(val) and val >= thresh:\n",
    "            G.add_edge(regions[q],regions[r])\n",
    "\n",
    "    G.remove_nodes_from(G.nodes_with_selfloops())\n",
    "\n",
    "    Gnets.append(G)\n",
    "\n",
    "degree_dist = np.zeros((len(regions)))\n",
    "\n",
    "for i,g in enumerate(Gnets):\n",
    "    degrees,counts = np.unique(nx.degree(g).values(),return_counts=True)\n",
    "    degree_dist[degrees] += counts\n",
    "\n",
    "degree_dist = degree_dist / len(Gnets)\n",
    "\n",
    "## Plotting. \n",
    "fig = plt.figure(figsize=(16,6))\n",
    "\n",
    "ax1 = plt.subplot2grid((1,3),(0,0))\n",
    "ax1.bar(np.arange(len(regions)),degree_dist)\n",
    "ax1.set_xlabel('Degree')\n",
    "ax1.set_ylabel('Number of regions')\n",
    "\n",
    "ax2 = plt.subplot2grid((1,3),(0,1))\n",
    "ax2.scatter(np.log(np.arange(len(regions))),np.log(degree_dist))\n",
    "ax2.plot(np.log(np.arange(len(regions))),np.log(degree_dist))\n",
    "ax2.set_xlabel('log2(Degree)')\n",
    "ax2.set_ylabel('log2(Number of regions)')\n",
    "\n",
    "ax2 = plt.subplot2grid((1,3),(0,2))\n",
    "ax2.scatter(np.log10(np.arange(len(regions))),np.log10(degree_dist))\n",
    "ax2.plot(np.log10(np.arange(len(regions))),np.log10(degree_dist))\n",
    "ax2.set_xlabel('Rank (10^x)')\n",
    "ax2.set_ylabel('log10(Number of regions)')\n",
    "\n",
    "plt.suptitle('Degree Distribution',fontsize=18)\n",
    "\n",
    "#plt.show()\n",
    "plt.savefig('/autofs/space/will_003/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/graph_metrics/degree_dist.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regress against site and age variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Questionnaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-27T16:58:21.288092",
     "start_time": "2017-03-27T16:58:21.237788"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm \n",
    "from collections import defaultdict\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "behav_dir = '/autofs/space/will_003/users/EMBARC/behavior/assessments_raw'\n",
    "ques_dir = '/autofs/space/will_003/users/EMBARC/behavior/assessments_raw'\n",
    "\n",
    "## Load graph metrics. \n",
    "metrics_pd = pd.read_csv(os.path.join(gt_dir, 'graph_metrics', 'metrics_by_density.csv'))\n",
    "metrics_pd = metrics_pd.set_index(metrics_pd.Subject_ID)\n",
    "\n",
    "## Load questionnaires.\n",
    "ques = pd.read_csv(os.path.join(ques_dir,'Score_Summary_-_Evaluation.csv'), usecols=['ProjectSpecificID','sample','asrm_score2','aaq_score_result','hamd_score_24','hamd_36',\n",
    "                    'masq2_score_aa','masq2_score_ad','masq2_score_gd','mdqscore_total','sas_overall_mean','shaps_total_dichotomous','stai_post_final_score'])\n",
    "\n",
    "## Restrict questionniares to subjects of interest. \n",
    "ques = ques.loc[np.in1d(ques.ProjectSpecificID.str.upper(), [s[:6].upper() for s in np.unique(metrics_pd.Subject_ID)])]\n",
    "ques = ques.sort_values(by='ProjectSpecificID')\n",
    "ques = ques.set_index(np.unique(metrics_pd.index))\n",
    "\n",
    "## Load demographics data. \n",
    "dem = pd.read_csv(os.path.join(behav_dir, 'Demographics.csv'), usecols=['sample','ProjectSpecificID','SiteName','sex','hispa','race','age_evaluation'])\n",
    "dem = dem.sort_values(by='ProjectSpecificID')\n",
    "\n",
    "## Restrict demographics to subjects of interest. \n",
    "dem = dem.where(dem.ProjectSpecificID[np.in1d(dem.ProjectSpecificID, [s[:6] for s in np.unique(metrics_pd.Subject_ID)])])\n",
    "dem = dem.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-27T16:59:15.471227",
     "start_time": "2017-03-27T16:59:15.385253"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dem['site0'] = np.where(dem.SiteName == 'Columbia',1,0)\n",
    "dem['site1'] = np.where(dem.SiteName == 'Harvard',1,0)\n",
    "dem['site2'] = np.where(dem.SiteName == 'Michigan',1,0)\n",
    "dem['site3'] = np.where(dem.SiteName == 'UTSW',1,0)\n",
    "\n",
    "dem = dem.set_index(np.unique(metrics_pd.index))\n",
    "\n",
    "## Merge dataframes. \n",
    "merged = pd.merge(metrics_pd,dem,left_index=True,right_index=True)\n",
    "merged = pd.merge(merged,ques,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-27T17:00:20.226902",
     "start_time": "2017-03-27T17:00:20.221951"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Subject_ID', u'threshold', u'n_nodes', u'n_edges', u'avg_degree',\n",
       "       u'n_components', u'cc_node_ratio', u'cc_edge_ratio', u'density',\n",
       "       u'clustering_coefficient', u'avg_path_length', u'glob_efficiency',\n",
       "       u'loc_efficiency', u'modularity', u'cohort', u'sample_x',\n",
       "       u'ProjectSpecificID_x', u'SiteName', u'sex', u'hispa', u'race',\n",
       "       u'age_evaluation', u'site0', u'site1', u'site2', u'site3', u'sample_y',\n",
       "       u'ProjectSpecificID_y', u'asrm_score2', u'aaq_score_result',\n",
       "       u'hamd_score_24', u'hamd_36', u'masq2_score_aa', u'masq2_score_ad',\n",
       "       u'masq2_score_gd', u'mdqscore_total', u'sas_overall_mean',\n",
       "       u'shaps_total_dichotomous', u'stai_post_final_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-27T17:23:34.929482",
     "start_time": "2017-03-27T17:23:34.550463"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Density</th>\n",
       "      <th>Dependent</th>\n",
       "      <th>Independent</th>\n",
       "      <th>aic</th>\n",
       "      <th>beta</th>\n",
       "      <th>lb</th>\n",
       "      <th>p</th>\n",
       "      <th>t</th>\n",
       "      <th>ub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.2</td>\n",
       "      <td>avg_path_length</td>\n",
       "      <td>Cohort</td>\n",
       "      <td>-46.067154</td>\n",
       "      <td>-0.140516</td>\n",
       "      <td>-0.235420</td>\n",
       "      <td>0.010359</td>\n",
       "      <td>-3.198698</td>\n",
       "      <td>-0.045613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.2</td>\n",
       "      <td>glob_efficiency</td>\n",
       "      <td>Cohort</td>\n",
       "      <td>-123.689198</td>\n",
       "      <td>0.019315</td>\n",
       "      <td>0.007007</td>\n",
       "      <td>0.007225</td>\n",
       "      <td>3.390446</td>\n",
       "      <td>0.031622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.2</td>\n",
       "      <td>loc_efficiency</td>\n",
       "      <td>Cohort</td>\n",
       "      <td>-101.131704</td>\n",
       "      <td>-0.025280</td>\n",
       "      <td>-0.047562</td>\n",
       "      <td>0.040821</td>\n",
       "      <td>-2.451005</td>\n",
       "      <td>-0.002998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Density        Dependent Independent         aic      beta        lb  \\\n",
       "79      0.2  avg_path_length      Cohort  -46.067154 -0.140516 -0.235420   \n",
       "86      0.2  glob_efficiency      Cohort -123.689198  0.019315  0.007007   \n",
       "93      0.2   loc_efficiency      Cohort -101.131704 -0.025280 -0.047562   \n",
       "\n",
       "           p         t        ub  \n",
       "79  0.010359 -3.198698 -0.045613  \n",
       "86  0.007225  3.390446  0.031622  \n",
       "93  0.040821 -2.451005 -0.002998  "
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupdiff = defaultdict(list)\n",
    "\n",
    "## z-score age variables.\n",
    "merged['age_evaluation'] = (merged.age_evaluation - np.mean(merged.age_evaluation)) / np.std(merged.age_evaluation)\n",
    "merged = merged.fillna(value=0)\n",
    "\n",
    "## Run GLM for each density value.\n",
    "for d in [0.1,0.15,0.2,0.25,0.3]:\n",
    "    \n",
    "    temp = merged.where(merged.density == d)\n",
    "    temp = temp.dropna()\n",
    "    \n",
    "    temp = temp.drop([u'threshold', u'n_nodes', u'n_edges', u'avg_degree', u'n_components', u'cc_node_ratio', u'cc_edge_ratio',u'hispa', u'race',],1)\n",
    "    \n",
    "    for metric in ['clustering_coefficient','avg_path_length','glob_efficiency','loc_efficiency','modularity']: \n",
    "        \n",
    "        ##md = smf.OLS.from_formula(formula='%s ~ age_evaluation + cohort + site0 + site1 + site2 + site3 + hamd_36 + aaq_score_result + masq2_score_aa + masq2_score_ad + masq2_score_gd + shaps_total_dichotomous' \n",
    "                                 ## %metric, data=temp, groups=temp['SiteName'])\n",
    "        md = smf.OLS.from_formula(formula='%s ~ age_evaluation + cohort + site0 + site1 + site2 + site3' \n",
    "                                  %metric, data=temp, groups=temp['SiteName'])\n",
    "        mdf = md.fit()\n",
    "        \n",
    "        ##for iv,i,t,p,lb,ub in zip(['Intercept','Age','Cohort','Site0', 'Site1', 'Site2', 'Site3', 'hamd_36', 'aaq_score_result', 'masq2_score_aa', 'masq2_score_ad', 'masq2_score_gd','shape_total_dichotomous'],mdf.params,mdf.tvalues,mdf.pvalues,mdf.conf_int()[0],mdf.conf_int()[1]):\n",
    "        for iv,i,t,p,lb,ub in zip(['Intercept','Age','Cohort','Site0', 'Site1', 'Site2', 'Site3'],mdf.params,mdf.tvalues,mdf.pvalues,mdf.conf_int()[0],mdf.conf_int()[1]):  \n",
    "            groupdiff['Dependent'] += [metric]\n",
    "            groupdiff['Density'] += [d]\n",
    "            groupdiff['Independent'] += [iv]\n",
    "            groupdiff['beta'] += [i]\n",
    "            groupdiff['t'] += [t]\n",
    "            groupdiff['p'] += [p]\n",
    "            groupdiff['lb'] += [lb]\n",
    "            groupdiff['ub'] += [ub]\n",
    "            groupdiff['aic'] += [mdf.aic]\n",
    "\n",
    "## Convert to dataframe. \n",
    "gdf = pd.DataFrame(groupdiff)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Multiple comparisons correct.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "multcomp = multipletests(gdf.p, 0.05, method='fdr_bh')\n",
    "gdf.p = multcomp[1]\n",
    "\n",
    "## Save.\n",
    "# gdf_corr.to_csv(os.path.join(gt_dir, 'graph_metrics','regressed_metrics.csv'))\n",
    "\n",
    "## Significant differences between cohorts.\n",
    "sig_pd = gdf.where((gdf['p'] < 0.05) & (gdf.Independent == 'Cohort'))\n",
    "# sig_pd = gdf.where((gdf['p'] < 0.05))\n",
    "sig_pd = sig_pd.dropna()\n",
    "\n",
    "sig_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-27T17:24:04.651567",
     "start_time": "2017-03-27T17:24:04.506830"
    },
    "collapsed": false
   },
   "source": [
    "## Correlate global metrics with questionnaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute node-specific metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-20T17:33:47.936894",
     "start_time": "2017-03-20T17:33:47.628736"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Degree centrality, betweenness centrality, clustering coefficient, \n",
    "modularity (provincial hubs: highly connected to nodes in the same module, connector hubs: strongly connected to other modules), \n",
    "\n",
    "Calculating Participation Index to Identify Hub nodes: \n",
    "    Methods: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0001049 \n",
    "    See also: http://www.nature.com.ezp-prod1.hul.harvard.edu/nature/journal/v433/n7028/full/nature03288.html which says:\n",
    "        The participation coefficient P_i is close to 1 if its links are uniformly distributed among all the modules, and 0 if all its links are within its own module.\n",
    "'''\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Load graph metrics. \n",
    "metrics_pd = pd.read_csv(os.path.join(gt_dir, 'graph_metrics', 'metrics_by_density.csv'))\n",
    "metrics_pd = metrics_pd.set_index(metrics_pd.Subject_ID)\n",
    "metrics_pd.density = np.round(metrics_pd.density,2)\n",
    "\n",
    "## Threshold by density based on significant differences between cohorts. See Regress against site/age variables section.\n",
    "metrics_pd = metrics_pd.where(metrics_pd.density == 0.20).dropna()\n",
    "\n",
    "## Load regions.\n",
    "regions = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')]\n",
    "n_regions = len(regions)\n",
    "\n",
    "## Specify graph metrics to be computed. \n",
    "metrics = ['degree','degree_centrality','betweenness_centrality','clustering_coefficient','participation_index']\n",
    "\n",
    "## Matrix to store results.\n",
    "metrics_dat = np.zeros((metrics_pd.shape[0],len(metrics),n_regions))\n",
    "\n",
    "## Assign each node to a module (to calculate modularity of hub nodes below). \n",
    "part = defaultdict()\n",
    "\n",
    "for r in regions: \n",
    "    if 'Cont' in r:\n",
    "        val = 0\n",
    "    elif 'Default' in r: \n",
    "        val = 1\n",
    "    elif 'DorsAttn' in r: \n",
    "        val = 2\n",
    "    elif 'Limbic' in r: \n",
    "        val = 3\n",
    "    elif 'SomMot' in r: \n",
    "        val = 4\n",
    "    elif 'SalVentAttn' in r: \n",
    "        val = 5\n",
    "    elif 'TempPar' in r: \n",
    "        val = 4\n",
    "    elif 'Vis' in r: \n",
    "        val = 7\n",
    "    else: \n",
    "        val = 3\n",
    "    part[r] = val\n",
    "    \n",
    "## Main loop. \n",
    "for idx,subject,thresh in zip(np.arange(metrics_pd.shape[0]),metrics_pd.index,metrics_pd.threshold):\n",
    "    \n",
    "    ## Load data. \n",
    "    rtoz = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Create graph.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(regions)\n",
    "\n",
    "    for q,r,val in zip(np.tril_indices(126,k=-1)[0], np.tril_indices(126,k=-1)[1], rtoz):\n",
    "        if not np.isnan(val) and val >= thresh:\n",
    "            G.add_edge(regions[q],regions[r])\n",
    "\n",
    "    G.remove_nodes_from(G.nodes_with_selfloops())\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute metrics.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Node degree. \n",
    "    degrees = dict(nx.degree(G))\n",
    "    metrics_dat[idx,0] = degrees.values()\n",
    "    \n",
    "    ## Degree centrality.\n",
    "    metrics_dat[idx,1] = (nx.degree_centrality(G)).values()\n",
    "    \n",
    "    ## Betweenness centrality.\n",
    "    metrics_dat[idx,2] = (nx.betweenness_centrality(G)).values()\n",
    "    \n",
    "    ## Clustering coefficient. \n",
    "    metrics_dat[idx,3] = (nx.clustering(G)).values()\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Compute participation index (node modularity).\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Filter by high-degree nodes s.t. degree of node is >1 standard deviation above mean network degree. \n",
    "    zvals = (degrees.values() - np.mean(degrees.values())) / np.std(degrees.values())\n",
    "    for r_idx,r,z in zip(np.arange(len(regions)),regions, zvals):\n",
    "        if abs(zvals[r_idx]) < 1: \n",
    "            degrees[r] = 0    \n",
    "\n",
    "    ## Specify number of modules. \n",
    "    n_m = len(np.unique(part.values()))\n",
    "\n",
    "    ## Create empty matrix to store results. \n",
    "    n_edges_in_module = np.empty((len(degrees.keys()),len(np.unique(part.values()))))\n",
    "    \n",
    "    ## matrix to store participation indices. \n",
    "    p_index = np.zeros((len(list(degrees.keys()))))\n",
    "    \n",
    "    ## Iterate through nodes\n",
    "    for n_idx, node in enumerate(list(degrees.keys())): \n",
    "        \n",
    "        ## Only iterate through nodes with degree > 1 SD\n",
    "        if degrees[node] > 0:\n",
    "            ## Iterate through all modules\n",
    "            for m_idx, mod in enumerate(np.unique(part.values())): \n",
    "                count = 0 \n",
    "                ## Iterate though all edges connceted to that node\n",
    "                for e1 in zip(*list(G.edges(node)))[1]: \n",
    "                    ## Check if destination node in selected module\n",
    "                    if e1 in [k for k,v in part.iteritems() if v == mod]: \n",
    "                        count += 1\n",
    "                n_edges_in_module[n_idx,m_idx] = count\n",
    "        else: \n",
    "            n_edges_in_module[n_idx] = np.zeros((n_m))\n",
    "        \n",
    "        ## Calculate participation index. \n",
    "        p_index[n_idx] = 1 - sum((n_edges_in_module[n_idx] / degrees[node])**2)\n",
    "     \n",
    "    ## Add to results matrix. \n",
    "    metrics_dat[idx,4] = p_index\n",
    "\n",
    "## Reshape. \n",
    "metrics_dat = metrics_dat.reshape((metrics_pd.shape[0]*len(metrics),n_regions), order='C')\n",
    "\n",
    "## Prepare columns for dataframe. \n",
    "subject_ids = np.expand_dims(np.repeat(metrics_pd.Subject_ID,len(metrics)),axis=1)\n",
    "thresholds = np.expand_dims(np.repeat(metrics_pd.threshold,len(metrics)),axis=1)\n",
    "densities = np.expand_dims(np.repeat(metrics_pd.density,len(metrics)),axis=1)\n",
    "metrics_col = np.expand_dims(np.tile(metrics,len(metrics_pd.Subject_ID)),axis=1)\n",
    "\n",
    "## Convert to dataframe.\n",
    "node_pd = pd.DataFrame(np.hstack([subject_ids, thresholds, densities, metrics_col, metrics_dat]), \n",
    "                       columns=['Subject_ID','threshold','density','metrics'] + regions)\n",
    "\n",
    "## Save.\n",
    "node_pd.to_csv(os.path.join(gt_dir, 'graph_metrics', 'node_specific_metrics.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute differences between node-specific metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-27T15:37:59.326009",
     "start_time": "2017-03-27T15:37:54.134539"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Load node-specific metrics. \n",
    "metrics_dat = pd.read_csv(os.path.join(gt_dir, 'graph_metrics', 'node_specific_metrics.csv'))\n",
    "\n",
    "## Specify regions. \n",
    "regions = list(metrics_dat.columns[5:])\n",
    "\n",
    "## Compute difference between healthy and psychiatric patients for each graph metric\n",
    "for metric in metrics_dat.metrics: \n",
    "    \n",
    "    ## Constrain data to cohort and metric\n",
    "    pp_metrics = metrics_dat.loc[np.where((metrics_dat.cohort == 1) & (metrics_dat.metrics == 'participation_index'))]\n",
    "    hc_metrics = metrics_dat.loc[np.where((metrics_dat.cohort == 2) & (metrics_dat.metrics == 'participation_index'))]\n",
    "\n",
    "    ## Two-tailed independent t-test\n",
    "    tvals, pvals = ttest_ind(hc_metrics[regions],pp_metrics[regions])\n",
    "\n",
    "    ## FDR correction for multiple comparisons.\n",
    "    pvals = multipletests(pvals, 0.05, method='fdr_bh')[1]\n",
    "\n",
    "    ## Print metrics that are significantly different between cohorts\n",
    "    for i in np.where(pvals < 0.05)[0]:\n",
    "        print metric, tvals[i], pvals[i], regions[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlate node-specific metrics with questionnaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize node-specific metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-22T16:10:26.700557",
     "start_time": "2017-03-22T16:10:24.801422"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pylab as plt \n",
    "from collections import defaultdict \n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "n_subs = len(subjects)\n",
    "\n",
    "## Load data.\n",
    "metrics_pd = pd.read_csv(os.path.join(gt_dir, 'graph_metrics','node_specific_metrics.csv'))\n",
    "metrics_pd['cohort'] = [1 if subject in pps else 2 for subject in metrics_pd.Subject_ID]\n",
    "\n",
    "## Rename columns so they're easier to read. \n",
    "cols = list(metrics_pd.columns.copy())\n",
    "\n",
    "for i,col in enumerate(cols): \n",
    "    if 'Left' in col: \n",
    "        cols[i] = 'Limbic-%s-lh' %re.split('-',col)[1]\n",
    "    elif 'Right' in col: \n",
    "        cols[i] = 'Limbic-%s-rh' %re.split('-',col)[1]\n",
    "    elif 'lh.' in col or 'rh.' in col: \n",
    "        cols[i] = '-'.join(re.split('_|-',col)[2:])\n",
    "    else:\n",
    "        cols[i] = col\n",
    "        \n",
    "metrics_pd.columns = cols       \n",
    "\n",
    "## Remove SomatoMotor and Visual areas\n",
    "# cols = [c for c in cols if 'SomMot' not in c and 'Vis' not in c and 'TempPar' not in c]\n",
    "cols = [c for c in cols if c not in [u'Subject_ID', u'threshold', u'density', u'metrics', u'cohort',]]\n",
    "\n",
    "for metric in ['betweenness_centrality', 'clustering_coefficient', 'degree', 'degree_centrality', 'participation_index'][:1]: \n",
    "\n",
    "    for cohort_id, cohort_name in zip([1,2],['psychiatric','healthy'])[:1]:\n",
    "\n",
    "        ## Extract metric info from metrics dataframe, only for psychiatric subjects. \n",
    "        metricidx = metrics_pd.loc[np.where((metrics_pd.metrics == metric) & (metrics_pd.cohort==cohort_id))]\n",
    "            \n",
    "        for hemi in ['lh','rh'][:1]: \n",
    "            \n",
    "            ## Separate regions by hemisphere. \n",
    "            hemi_cols = [c for c in cols if hemi in c]\n",
    "            \n",
    "            ## Assign each node to a module. \n",
    "            part = defaultdict()\n",
    "            for r in hemi_cols: \n",
    "                if 'Cont' in r:\n",
    "                    val = 0\n",
    "                elif 'Default' in r: \n",
    "                    val = 1\n",
    "                elif 'DorsAttn' in r: \n",
    "                    val = 2\n",
    "                elif 'Limbic' in r: \n",
    "                    val = 3\n",
    "                elif 'SomMot' in r: \n",
    "                    val = 5\n",
    "                elif 'SalVentAttn' in r: \n",
    "                    val = 4\n",
    "                elif 'TempPar' in r: \n",
    "                    val = 5\n",
    "                elif 'Vis' in r: \n",
    "                    val = 6\n",
    "                else: \n",
    "                    val = 3\n",
    "                part[r] = val\n",
    "\n",
    "            ## Seprate regions by functional module. \n",
    "            for part_idx in np.arange(len(np.unique(part.values()))):\n",
    "                mod_cols = [k for (k,v) in part.iteritems() if v == part_idx]\n",
    "\n",
    "                ## Compute mean value of given metric per region across subjects. \n",
    "                mean_metric = dict.fromkeys(mod_cols)\n",
    "                for r,idx in zip(mod_cols, [np.nanmean(metricidx[col]) for col in metricidx[mod_cols]]):\n",
    "                    mean_metric[r] = idx\n",
    "\n",
    "                ## Sort dictionary of by participation index.  \n",
    "                pidx = [(k,mean_metric[k]) for k in sorted(mean_metric, key=mean_metric.get, reverse=True)]\n",
    "                _, vals = zip(*pidx)\n",
    "\n",
    "                ## Plotting. \n",
    "                # colors = ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3', '#a6d854', '#ffd92f', '#e5c494']\n",
    "                # colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00', '#ffff33', '#a65628']\n",
    "                colors = ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3', '#a6d854']\n",
    "\n",
    "                fig = plt.figure(figsize=(12,10))\n",
    "                barplot = plt.bar(np.arange(len(mod_cols)),vals)\n",
    "    #             for col_idx,col_name in enumerate([k for (k,v) in pidx]): \n",
    "    #                 barplot[col_idx].set_color(colors[part[col_name]])\n",
    "                plt.hlines(y=np.nanmean(mean_metric.values()), xmin=0, xmax=len(mod_cols), linestyles='dashed')\n",
    "\n",
    "                ## Create custom artists\n",
    "    #             custom_artists = []\n",
    "    #             for c in colors: \n",
    "    #                 custom_artists.append(plt.Line2D((0,1),(0,0), color=c))\n",
    "\n",
    "                ## Create legend from custom artist/label lists\n",
    "    #             plt.legend(custom_artists, ['Cont','Default','DorsAttn','Limbic','SalVentAttn'], fontsize=14)\n",
    "\n",
    "                plt.xticks(np.arange(len(mod_cols)), mod_cols, rotation='vertical')\n",
    "                plt.ylabel(metric)\n",
    "                plt.title('Mean %s (%s)' %(metric,cohort_name), fontsize=24)\n",
    "                plt.tight_layout()\n",
    "    #             plt.savefig(os.path.join(gt_dir, 'plots','graph_metrics','mean_%s_%s_%s.png' %(metric,cohort_name,hemi)))\n",
    "    #             plt.close()\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create null networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-17T13:52:01.742545",
     "start_time": "2017-03-17T13:52:01.596765"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy.io as sio\n",
    "from itertools import permutations\n",
    "\n",
    "## Define functions. \n",
    "def efficiency(G, u, v):\n",
    "    return 1. / nx.shortest_path_length(G, u, v)\n",
    "\n",
    "def global_efficiency(G):\n",
    "    n = len(G)\n",
    "    denom = n * (n - 1)\n",
    "    return sum(efficiency(G, u, v) for u, v in permutations(G, 2)) / denom\n",
    "\n",
    "def local_efficiency(G):\n",
    "    return sum(global_efficiency(nx.ego_graph(G, v)) for v in G) / len(G)\n",
    "\n",
    "def null_covariance(W, sd):\n",
    "    '''\n",
    "Uses the Hirschberg-Qi-Steuer algorithm to generate a null correlation matrix\n",
    "appropriate for use as a null model that is matched to the given correlation \n",
    "matrix in node mean and variance, as well as average covariance.\n",
    "Inputs:\tW, the NxN adjacency matrix which should be a correlation matrix of\n",
    "                    R timepoints or observations\n",
    "            sd, An Nx1 vector of standard deviations for each ROI timeseries \n",
    "Output: C, a null model adjacency matrix\n",
    "see Zalesky et al. (2012) \"On the use of correlation as a measure of network\n",
    "    connectivity\"\n",
    "Source: https://github.com/CarloNicolini/pyconnectivity/blob/master/bct_sbx.py\n",
    "    '''\n",
    "    n = len(W)                            ## size of matrix\n",
    "    sdd = np.diag(sd)                     ## create diagonal matrix from std of original matrix\n",
    "    w = np.dot(sdd, np.dot(W, sdd))       ## multiply std by (W * std) == create variance-covariance matrix\n",
    "    e = np.mean(np.triu(w, 1))            ## mean of off-diagonal elements \n",
    "    v = np.var(np.triu(w, 1))             ## variance of off-diagonal elements\n",
    "    ed = np.mean(np.diag(w))              ## mean of diagonal elements\n",
    "\n",
    "    m = max(2, np.floor((e ** 2 - ed ** 2) / v))\n",
    "    mu = np.sqrt(e / m)\n",
    "    sigma = np.sqrt(-mu ** 2 + np.sqrt(mu ** 4 + v / m))\n",
    "\n",
    "    from scipy import stats\n",
    "    x = stats.norm.rvs(loc=mu, scale=sigma, size=(n, m))\n",
    "    c = np.dot(x, x.T)\n",
    "    a = np.diag(1 / np.diag(c))\n",
    "    return np.dot(a, np.dot(c, a))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "## Create null correlation matrix. \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "subjects = pps + hcs\n",
    "\n",
    "## Load regions.\n",
    "regions = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')]\n",
    "n_regions = len(regions)\n",
    "\n",
    "## Specify parameters. \n",
    "metrics = ['null_density', 'null_clustering_coefficient', 'null_avg_path_length']\n",
    "density_range = [0.3,0.1]\n",
    "\n",
    "## Load graph metrics.\n",
    "metrics_pd = pd.read_csv(os.path.join(gt_dir, 'graph_metrics', 'metrics_by_density.csv'), usecols=['Subject_ID','threshold','density'])\n",
    "\n",
    "## Constrain by density values. \n",
    "metrics_pd.density = np.round(metrics_pd.density, 2)\n",
    "metrics_pd = metrics_pd.where(((metrics_pd.density<=density_range[0])&(metrics_pd.density>=density_range[1])))\n",
    "metrics_pd = metrics_pd.drop_duplicates(['Subject_ID','density'])\n",
    "metrics_pd = metrics_pd.dropna()\n",
    "\n",
    "## Matrix to store data. \n",
    "metrics_dat = np.zeros((metrics_pd.shape[0], 5))\n",
    "\n",
    "## Main loop. \n",
    "for idx, subject, thresh in zip(np.arange(metrics_pd.shape[0]),metrics_pd.Subject_ID,metrics_pd.threshold):\n",
    "        \n",
    "    ## Load original correlation matrix. \n",
    "    rvals = np.load(os.path.join(gt_dir, 'correlations', '%s_rtoz.npz' %subject))['zmat']\n",
    "\n",
    "    ## Convert to square matrix.\n",
    "    rtoz = np.zeros((126,126))\n",
    "    rtoz[np.triu_indices(126,k=1)] = rvals\n",
    "    rtoz[np.tril_indices(126,k=-1)] = rvals\n",
    "    np.fill_diagonal(rtoz, 1)\n",
    "\n",
    "    ## Load regressed timeseries. \n",
    "    regressed = np.load(os.path.join(gt_dir, 'regressed', '%s_regressed.npz' %subject))['regressed']\n",
    "    sd = np.std(regressed,1)\n",
    "\n",
    "    ## Create null correlation matrices. \n",
    "    clustering_coeffs = np.zeros((100))\n",
    "    avg_path_lengths = np.zeros((100))\n",
    "    densities = np.zeros((100))\n",
    "    total_edges = np.zeros((100))\n",
    "\n",
    "    for p in np.arange(100): \n",
    "        nullmat = null_covariance(rtoz, sd)\n",
    "\n",
    "        ## Restrict to lower triangular matrix. \n",
    "        nullmat = nullmat[np.tril_indices_from(nullmat,k=-1)]\n",
    "\n",
    "        ## Remove negative correlations. \n",
    "        nullmat = np.where(nullmat<0,np.nan,nullmat)\n",
    "\n",
    "        # Fisher's r-to-z transform.\n",
    "        nullmat = np.arctanh(nullmat)\n",
    "\n",
    "        ## Remove inf created by correlation values >= 1.\n",
    "        nullmat = np.where(nullmat==np.inf,np.nan,nullmat)\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Create graph.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(regions)\n",
    "\n",
    "        for q,r,val in zip(np.tril_indices(126,k=-1)[0], np.tril_indices(126,k=-1)[1], nullmat):\n",
    "            if not np.isnan(val) and val>=thresh:\n",
    "                G.add_edge(regions[q],regions[r])\n",
    "\n",
    "        G.remove_nodes_from(G.nodes_with_selfloops())\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute graph metrics.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        n_nodes = nx.number_of_nodes(G)\n",
    "        n_edges = nx.number_of_edges(G)\n",
    "\n",
    "        max_edges = (n_nodes*(n_nodes-1))/2\n",
    "        avg_degree = sum(nx.degree(G).values()) / n_nodes  \n",
    "\n",
    "        n_components = nx.number_connected_components(G)\n",
    "        largest_cc = max(nx.connected_component_subgraphs(G), key=len)\n",
    "\n",
    "        total_edges[p] = n_edges\n",
    "        densities[p] = np.round(nx.density(G),2)\n",
    "\n",
    "        ## Global metrics.\n",
    "        clustering_coeffs[p] = np.round(nx.average_clustering(G),3)\n",
    "\n",
    "        if n_nodes > n_components > 1: \n",
    "            avg_path_lengths[p] = np.round(nx.average_shortest_path_length(largest_cc),3)\n",
    "        elif n_components == 1: \n",
    "            avg_path_lengths[p] = np.round(nx.average_shortest_path_length(G),3)\n",
    "        else: \n",
    "            continue\n",
    "    ## Average metrics across null networks and store.\n",
    "    metrics_dat[idx] = np.array([thresh, np.mean(total_edges), np.mean(densities), np.mean(clustering_coeffs), np.mean(avg_path_lengths)])\n",
    "\n",
    "print 'Done.' \n",
    "\n",
    "## Convert to dataframe. \n",
    "null_metrics_pd = pd.DataFrame(np.hstack([np.expand_dims(metrics_pd.Subject_ID,1), metrics_dat]), \n",
    "                               columns=['Subject_ID','threshold','n_edges','density','clustering_coefficient','avg_path_length'])\n",
    "## Save.\n",
    "null_metrics_pd.to_csv(os.path.join(gt_dir, 'graph_metrics', 'null_metrics_by_threshold.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate small-world index. (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-02T16:36:05.488385",
     "start_time": "2017-03-02T16:36:05.123896"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Load graph metrics from correlation matrices. \n",
    "sub_metrics = pd.read_csv(os.path.join(gt_dir, 'graph_metrics', 'metrics_by_density.csv'), \n",
    "              usecols=['Subject_ID', 'threshold', 'n_edges', 'density', 'clustering_coefficient', 'avg_path_length'])\n",
    "sub_metrics.density = np.round(sub_metrics.density, 2)\n",
    "\n",
    "## Load graph metrics from null matrices. \n",
    "null_metrics = pd.read_csv(os.path.join(gt_dir, 'graph_metrics', 'null_metrics_by_threshold.csv'),\n",
    "               usecols=['Subject_ID', 'threshold', 'n_edges', 'density', 'clustering_coefficient', 'avg_path_length'])\n",
    "null_metrics.columns = ['Subject_ID', 'threshold', 'null_n_edges', 'null_density', 'null_clustering_coefficient', 'null_avg_path_length']\n",
    "\n",
    "## Merge dataframes on subject_id and threshold.\n",
    "merged = pd.merge(sub_metrics,null_metrics)\n",
    "\n",
    "density_range = [0.3, 0.1]\n",
    "\n",
    "temp = null_metrics.where((null_metrics.null_density<=0.3) & (null_metrics.null_density>=0.1))\n",
    "temp = temp.dropna()\n",
    "\n",
    "temp.columns = ['Subject_ID', 'null_threshold', 'null_n_edges', 'density', 'null_clustering_coefficient', 'null_avg_path_length']\n",
    "temp_merged = pd.merge(sub_metrics, temp)\n",
    "\n",
    "for t in np.unique(merged.threshold): \n",
    "    \n",
    "    g = merged.clustering_coefficient[merged.threshold == t] / merged.null_clustering_coefficient[merged.threshold == t]\n",
    "    l = merged.avg_path_length[merged.threshold == t] / merged.null_avg_path_length[merged.threshold == t]\n",
    "    s = g / l\n",
    "    \n",
    "    print s.mean()\n",
    "\n",
    "## Plotting. \n",
    "fig, axes = plt.subplots(3, sharex=True, sharey=True, figsize=(12,12))\n",
    "\n",
    "axes[0].scatter(np.unique(metrics_pd.density),gamma)\n",
    "axes[0].plot(np.unique(metrics_pd.density),gamma)\n",
    "axes[0].set_ylabel('gamma', fontsize=16)\n",
    "axes[0].set_title('Clustering Coefficient Ratio',fontsize=20)\n",
    "\n",
    "axes[1].scatter(np.unique(metrics_pd.density),_lambda)\n",
    "axes[1].plot(np.unique(metrics_pd.density),_lambda)\n",
    "axes[1].set_ylabel('lambda', fontsize=16)\n",
    "axes[1].set_title('Average Shorted Path Length Ratio',fontsize=20)\n",
    "\n",
    "axes[2].scatter(np.unique(metrics_pd.density),sigma)\n",
    "axes[2].plot(np.unique(metrics_pd.density),sigma)\n",
    "\n",
    "axes[2].axhline(y=1, xmin=0, xmax=1, linewidth=1, color = 'k', linestyle='--')\n",
    "plt.xlabel('Density', fontsize=16)\n",
    "axes[2].set_ylabel('sigma', fontsize=16)\n",
    "axes[2].set_title('Small-world Index (Psychiatric vs. Healthy)',fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/autofs/space/will_003/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/graph_metrics/small_world.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T16:11:07.033964",
     "start_time": "2017-03-06T16:11:06.983502"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from scipy.stats import chi2 \n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "''' \n",
    "Sex: 0 male, 1 female\n",
    "\n",
    "Race: \n",
    "0, no, White_x000D_\n",
    "1, no, Black or African American_x000D_\n",
    "2, no, Asian_x000D_\n",
    "3, no, American Indian or Alaska Native_x000D_\n",
    "4, no, Native Hawaiian or Other Pacific Islander_x000D_\n",
    "5, no, Other_x000D_\n",
    "6, no, Unknown \n",
    "\n",
    "Sites: {'Columbia':1, 'Harvard':2, 'Michigan': 3, 'UTSW': 4}\n",
    "'''\n",
    "\n",
    "## Specify directories. \n",
    "behav_dir = '/autofs/space/will_003/users/EMBARC/behavior/assessments_raw'\n",
    "\n",
    "## Load data. \n",
    "dem = pd.read_csv(os.path.join(behav_dir, 'Demographics.csv'), usecols=['sample','ProjectSpecificID','SiteName','sex','hispa','race','age_evaluation'])\n",
    "dem = dem.sort_values(by='ProjectSpecificID')\n",
    "\n",
    "## Restrict to subjects of interest. \n",
    "dem = dem.where(dem.ProjectSpecificID[np.in1d(dem.ProjectSpecificID, [s[:6] for s in subjects])])\n",
    "dem = dem.dropna()\n",
    "\n",
    "## Separate by group. \n",
    "pps = (dem.where(dem['sample'] == 1)).dropna()\n",
    "hcs = (dem.where(dem['sample'] == 2)).dropna()\n",
    "\n",
    "## T-test to compare age across cohorts\n",
    "print ttest_ind(hcs.age_evaluation, pps.age_evaluation)\n",
    "\n",
    "## Chi-squared Goodness-of-fit test for sex across cohorts\n",
    "observed = pd.crosstab(index=pps.sex, columns=\"count\")\n",
    "expected = (pd.crosstab(index=hcs.sex, columns=\"count\") / len(hcs)) * len(pps)\n",
    "\n",
    "print chisquare(f_obs = observed, f_exp = expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Questionnaires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-27T15:44:09.108827",
     "start_time": "2017-03-27T15:44:08.967680"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv \n",
    "\n",
    "def normalize(arr):\n",
    "    return [( float(i) - min(arr) )/( max(arr) - min(arr)) for i in arr] \n",
    "\n",
    "## Load questionnaires. \n",
    "csv = read_csv('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/questionnaires/subtypes_qscores.csv')\n",
    "\n",
    "## Restrict to questionnaires of interest.\n",
    "csv.columns = ['Subject_ID', 'Diagnosis','MASQ_AA', 'SHAPS_Cont', 'AAQ']\n",
    "csv = csv.set_index('Subject_ID', drop=True)\n",
    "\n",
    "## Restrict to psychiatric subjects. \n",
    "csv = csv[csv['Diagnosis'] == 1]\n",
    "\n",
    "## Remove subjects with corrupted data.\n",
    "csv = csv[csv.index != 'CU0068CUMR1R1'] ## need to re-proproc\n",
    "\n",
    "## Remove subjects with missing data.\n",
    "csv = csv.dropna()\n",
    "subjects = list(csv.index)\n",
    "\n",
    "## Create questionnaire matrix. \n",
    "qmat = np.vstack([csv['MASQ_AA'], csv['SHAPS_Cont'], csv['AAQ']])\n",
    "\n",
    "## Normalize questionnaires.\n",
    "for i in np.arange(qmat.shape[0]):\n",
    "    qmat[i] = normalize(qmat[i])\n",
    "    \n",
    "## Add intercept column. \n",
    "intercept = np.array([1]*len(subjects))\n",
    "qmat = np.vstack([intercept,qmat])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "929px",
   "left": "0px",
   "right": "1708px",
   "top": "106px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
