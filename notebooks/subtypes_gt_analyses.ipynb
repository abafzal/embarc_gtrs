{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev2 toc-item\"><a href=\"#Preprocess.\" data-toc-modified-id=\"Preprocess.-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Preprocess.</a></div><div class=\"lev3 toc-item\"><a href=\"#Load-Labels.\" data-toc-modified-id=\"Load-Labels.-0.1.1\"><span class=\"toc-item-num\">0.1.1&nbsp;&nbsp;</span>Load Labels.</a></div><div class=\"lev3 toc-item\"><a href=\"#Load-Data.-Average-by-Labels.-(DONE)\" data-toc-modified-id=\"Load-Data.-Average-by-Labels.-(DONE)-0.1.2\"><span class=\"toc-item-num\">0.1.2&nbsp;&nbsp;</span>Load Data. Average by Labels. (DONE)</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-average-signal-by-label.\" data-toc-modified-id=\"Plot-average-signal-by-label.-0.1.3\"><span class=\"toc-item-num\">0.1.3&nbsp;&nbsp;</span>Plot average signal by label.</a></div><div class=\"lev4 toc-item\"><a href=\"#Save-out-ratio-signal.-(DONE)\" data-toc-modified-id=\"Save-out-ratio-signal.-(DONE)-0.1.3.1\"><span class=\"toc-item-num\">0.1.3.1&nbsp;&nbsp;</span>Save out ratio signal. (DONE)</a></div><div class=\"lev4 toc-item\"><a href=\"#Plot-ratio-signal.\" data-toc-modified-id=\"Plot-ratio-signal.-0.1.3.2\"><span class=\"toc-item-num\">0.1.3.2&nbsp;&nbsp;</span>Plot ratio signal.</a></div><div class=\"lev3 toc-item\"><a href=\"#Filter-and-Nuisance-Regress.-(Includes-GSR)\" data-toc-modified-id=\"Filter-and-Nuisance-Regress.-(Includes-GSR)-0.1.4\"><span class=\"toc-item-num\">0.1.4&nbsp;&nbsp;</span>Filter and Nuisance Regress. (Includes GSR)</a></div><div class=\"lev3 toc-item\"><a href=\"#Filter-and-Nuisance-Regress.-(NOT-including-GSR)\" data-toc-modified-id=\"Filter-and-Nuisance-Regress.-(NOT-including-GSR)-0.1.5\"><span class=\"toc-item-num\">0.1.5&nbsp;&nbsp;</span>Filter and Nuisance Regress. (NOT including GSR)</a></div><div class=\"lev3 toc-item\"><a href=\"#Calculate-SNR.\" data-toc-modified-id=\"Calculate-SNR.-0.1.6\"><span class=\"toc-item-num\">0.1.6&nbsp;&nbsp;</span>Calculate SNR.</a></div><div class=\"lev2 toc-item\"><a href=\"#Calculate-and-plot-motion.\" data-toc-modified-id=\"Calculate-and-plot-motion.-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>Calculate and plot motion.</a></div><div class=\"lev3 toc-item\"><a href=\"#Calculate-functional-displacement-and-other-motion-parameters.\" data-toc-modified-id=\"Calculate-functional-displacement-and-other-motion-parameters.-0.2.1\"><span class=\"toc-item-num\">0.2.1&nbsp;&nbsp;</span>Calculate functional displacement and other motion parameters.</a></div><div class=\"lev3 toc-item\"><a href=\"#Mask-grey-and-white-matter-timeseries.\" data-toc-modified-id=\"Mask-grey-and-white-matter-timeseries.-0.2.2\"><span class=\"toc-item-num\">0.2.2&nbsp;&nbsp;</span>Mask grey and white matter timeseries.</a></div><div class=\"lev3 toc-item\"><a href=\"#Prepare-motion-and-masked-fMRI-data.-Plot.\" data-toc-modified-id=\"Prepare-motion-and-masked-fMRI-data.-Plot.-0.2.3\"><span class=\"toc-item-num\">0.2.3&nbsp;&nbsp;</span>Prepare motion and masked fMRI data. Plot.</a></div><div class=\"lev2 toc-item\"><a href=\"#Correlate.\" data-toc-modified-id=\"Correlate.-0.3\"><span class=\"toc-item-num\">0.3&nbsp;&nbsp;</span>Correlate.</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-z-scores-for-group.\" data-toc-modified-id=\"Plot-z-scores-for-group.-0.3.1\"><span class=\"toc-item-num\">0.3.1&nbsp;&nbsp;</span>Plot z-scores for group.</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-Correlation-Heatmap.\" data-toc-modified-id=\"Plot-Correlation-Heatmap.-0.3.2\"><span class=\"toc-item-num\">0.3.2&nbsp;&nbsp;</span>Plot Correlation Heatmap.</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-correlation-distribution-(GSR-vs.-noGSR).\" data-toc-modified-id=\"Plot-correlation-distribution-(GSR-vs.-noGSR).-0.3.3\"><span class=\"toc-item-num\">0.3.3&nbsp;&nbsp;</span>Plot correlation distribution (GSR vs. noGSR).</a></div><div class=\"lev2 toc-item\"><a href=\"#Network-Analysis-(GSR).\" data-toc-modified-id=\"Network-Analysis-(GSR).-0.4\"><span class=\"toc-item-num\">0.4&nbsp;&nbsp;</span>Network Analysis (GSR).</a></div><div class=\"lev3 toc-item\"><a href=\"#Compute-network-thresholds.\" data-toc-modified-id=\"Compute-network-thresholds.-0.4.1\"><span class=\"toc-item-num\">0.4.1&nbsp;&nbsp;</span>Compute network thresholds.</a></div><div class=\"lev3 toc-item\"><a href=\"#Create-thresholded-networks.\" data-toc-modified-id=\"Create-thresholded-networks.-0.4.2\"><span class=\"toc-item-num\">0.4.2&nbsp;&nbsp;</span>Create thresholded networks.</a></div><div class=\"lev2 toc-item\"><a href=\"#Network-Construction-(without-GSR).\" data-toc-modified-id=\"Network-Construction-(without-GSR).-0.5\"><span class=\"toc-item-num\">0.5&nbsp;&nbsp;</span>Network Construction (without GSR).</a></div><div class=\"lev3 toc-item\"><a href=\"#Create-network.\" data-toc-modified-id=\"Create-network.-0.5.1\"><span class=\"toc-item-num\">0.5.1&nbsp;&nbsp;</span>Create network.</a></div><div class=\"lev3 toc-item\"><a href=\"#Compute-global-network-metrics.\" data-toc-modified-id=\"Compute-global-network-metrics.-0.5.2\"><span class=\"toc-item-num\">0.5.2&nbsp;&nbsp;</span>Compute global network metrics.</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-distribution-of-global-metrics.-(GSR-vs.-noGSR)\" data-toc-modified-id=\"Plot-distribution-of-global-metrics.-(GSR-vs.-noGSR)-0.5.3\"><span class=\"toc-item-num\">0.5.3&nbsp;&nbsp;</span>Plot distribution of global metrics. (GSR vs. noGSR)</a></div><div class=\"lev2 toc-item\"><a href=\"#Load-Questionnaires.\" data-toc-modified-id=\"Load-Questionnaires.-0.6\"><span class=\"toc-item-num\">0.6&nbsp;&nbsp;</span>Load Questionnaires.</a></div><div class=\"lev2 toc-item\"><a href=\"#Healthy-Subjects\" data-toc-modified-id=\"Healthy-Subjects-0.7\"><span class=\"toc-item-num\">0.7&nbsp;&nbsp;</span>Healthy Subjects</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T13:00:45.823145",
     "start_time": "2017-02-21T13:00:43.819008"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from mne import read_label\n",
    "from pandas import read_csv\n",
    "from pandas import read_table\n",
    "\n",
    "## Specify directories and parameters.\n",
    "fast_dir = '/space/will/3/users/EMBARC/EMBARC-FAST'\n",
    "fsd = 'rest'\n",
    "run = '001'\n",
    "\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "### Load cortical labels.\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "\n",
    "parc_dir = '/space/lilli/1/users/DARPA-Recons/fscopy/label/yeo114_orig'\n",
    "lh_labels = [read_label(os.path.join(parc_dir, l)) for l in sorted(os.listdir(parc_dir)) if 'LH' in l]\n",
    "rh_labels = [read_label(os.path.join(parc_dir, l)) for l in sorted(os.listdir(parc_dir)) if 'RH' in l]\n",
    "\n",
    "labels = np.append(lh_labels,rh_labels)\n",
    "\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "### Load subcortical labels.\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "\n",
    "## Read Freesurfer Color Lookup Table. \n",
    "rois = ['Left-Accumbens-area', 'Left-Thalamus-Proper', 'Left-Caudate', 'Left-Putamen', 'Left-Hippocampus', 'Left-Amygdala', \n",
    "        'Right-Accumbens-area', 'Right-Thalamus-Proper', 'Right-Caudate', 'Right-Putamen', 'Right-Hippocampus', 'Right-Amygdala']\n",
    "\n",
    "subcort_labels = dict.fromkeys([l for l in rois])\n",
    "\n",
    "clut = read_table(os.path.join('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/labels', 'FreeSurferColorLUT_truncated.txt'), \n",
    "                                              sep=' *', skiprows=4, names=['No', 'Label','R','G','B','A'], engine='python')\n",
    "clut = clut.loc[np.where(np.in1d(clut.Label,rois))]\n",
    "\n",
    "## Load subcortical segmentation.\n",
    "aseg_obj = nib.load('/space/lilli/1/users/DARPA-Recons/fsaverage/mri.2mm/aseg.mgz')\n",
    "aseg_dat = aseg_obj.get_data()\n",
    "\n",
    "for l in rois: subcort_labels[l] = np.where(aseg_dat == int(clut.No[clut.Label == l]))\n",
    "print 'cortical lh: %d,' %len(lh_labels), 'rh: %d\\n' %len(rh_labels), 'subcortical: %d' %len(subcort_labels)\n",
    "\n",
    "## Save list of regions. \n",
    "regions = np.array([l.name for l in labels] + rois)\n",
    "np.savetxt(os.path.join(fast_dir, 'scripts', 'subtypes_gt', 'gt_regions'), regions, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data. Average by Labels. (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T13:19:24.775017",
     "start_time": "2017-02-21T13:01:34.106725"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from pandas import DataFrame\n",
    "from scipy.stats import pearsonr\n",
    "from mne.filter import band_pass_filter\n",
    "\n",
    "## Define functions and directories.\n",
    "def zscore(arr): return (arr - arr.mean()) / arr.std()\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Define matrices.\n",
    "n_regions = len(labels) + len(subcort_labels)\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "\n",
    "# ## Subjects with manual coregistration.\n",
    "# subjects = ['CU0092CUMR1R1', 'CU0087CUMR1R1', 'CU0095CUMR1R1', 'CU0104CUMR1R1', 'CU0131CUMR1R1', 'CU0069CUMR1R1', \n",
    "#             'CU0025CUMR1R1', 'CU0094CUMR1R1', 'CU0067CUMR1R1', 'CU0106CUMR1R1', 'CU0009CUMR1R1', 'CU0105CUMR1R1', \n",
    "#             'CU0070CUMR1R1', 'CU0129CUMR1R1', 'CU0113CUMR1R1', 'CU0125CUMR1R1', 'CU0133CUMR1R1']\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print i, subject, \n",
    "\n",
    "    ## Load cortical/subcortical data.\n",
    "    lh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'lh'))).get_data().squeeze()\n",
    "    rh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'rh'))).get_data().squeeze()\n",
    "    sc_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.%s.2mm.nii.gz' %('mni305')))).get_data()\n",
    "    \n",
    "    ## Drop first 4 volumes. \n",
    "    lh_dat = np.delete(lh_dat, np.arange(4), axis=1)\n",
    "    rh_dat = np.delete(rh_dat, np.arange(4), axis=1)\n",
    "    sc_dat = np.delete(sc_dat, np.arange(4), axis=3)\n",
    "\n",
    "    n_times = sc_dat.shape[3]\n",
    "    n_verts = lh_dat.shape[0]\n",
    "    \n",
    "    ## Remove any extra timepoints collected.\n",
    "    if n_times > 176:\n",
    "        print 'Removing last %d timepoints for %s' %(n_times-176,subject)\n",
    "        lh_dat = np.delete(lh_dat, np.arange(176,n_times), axis=1)\n",
    "        rh_dat = np.delete(rh_dat, np.arange(176,n_times), axis=1)\n",
    "        sc_dat = np.delete(sc_dat, np.arange(176,n_times), axis=3)\n",
    "        n_times = sc_dat.shape[3]\n",
    "            \n",
    "    elif n_times < 176: \n",
    "        print 'Mismatch in number of timepoints. %s, n_times: %d. Missing %d timepoints.' %(subject, n_times, 176-n_times)\n",
    "        break\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Average by labels.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "    mean_dat = np.zeros((n_regions,n_times))\n",
    "    \n",
    "    for idx, label in enumerate(lh_labels):\n",
    "        mean_dat[idx] = lh_dat[label.vertices].mean(axis=0)\n",
    "        \n",
    "    for idx, label in enumerate(rh_labels): \n",
    "        mean_dat[idx+len(lh_labels)] = rh_dat[label.vertices].mean(axis=0)\n",
    "            \n",
    "    for idx,k in zip(np.arange(len(labels), n_regions),subcort_labels.keys()):\n",
    "        x,y,z = subcort_labels[k]\n",
    "        mean_dat[idx] = sc_dat[x,y,z].mean(axis=0)\n",
    "        \n",
    "    ## Save data averaged by labels. \n",
    "    np.savez_compressed(os.path.join(gt_dir, 'raw', '%s_raw' %subject), mean_dat=mean_dat)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T16:09:31.472985",
     "start_time": "2017-02-02T16:09:31.470878"
    }
   },
   "source": [
    "### Plot average signal by label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save out ratio signal. (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T13:41:56.527190",
     "start_time": "2017-02-21T13:41:44.469311"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "\n",
    "def get_signal_ratio(arr, idx): return np.count_nonzero(arr[idx]) / float(arr[idx].shape[0] * arr[idx].shape[1])\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "n_regions = len(labels) + len(subcort_labels)\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "subjects += [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "\n",
    "sig_ratio = np.zeros((len(subjects), n_regions))\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print i, subject, \n",
    "\n",
    "    ## Load cortical/subcortical data.\n",
    "    lh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'lh'))).get_data().squeeze()\n",
    "    rh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'rh'))).get_data().squeeze()\n",
    "    sc_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.%s.2mm.nii.gz' %('mni305')))).get_data()\n",
    "    \n",
    "    ## Drop first 4 volumes. \n",
    "    lh_dat = np.delete(lh_dat, np.arange(4), axis=1)\n",
    "    rh_dat = np.delete(rh_dat, np.arange(4), axis=1)\n",
    "    sc_dat = np.delete(sc_dat, np.arange(4), axis=3)\n",
    "\n",
    "    n_times = lh_dat.shape[1]\n",
    "    n_verts = lh_dat.shape[0]\n",
    "    \n",
    "    ## Remove any extra timepoints collected.\n",
    "    if n_times > 176:\n",
    "        print 'Removing last %d timepoints for %s' %(n_times-176,subject)\n",
    "        lh_dat = np.delete(lh_dat, np.arange(176,n_times), axis=1)\n",
    "        rh_dat = np.delete(rh_dat, np.arange(176,n_times), axis=1)\n",
    "        sc_dat = np.delete(sc_dat, np.arange(176,n_times), axis=3)\n",
    "        n_times = lh_dat.shape[1]\n",
    "            \n",
    "    elif n_times < 176: \n",
    "        print 'Mismatch in number of timepoints. %s, n_times: %d. Missing %d timepoints.' %(subject, n_times, 176-n_times)\n",
    "        break\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Calcualate label signal ratio.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "    \n",
    "    sub_ratio = np.zeros((n_regions))\n",
    "    \n",
    "    for idx, label in enumerate(lh_labels):\n",
    "        sub_ratio[idx] = get_signal_ratio(lh_dat, label.vertices)\n",
    "        \n",
    "    for idx, label in enumerate(rh_labels): \n",
    "        sub_ratio[idx+len(lh_labels)] = get_signal_ratio(rh_dat, label.vertices)\n",
    "            \n",
    "    for idx,k in zip(np.arange(len(labels), n_regions),subcort_labels.keys()):\n",
    "        x,y,z = subcort_labels[k]\n",
    "        sub_ratio[idx] = get_signal_ratio(sc_dat, [x,y,z])\n",
    "        \n",
    "    sig_ratio[i] = sub_ratio\n",
    "\n",
    "## Save. \n",
    "np.save(os.path.join(gt_dir, 'signal_ratio'), sig_ratio)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ratio signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T13:40:36.470080",
     "start_time": "2017-02-21T13:40:35.684000"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pylab as plt \n",
    "\n",
    "rs = np.load(os.path.join(gt_dir, 'signal_ratio.npy')).T\n",
    "rs = rs * 100\n",
    "rs_mean = np.mean(rs, axis=1)\n",
    "\n",
    "label_names = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_cortical_labels\", 'r')])\n",
    "\n",
    "## Scatterplot for percent signal across subjects. \n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "y = (rs_mean)\n",
    "mask = np.where(y<99.9)[0]\n",
    "colors = np.where(y<99.9, '#d7191c', '#2b83ba')\n",
    "\n",
    "plt.scatter(np.arange(rs_mean.shape[0]),y,c=colors)\n",
    "\n",
    "for i,j,k in zip(label_names[mask], mask, y[mask]): \n",
    "    plt.annotate('%s: %.2f' %('_'.join(i.split('_')[2:4]),y[j]), (j,k))\n",
    "\n",
    "plt.title('Percent label signal', fontsize=18)\n",
    "plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/percent_signal_labels.png')\n",
    "plt.close()\n",
    "\n",
    "## Plot histogram for each label. \n",
    "for r in mask: \n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    \n",
    "    plt.hist(rs[r]*100, normed=True)\n",
    "    plt.title(label_names[r])\n",
    "    plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/%s.png' %label_names[r])\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Nuisance Regress. (Includes GSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T13:42:37.658407",
     "start_time": "2017-02-21T13:42:25.700625"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from pandas import DataFrame\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import detrend\n",
    "from sklearn.decomposition import PCA\n",
    "from mne.filter import band_pass_filter\n",
    "\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print i, subject\n",
    "    \n",
    "    ## Load data.\n",
    "    mean_dat = np.load(os.path.join(gt_dir, 'raw', '%s_raw.npz' %subject))['mean_dat']\n",
    "    \n",
    "    ## Remove first 4 timepoints from data.\n",
    "    mean_dat = np.delete(mean_dat, np.arange(4), axis=1)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Perform Nuissance Regression and Filtering.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    n_components = 5        # No. of components to use from PCA.\n",
    "\n",
    "    ## Load global signal regressors. \n",
    "    gs = os.path.join(fast_dir, subject,fsd, run, 'global.waveform.dat')\n",
    "    gs = read_table(gs, sep=' *', header=None, engine='python').as_matrix()\n",
    "    \n",
    "    ## Load wm and csf regressors. \n",
    "    wm = read_table(os.path.join(fast_dir, subject,fsd, run, 'wm.dat'), sep=' *', header=None, engine='python')\n",
    "    vcsf = read_table(os.path.join(fast_dir, subject,fsd, run, 'vcsf.dat'), sep=' *', header=None, engine='python')\n",
    "    \n",
    "    ## Keep only top 5 components.\n",
    "    n_components = 5 \n",
    "    wm = wm[wm.columns[:n_components]].as_matrix()\n",
    "    vcsf = vcsf[vcsf.columns[:n_components]].as_matrix()\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if gs.shape[0] > 176: gs = np.delete(gs, np.arange(176,gs.shape[0]), axis=0)\n",
    "    if wm.shape[0] > 176: wm = np.delete(wm, np.arange(176,wm.shape[0]), axis=0)\n",
    "    if vcsf.shape[0] > 176: vcsf = np.delete(vcsf, np.arange(176,vcsf.shape[0]), axis=0)\n",
    "    \n",
    "    ## Load motion regressors.\n",
    "    mc = os.path.join(fast_dir, subject, fsd, run, 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:7]\n",
    "    if mc.shape[0] > 176:\n",
    "        mc = np.delete(mc, np.arange(176,mc.shape[0]), axis=0)\n",
    "\n",
    "    ## Demean/detrend data.\n",
    "    mc = detrend(mc, axis=0, type='constant')\n",
    "    mc = detrend(mc, axis=0, type='linear') ## Keeps breaking\n",
    "\n",
    "    ## Construct basis sets.\n",
    "    mc = np.concatenate([mc, np.roll(mc, -1, 0)], axis=-1)\n",
    "    mc = np.concatenate([mc, np.power(mc,2)], axis=-1)\n",
    "\n",
    "    ## Replace last timepoint with zeros \n",
    "    ## Otherwise you are putting motion from the first frame into the last of the run\n",
    "    mc[-1] = np.zeros((mc[-1].shape))\n",
    "\n",
    "    ## Apply PCA.  \n",
    "    pca = PCA(n_components=24)\n",
    "    mc = pca.fit_transform(mc)  \n",
    "\n",
    "    ## Concatenate nuisance regressors. \n",
    "    ns = np.hstack((mc,gs,wm,vcsf))\n",
    "    \n",
    "    ## Remove first 4 timepoints.\n",
    "    ns = np.delete(ns, np.arange(4), axis=0) ## remove first 4 timepoints\n",
    "    \n",
    "    ## Filter parameters.\n",
    "    tr = 3\n",
    "    Fs = 1. / tr \n",
    "    Fp1 = 0.01\n",
    "    Fp2 = 0.08\n",
    "\n",
    "    ## Filter data and regressors.\n",
    "    mean_dat = band_pass_filter(mean_dat.astype(np.float64), Fs, Fp1, Fp2, filter_length='120s', method='iir')\n",
    "    ns = band_pass_filter(ns.T, Fs, Fp1, Fp2, filter_length='120s', method='iir').T\n",
    "    \n",
    "    ## Regress. \n",
    "    betas,_,_,_ = np.linalg.lstsq(ns,mean_dat.T)\n",
    "    regressed = mean_dat - np.dot(ns, betas).T\n",
    "    \n",
    "    ## Save regressed data.\n",
    "    np.savez_compressed(os.path.join(gt_dir, 'regressed', '%s_regressed' %subject), regressed=regressed)\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Nuisance Regress. (NOT including GSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T14:10:18.423329",
     "start_time": "2017-02-21T14:10:18.317161"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from pandas import DataFrame, read_table\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import detrend\n",
    "from sklearn.decomposition import PCA\n",
    "from mne.filter import band_pass_filter\n",
    "\n",
    "\n",
    "## Specify directories. \n",
    "fast_dir = '/space/will/3/users/EMBARC/EMBARC-FAST'\n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print i, subject\n",
    "    \n",
    "    ## Load data.\n",
    "    mean_dat = np.load(os.path.join(gt_dir, 'raw', '%s_raw.npz' %subject))['mean_dat']\n",
    "    \n",
    "    ## Remove first 4 timepoints from data.\n",
    "    mean_dat = np.delete(mean_dat, np.arange(4), axis=1)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Perform Nuissance Regression and Filtering.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    n_components = 5        # No. of components to use from PCA.\n",
    "    \n",
    "    ## Load wm and csf regressors. \n",
    "    wm = read_table(os.path.join(fast_dir, subject,fsd, run, 'wm.dat'), sep=' *', header=None, engine='python')\n",
    "    vcsf = read_table(os.path.join(fast_dir, subject,fsd, run, 'vcsf.dat'), sep=' *', header=None, engine='python')\n",
    "    \n",
    "    ## Keep only top 5 components.\n",
    "    n_components = 5 \n",
    "    wm = wm[wm.columns[:n_components]].as_matrix()\n",
    "    vcsf = vcsf[vcsf.columns[:n_components]].as_matrix()\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if wm.shape[0] > 176: wm = np.delete(wm, np.arange(176,wm.shape[0]), axis=0)\n",
    "    if vcsf.shape[0] > 176: vcsf = np.delete(vcsf, np.arange(176,vcsf.shape[0]), axis=0)\n",
    "    \n",
    "    ## Load motion regressors.\n",
    "    mc = os.path.join(fast_dir, subject, fsd, run, 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:7]\n",
    "    if mc.shape[0] > 176:\n",
    "        mc = np.delete(mc, np.arange(176,mc.shape[0]), axis=0)\n",
    "\n",
    "    ## Demean/detrend data.\n",
    "    mc = detrend(mc, axis=0, type='constant')\n",
    "    mc = detrend(mc, axis=0, type='linear') ## Keeps breaking\n",
    "\n",
    "    ## Construct basis sets.\n",
    "    mc = np.concatenate([mc, np.roll(mc, -1, 0)], axis=-1)\n",
    "    mc = np.concatenate([mc, np.power(mc,2)], axis=-1)\n",
    "\n",
    "    ## Replace last timepoint with zeros \n",
    "    ## Otherwise you are putting motion from the first frame into the last of the run\n",
    "    mc[-1] = np.zeros((mc[-1].shape))\n",
    "\n",
    "    ## Apply PCA.  \n",
    "    pca = PCA(n_components=24)\n",
    "    mc = pca.fit_transform(mc)  \n",
    "\n",
    "    ## Concatenate nuisance regressors. \n",
    "    ns = np.hstack((mc,wm,vcsf))\n",
    "    \n",
    "    ## Remove first 4 timepoints.\n",
    "    ns = np.delete(ns, np.arange(4), axis=0) ## remove first 4 timepoints\n",
    "    \n",
    "    ## Filter parameters.\n",
    "    tr = 3\n",
    "    Fs = 1. / tr \n",
    "    Fp1 = 0.01\n",
    "    Fp2 = 0.08\n",
    "\n",
    "    ## Filter data and regressors.\n",
    "    mean_dat = band_pass_filter(mean_dat.astype(np.float64), Fs, Fp1, Fp2, filter_length='120s', method='iir')\n",
    "    ns = band_pass_filter(ns.T, Fs, Fp1, Fp2, filter_length='120s', method='iir').T\n",
    "    \n",
    "    ## Regress. \n",
    "    betas,_,_,_ = np.linalg.lstsq(ns,mean_dat.T)\n",
    "    regressed = mean_dat - np.dot(ns, betas).T\n",
    "    \n",
    "    ## Save regressed data.\n",
    "    np.savez_compressed(os.path.join(gt_dir, 'regressed', '%s_regressed_nogsr' %subject), regressed=regressed)\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate SNR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T12:42:53.738694",
     "start_time": "2017-02-02T12:09:17.543179"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "out_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/preproc'\n",
    "    \n",
    "snr_mat = np.zeros((len(subjects)))\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print subject\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load masks.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    mask_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001/masks' %subject\n",
    "    mri_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001' %subject\n",
    "\n",
    "    brainmask = os.path.join(mask_dir,'brain.nii.gz')\n",
    "    brainmask = np.where( nib.load(brainmask).get_data(), 1, 0 ) # Binarize the mask\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load functional data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#    \n",
    "    fmcpr = os.path.join(mri_dir,'fmcpr.nii.gz')\n",
    "    fmcpr = nib.load(fmcpr).get_data()\n",
    "    \n",
    "    fmcpr_mean = np.mean(fmcpr,axis=-1)\n",
    "    brain_mean = np.where(brainmask, fmcpr_mean, 0).mean()\n",
    "    noise_std = np.where(~brainmask, fmcpr_mean, 0).std()\n",
    "    \n",
    "    snr = brain_mean / noise_std\n",
    "    snr_mat[i] = snr\n",
    "    \n",
    "pd.DataFrame(snr_mat, columns=['SNR'], index=subjects).to_csv(os.path.join(out_dir, 'snr.csv'))\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and plot motion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate functional displacement and other motion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T12:11:43.903704",
     "start_time": "2017-02-21T12:11:43.165691"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "def demean(arr): return arr - arr.mean()\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "fmcpr.mcdat file format: \n",
    "1. n      : time point\n",
    "2. roll   : rotation about the I-S axis (degrees CCW)\n",
    "3. pitch  : rotation about the R-L axis (degrees CCW)\n",
    "4. yaw    : rotation about the A-P axis (degrees CCW)\n",
    "5. dS     : displacement in the Superior direction (mm)\n",
    "6. dL     : displacement in the Left direction (mm)\n",
    "7. dP     : displacement in the Posterior direction (mm)\n",
    "8. rmsold : RMS difference between input frame and reference frame\n",
    "9. rmsnew : RMS difference between output frame and reference frame\n",
    "10. trans : translation (mm) = sqrt(dS^2 + dL^2 + dP^2)\n",
    "\n",
    "Translations (displacement) are in mm, rotations are in degrees.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "threshold = 0.5\n",
    "fast_dir = '/space/will/3/users/EMBARC/EMBARC-FAST'\n",
    "fsd = 'rest'\n",
    "run = '001'\n",
    "\n",
    "out_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/preproc'\n",
    "\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "subjects += [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "\n",
    "## Subjects with manual coregistration.\n",
    "# subjects = ['CU0092CUMR1R1', 'CU0087CUMR1R1', 'CU0095CUMR1R1', 'CU0104CUMR1R1', 'CU0131CUMR1R1', 'CU0069CUMR1R1', \n",
    "#             'CU0025CUMR1R1', 'CU0094CUMR1R1', 'CU0067CUMR1R1', 'CU0106CUMR1R1',  'CU0105CUMR1R1', \n",
    "#             'CU0070CUMR1R1', 'CU0129CUMR1R1', 'CU0113CUMR1R1', 'CU0125CUMR1R1', 'CU0133CUMR1R1']\n",
    "\n",
    "motion = np.zeros((len(subjects),6))\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare motion data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Read motion data.\n",
    "    mc = os.path.join(fast_dir, subject, fsd, run, 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:]\n",
    "\n",
    "    ## Remove first four timepoints.\n",
    "    mc = np.delete(mc, np.arange(4),axis=0)\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if mc.shape[0] > 176:\n",
    "        mc = np.delete(mc, np.arange(176,mc.shape[0]), axis=0)\n",
    "\n",
    "    ## Invert angular displacement.\n",
    "    mc[:,:3] = np.deg2rad(mc[:,:3]) \n",
    "    mc[:,:3] *= 50\n",
    "    \n",
    "    abs_mean = mc[8].mean()\n",
    "    abs_max = mc[8].max()\n",
    "    rel_mean = abs(np.diff(mc[9])).mean()\n",
    "    rel_max = abs(np.diff(mc[9])).max()\n",
    "\n",
    "    ## Compute framewise displacement (See Power 2012, 2014).\n",
    "    fd = np.insert( np.abs( np.diff(mc[:,3:], axis=0) ).sum(axis=1), 0, 0 )\n",
    "    \n",
    "    fd_mean = fd.mean()\n",
    "    fd_max = fd.max()\n",
    "    \n",
    "    motion[i] = np.round(np.vstack([abs_mean, abs_max, rel_mean, rel_max, fd_mean, fd_max]).flatten(),2)\n",
    "        \n",
    "pd.DataFrame(motion, columns=['abs_mean', 'abs_max', 'rel_mean', 'rel_max', 'FD_mean', 'FD_max'], index=subjects).to_csv(os.path.join(out_dir, 'fd.csv'))\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask grey and white matter timeseries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T14:49:57.770579",
     "start_time": "2017-02-02T14:43:32.351780"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Please see /space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/wm_masks.csh for instructions on how to make wm masks.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pandas import read_csv\n",
    "from mne.filter import construct_iir_filter, filter_data\n",
    "def demean(arr): return arr - arr.mean()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "decim = 250\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "\n",
    "for subject in subjects[:3]:\n",
    "    \n",
    "    print subject\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and prepare masks.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    mask_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001/masks' %subject\n",
    "    mri_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001' %subject\n",
    "    out_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001/motion' %subject\n",
    "\n",
    "    brainmask = os.path.join(mask_dir,'brain.nii.gz')\n",
    "    brainmask = np.where( nib.load(brainmask).get_data(), 1, 0 ) # Binarize the mask\n",
    "\n",
    "    wm = os.path.join(mask_dir,'wm.mgz')\n",
    "    wm = np.where( nib.load(wm).get_data(), 1, 0 ) # Binarize the mask\n",
    "\n",
    "    gm = brainmask - wm\n",
    "\n",
    "    ## Reduce to indices of interest.\n",
    "    gm = np.vstack(np.where(gm))[:,::decim]\n",
    "    wm = np.vstack(np.where(wm))[:,::decim]\n",
    "    del brainmask\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and slice through EPI image.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load data.\n",
    "    obj = nib.load(os.path.join(mri_dir, 'fmcpr.nii.gz'))\n",
    "    _,_,_,n_acq = obj.shape\n",
    "\n",
    "    ## Preallocoate space for timeseries.\n",
    "    gmts = np.zeros((n_acq, gm.shape[-1]))\n",
    "    wmts = np.zeros((n_acq, wm.shape[-1]))\n",
    "\n",
    "    for n in range(n_acq):\n",
    "\n",
    "        ## Slice image.\n",
    "        acq = obj.dataobj[..., n]\n",
    "\n",
    "        ## Store grey matter.\n",
    "        gmts[n] += acq[gm[0],gm[1],gm[2]]\n",
    "\n",
    "        ## Store white matter.\n",
    "        wmts[n] += acq[wm[0],wm[1],wm[2]]\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Preprocessing data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ## Construct highpass filter.\n",
    "    tr = 3\n",
    "    sfreq = 1. / tr\n",
    "    high_pass = 0.01\n",
    "    iir_params = dict(order=2, ftype='butter', output='sos') # Following Power et al. (2014)\n",
    "    iir_params = construct_iir_filter(iir_params, high_pass, None, sfreq, 'highpass', return_copy=False)  \n",
    "\n",
    "    ## Filter data.\n",
    "    gmts = filter_data(gmts.T, sfreq, high_pass, None, method='iir', iir_params=iir_params, verbose=False)\n",
    "    wmts = filter_data(wmts.T, sfreq, high_pass, None, method='iir', iir_params=iir_params, verbose=False)\n",
    "\n",
    "    ## De-mean.\n",
    "    gmts = np.apply_along_axis(demean, 1, gmts)\n",
    "    wmts = np.apply_along_axis(demean, 1, wmts)\n",
    "\n",
    "    ## Re-organize (center outwards).\n",
    "    gmts = gmts[ np.argsort( np.power( np.apply_along_axis(demean, 1, gm), 2 ).sum(axis=0) ) ]\n",
    "    wmts = gmts[ np.argsort( np.power( np.apply_along_axis(demean, 1, wm), 2 ).sum(axis=0) ) ]\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Save data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    if not os.path.isdir(out_dir): os.makedirs(out_dir)\n",
    "    f = os.path.join(out_dir, '%s_rest_qc_data' %subject)\n",
    "    np.savez_compressed(f, gm=gmts, wm=wmts, iir_params=iir_params )\n",
    "    del gmts, wmts, obj\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare motion and masked fMRI data. Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-02T15:32:49.355229",
     "start_time": "2017-02-02T15:19:33.986174"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "from scipy.signal import detrend\n",
    "from sklearn.decomposition import PCA\n",
    "def demean(arr): return arr - arr.mean()\n",
    "def rms(arr): return np.sqrt( np.mean( np.power(arr, 2) ) )\n",
    "%matplotlib inline\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "threshold = 0.5\n",
    "fast_dir = '/space/will/3/users/EMBARC/EMBARC-FAST'\n",
    "fsd = 'rest'\n",
    "run = '001'\n",
    "\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "## Subjects with manual coregistration.\n",
    "# subjects = ['CU0092CUMR1R1', 'CU0087CUMR1R1', 'CU0095CUMR1R1', 'CU0104CUMR1R1', 'CU0131CUMR1R1', 'CU0069CUMR1R1', \n",
    "#             'CU0025CUMR1R1', 'CU0094CUMR1R1', 'CU0067CUMR1R1', 'CU0106CUMR1R1',  'CU0105CUMR1R1', \n",
    "#             'CU0070CUMR1R1', 'CU0129CUMR1R1', 'CU0113CUMR1R1', 'CU0125CUMR1R1', 'CU0133CUMR1R1']\n",
    "\n",
    "for subject in subjects[18:]:\n",
    "    \n",
    "    print subject, \n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare motion data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Read motion data.\n",
    "    mc = os.path.join(fast_dir, subject, fsd, run, 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:7]\n",
    "\n",
    "    ## Remove first four timepoints.\n",
    "    mc = np.delete(mc, np.arange(4),axis=0)\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if mc.shape[0] > 176:\n",
    "        mc = np.delete(mc, np.arange(176,mc.shape[0]), axis=0)\n",
    "\n",
    "    ## Invert angular displacement.\n",
    "    copy = mc.copy()\n",
    "    mc[:,:3] = np.deg2rad(mc[:,:3]) \n",
    "    mc[:,:3] *= 50\n",
    "\n",
    "    ## Compute framewise displacement (See Power 2012, 2014).\n",
    "    fd = np.insert( np.abs( np.diff(mc[:,3:], axis=0) ).sum(axis=1), 0, 0)\n",
    "\n",
    "    ## Demean/detrend data.\n",
    "    mc = detrend(copy, axis=0, type='constant')\n",
    "    mc = detrend(mc, axis=0, type='linear')\n",
    "\n",
    "    ## Construct basis sets.\n",
    "    mcreg24 = mc.copy()\n",
    "    mcreg24 = np.concatenate([mcreg24, np.roll(mcreg24, -1, 0)], axis=-1)\n",
    "    mcreg24 = np.concatenate([mcreg24, np.power(mcreg24,2)], axis=-1)\n",
    "\n",
    "    ## Replace last timepoint with zeros.\n",
    "    ## Otherwise you are putting motion from the first frame into the last of the run.\n",
    "    mcreg24[-1] = np.zeros((mcreg24[-1].shape))\n",
    "    \n",
    "    ## Apply PCA.  \n",
    "    pca = PCA(n_components=24)\n",
    "    mcreg24 = pca.fit_transform(mcreg24)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare fMRI data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load gray/white matter timeseries.\n",
    "    npz = np.load(os.path.join(fast_dir, subject, fsd, run, 'motion/%s_rest_qc_data.npz' %subject))\n",
    "    \n",
    "    gm_pre = np.apply_along_axis(demean, 1, np.delete(npz['gm'],np.arange(4),axis=1))\n",
    "    wm_pre = np.apply_along_axis(demean, 1, np.delete(npz['wm'],np.arange(4),axis=1))\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if gm_pre.shape[1] > 176:\n",
    "        gm_pre = np.delete(gm_pre, np.arange(176,gm_pre.shape[1]), axis=1)\n",
    "        wm_pre = np.delete(wm_pre, np.arange(176,wm_pre.shape[1]), axis=1)\n",
    "\n",
    "    gm_beta, _, _, _ = np.linalg.lstsq(mcreg24, gm_pre.T)\n",
    "    wm_beta, _, _, _ = np.linalg.lstsq(mcreg24, wm_pre.T)\n",
    "    gm_post = gm_pre - np.dot(mcreg24, gm_beta).T\n",
    "    wm_post = wm_pre - np.dot(mcreg24, wm_beta).T\n",
    "\n",
    "    ## Compute DVARS.\n",
    "    gm_DVARS = np.zeros((2,fd.shape[0]))\n",
    "    wm_DVARS = np.zeros((2,fd.shape[0]))\n",
    "    for n, arr in enumerate([gm_pre,gm_post]): gm_DVARS[n,1:] += np.apply_along_axis(rms, 0, np.diff(arr, axis=1))\n",
    "    for n, arr in enumerate([wm_pre,wm_post]): wm_DVARS[n,1:] += np.apply_along_axis(rms, 0, np.diff(arr, axis=1))\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plotting\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    fig = plt.figure(figsize=(16,18))\n",
    "    nrow = 16\n",
    "\n",
    "    ## Plot framewise displacement.\n",
    "    ax = plt.subplot2grid((nrow,1),(0,0),rowspan=1)\n",
    "    ax.plot(fd, linewidth=1.5, color='k')\n",
    "    ax.hlines(0.5, 0, fd.shape[0], linestyle='--', color='k', alpha=0.5)\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([0.0,0.5,1.0])\n",
    "    ax.set_yticklabels([0.0,0.5,1.0], rotation=90)\n",
    "    ax.set_ylabel('FD (mm)', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.set_title(subject, fontsize=24)\n",
    "\n",
    "    ## Plot grey matter (pre-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(1,0),rowspan=4)\n",
    "    cbar = ax.imshow(gm_pre, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('Grey Matter\\nPre-Regression', fontsize=12)\n",
    "\n",
    "    # Colobar setup.\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width, box.height])\n",
    "    axColor = plt.axes([0.905, 0.238, 0.015, 0.660])\n",
    "    cbar = plt.colorbar(cbar, cax = axColor, orientation=\"vertical\")\n",
    "    cbar.set_ticks([-50,0,50])\n",
    "    axColor.set_ylabel('BOLD', fontsize=12, rotation=270)\n",
    "\n",
    "    ## Plot grey matter (post-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(5,0),rowspan=4)\n",
    "    cbar = ax.imshow(gm_post, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('Grey Matter\\nPost-Regression', fontsize=12)\n",
    "\n",
    "    ## Plot DVARS.\n",
    "    ax = plt.subplot2grid((nrow,1),(9,0),rowspan=1)\n",
    "    for arr, label in zip(gm_DVARS, ['Pre','Post']): ax.plot(arr, linewidth=1.5, alpha=0.8, label=label)\n",
    "    ax.legend(loc=2, fontsize=8, frameon=False, borderpad=0.0,  handlelength=1.4, handletextpad=0.2)\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_xlabel('Acquisitions', fontsize=12)\n",
    "    ax.set_ylim(20,100)\n",
    "    ax.set_yticks([20,60,100])\n",
    "    ax.set_yticklabels([20,60,100], rotation=90)\n",
    "    ax.set_ylabel('DVARS_gm', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    ## Plot white matter (pre-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(11,0),rowspan=2)\n",
    "    cbar = ax.imshow(wm_pre, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('White Matter\\nPre-Regression', fontsize=12)\n",
    "\n",
    "    ## Plot white matter (post-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(13,0),rowspan=2)\n",
    "    cbar = ax.imshow(wm_post, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('White Matter\\nPost-Regression', fontsize=12)\n",
    "\n",
    "    ## Plot DVARS.\n",
    "    ax = plt.subplot2grid((nrow,1),(15,0))\n",
    "    for arr, label in zip(wm_DVARS, ['Pre','Post']): ax.plot(arr, linewidth=1.5, alpha=0.8, label=label)\n",
    "    ax.legend(loc=2, fontsize=8, frameon=False, borderpad=0.0,  handlelength=1.4, handletextpad=0.2)\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_xlabel('Acquisitions', fontsize=12)\n",
    "    ax.set_ylim(20,100)\n",
    "    ax.set_yticks([20,60,100])\n",
    "    ax.set_yticklabels([20,60,100], rotation=90)\n",
    "    ax.set_ylabel('DVARS_wm', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    plt.subplots_adjust(left=0.05, right=0.90, top=0.95, bottom=0.05, hspace=0.05)\n",
    "#     plt.show()\n",
    "    plt.savefig(os.path.join(fast_dir,subject,fsd,run,'motion/%s_supp_motion.png' %subject), dpi=180)\n",
    "    plt.savefig(os.path.join(fast_dir, 'jan2017', 'motion_plots', '%s_supp_motion.png' %subject), dpi=180)\n",
    "    plt.close('all')\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T14:09:20.727393",
     "start_time": "2017-02-21T14:09:20.694016"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "\n",
    "for i, subject in enumerate(hcs):\n",
    "        \n",
    "    ## Load regressed data. \n",
    "    regressed = np.load(os.path.join(gt_dir, 'regressed', '%s_regressed_nogsr.npz' %subject))['regressed']\n",
    "    \n",
    "    ## Correlate. Remove negative correlations.\n",
    "    corrmat = np.corrcoef(regressed)\n",
    "#     corrmat = np.where(corrmat<=0, np.nan, corrmat)\n",
    "    \n",
    "    # Fisher's r-to-z transform.\n",
    "    zmat = np.arctanh(corrmat)\n",
    "    zmat = np.where(zmat==np.inf,np.nan,zmat)\n",
    "\n",
    "    ## Save correlation.\n",
    "    np.savez(os.path.join(gt_dir, 'correlations', '%s_correlations_nogsr_unthresholded' %subject), corrmat=corrmat)\n",
    "    np.savez(os.path.join(gt_dir, 'correlations', '%s_zscores_nogsr_unthresholded' %subject), zmat=zmat)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot z-scores for group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T14:06:17.369583",
     "start_time": "2017-02-21T14:06:03.737964"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "\n",
    "## Specify labels.\n",
    "regions = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')])\n",
    "\n",
    "lh = np.concatenate([regions[:57],regions[114:120]])\n",
    "rh = np.concatenate([regions[57:114],regions[120:]])\n",
    "\n",
    "regions = np.concatenate([lh,rh])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "zmat_master = np.zeros((len(subjects),len(regions),len(regions)))\n",
    "\n",
    "for i, subject in enumerate(hcs): \n",
    "    \n",
    "    ## Load data.\n",
    "    zmat = np.load(os.path.join(gt_dir, 'correlations', '%s_zscores_unthresholded.npz' %subject))['zmat']\n",
    "    \n",
    "    ## Re-arrange.\n",
    "    ztop = np.concatenate([zmat[:57],zmat[114:120]])\n",
    "    zbottom = np.concatenate([zmat[57:114],zmat[120:]])\n",
    "    \n",
    "    zmat = np.concatenate([ztop,zbottom],axis=0)\n",
    "    \n",
    "    zleft = np.concatenate([zmat[:,:57],zmat[:,114:120]],axis=1)\n",
    "    zright = np.concatenate([zmat[:,57:114],zmat[:,120:]],axis=1)\n",
    "    \n",
    "    zmat = np.concatenate([zleft,zright],axis=1)\n",
    "\n",
    "    zmat_master[i] = zmat\n",
    "\n",
    "## Average across subjects. \n",
    "zmat_mean = np.nanmean(zmat_master, axis=0)\n",
    "\n",
    "## Convert to DataFrame.\n",
    "zpd = pd.DataFrame(zmat_mean, columns=regions, index=regions)\n",
    "\n",
    "## Generate a mask for the upper triangle. \n",
    "mask = np.zeros_like(zpd, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask,k=0)] = True\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot heatmap.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "# vvals = [0.5,0.6,0.7,0.8,0.9,1.0,zmat_mean[~mask].max()]\n",
    "vvals = np.arange(zmat_mean[~mask].min(),zmat_mean[~mask].max(),.1)\n",
    "\n",
    "for vmax in vvals:\n",
    "    f, ax = plt.subplots(figsize=(25,20))\n",
    "    sns.heatmap(zpd, cmap=\"RdYlBu_r\", vmin=zmat_mean[~mask].min(), vmax=vmax, mask=mask, square=True, xticklabels=True, yticklabels=True, linewidth=0.1, \n",
    "                cbar_kws={'shrink':1.0}, ax=ax)\n",
    "\n",
    "    ## Draw lines. \n",
    "    xlines = xlines = [14,26,33,35,46,51,52,57,63,75,87,94,96,109,114,115,120]\n",
    "    ylines = [126-x for x in xlines]\n",
    "    plt.vlines(x=xlines, ymin=0, ymax=[126-x for x in xlines],linestyles='dotted')\n",
    "    plt.hlines(y=ylines, xmin=0, xmax=xlines, linestyles='dotted')\n",
    "\n",
    "    ## Flourishes\n",
    "    plt.title('Z-scores for group (GSR, unthresholded)', fontsize=30)\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    ## Save. \n",
    "    plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/correlations/hcs_zscores_gsr_unthresh_%.1f.png' %vmax)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Correlation Heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T14:08:10.554779",
     "start_time": "2017-02-21T14:08:10.387390"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "\n",
    "## Specify labels.\n",
    "regions = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')])\n",
    "\n",
    "lh = np.concatenate([regions[:57],regions[114:120]])\n",
    "rh = np.concatenate([regions[57:114],regions[120:]])\n",
    "\n",
    "regions = np.concatenate([lh,rh])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot correlations per subject (with/without GSR).\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for subject in subjects: \n",
    "    \n",
    "    print subject, \n",
    "    \n",
    "    zmat_gsr = np.load(os.path.join(gt_dir, 'correlations', '%s_correlations_unthresholded.npz' %subject))['corrmat']\n",
    "    zmat_nogsr = np.load(os.path.join(gt_dir, 'correlations', '%s_correlations_nogsr_unthresholded.npz' %subject))['corrmat']\n",
    "    \n",
    "    zmat_diff = zmat_gsr - zmat_nogsr\n",
    "    zmat = np.concatenate([np.concatenate([zmat_diff[:57],zmat_diff[114:120]]),np.concatenate([zmat_diff[57:114],zmat_diff[120:]])])\n",
    "    \n",
    "    ## Convert to DataFrame.\n",
    "    zpd = pd.DataFrame(zmat, columns=regions, index=regions)\n",
    "    \n",
    "    ## Generate a mask for the upper triangle. \n",
    "    mask = np.zeros_like(zpd, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plot heatmap.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    f, ax = plt.subplots(figsize=(25,20))\n",
    "    cmap=sns.color_palette(\"RdBu_r\")\n",
    "    sns.heatmap(zpd, mask=mask, cmap=cmap, square=True, xticklabels=True, yticklabels=True, linewidth=0.1, cbar_kws={'shrink':1.0}, ax=ax)\n",
    "    \n",
    "    ## Draw lines. \n",
    "    xlines = [14,26,33,35,46,52,57,63] + [63+x for x in [14,26,33,35,46,52,57]]\n",
    "    ylines = [126-x for x in xlines]\n",
    "    plt.vlines(x=xlines, ymin=0, ymax=[126-x for x in xlines],linestyles='dotted')\n",
    "    plt.hlines(y=ylines, xmin=0, xmax=xlines, linestyles='dotted')\n",
    "        \n",
    "    ## Flourishes.\n",
    "    ax.set_title('%s: Correlations GSR - noGSR, unthresholded' %subject, fontsize=30)\n",
    "    cax = plt.gcf().axes[-1]\n",
    "    cax.tick_params(labelsize=20,labeltop=True,labelbottom=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "#     plt.show()\n",
    "    plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/correlations/hcs/%s_corr_unthresh_GSRdiff.png' %subject)\n",
    "    plt.close()\n",
    "    \n",
    "print '\\nDone.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot correlation distribution (GSR vs. noGSR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify subjects.\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "## Specify regions.\n",
    "regions = np.array([line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot correlations (with/without GSR).\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "for subject in subjects: \n",
    "    \n",
    "    ## Load data.\n",
    "    zmat_gsr = np.load(os.path.join(gt_dir, 'correlations', '%s_correlations_unthresholded.npz' %subject))['corrmat']\n",
    "    zmat_nogsr = np.load(os.path.join(gt_dir, 'correlations', '%s_correlations_nogsr_unthresholded.npz' %subject))['corrmat']\n",
    "    \n",
    "    ## Convert to DataFrame.\n",
    "    zpd_gsr = pd.DataFrame(zmat_gsr, columns=regions, index=regions)\n",
    "    \n",
    "    ## Plotting.\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    sns.set(color_codes=True)\n",
    "    sns.set_palette(sns.color_palette(\"GnBu_d\"))\n",
    "    sns.distplot(zmat_gsr.flatten(),hist=False, kde=True, label='GSR', hist_kws={'alpha':0.7})\n",
    "    sns.distplot(zmat_nogsr.flatten(), hist=False, kde=True, label='No GSR', hist_kws={'alpha':0.7})\n",
    "    \n",
    "    ## Flourishes.\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.title('%s \\nDistribution of Correlations (unthresholded)' %subject, fontsize=16)\n",
    "    \n",
    "    ## Save.\n",
    "    plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/correlations/histograms/%s_corr_GSRhist_unthresh.png' %subject)\n",
    "    plt.close()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis (GSR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute network thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-02-21T19:36:28.769Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import networkx as nx\n",
    "import scipy.io as sio\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "%matplotlib inline\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Construct network with GSR.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Specify directories.\n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Load regions.\n",
    "regions = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')]\n",
    "n_regions = len(regions)\n",
    "\n",
    "## Specify subjects.\n",
    "pps = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "hcs = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies\", 'r')]\n",
    "\n",
    "subjects = pps + hcs\n",
    "n_subs = len(subjects)\n",
    "\n",
    "## Specify parameters. \n",
    "thresholds = np.arange(0.2,1.05,0.05)\n",
    "metrics = ['threshold', 'n_nodes', 'n_edges', 'avg_degree', 'n_components', 'density', 'clustering_coefficient']\n",
    "metrics_dat = np.zeros((len(subjects),thresholds.shape[0],len(metrics)))\n",
    "\n",
    "## Network array. \n",
    "Gnets = []\n",
    "\n",
    "## Iterate over subjects. \n",
    "for i,subject in enumerate(subjects):\n",
    "    \n",
    "    if 1%20 == 0: print i, \n",
    "        \n",
    "    ## Load data. \n",
    "    zmat = np.load(os.path.join(gt_dir, 'correlations', '%s_zscores_unthresholded.npz' %subject))['zmat']\n",
    "    \n",
    "    for j,thresh in enumerate(thresholds): \n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Create graph.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(regions)\n",
    "\n",
    "        for r1,x in enumerate(regions):\n",
    "            for r2,y in enumerate(regions):\n",
    "                if not np.isnan(zmat[r1,r2]) and zmat[r1,r2] > thresh:\n",
    "                    G.add_edge(x,y)\n",
    "\n",
    "        G.remove_nodes_from(G.nodes_with_selfloops())\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute graph metrics.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        n_nodes = nx.number_of_nodes(G)\n",
    "        n_edges = nx.number_of_edges(G)\n",
    "        max_edges = (n_nodes*(n_nodes-1))/2\n",
    "        avg_degree = sum(nx.degree(G).values()) / n_nodes  \n",
    "        n_components = nx.number_connected_components(G)\n",
    "        density = nx.density(G)    \n",
    "        clustering_coefficient = nx.average_clustering(G) \n",
    "        \n",
    "        ## Store metrics. \n",
    "        metrics_dat[i,j] = np.array([thresh, n_nodes,n_edges,avg_degree,n_components,density,clustering_coefficient])\n",
    "\n",
    "## Flatten. \n",
    "metrics_dat = metrics_dat.reshape((len(subjects)*len(thresholds),len(metrics)),order='C')\n",
    "\n",
    "## Convert to csv. Save.\n",
    "subject_ids = np.expand_dims(np.repeat(np.array(subjects),len(thresholds)),axis=1)\n",
    "metrics_pd = pd.DataFrame(np.concatenate([subject_ids, metrics_dat],axis=1),columns=['Subject_ID'] + metrics)\n",
    "metrics_pd.to_csv(os.path.join(gt_dir, 'thresholding', 'all_graph_metrics_by_zscore.csv'))\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create thresholded networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T14:35:54.882638",
     "start_time": "2017-02-21T14:35:54.858401"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx \n",
    "\n",
    "## Specify directories.\n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Specify parameters. \n",
    "density_range = [0.3,0.1]\n",
    "metrics = ['threshold', 'n_nodes', 'n_edges', 'avg_degree', 'n_components', 'density', 'clustering_coefficient']\n",
    "\n",
    "## Load graph metrics.\n",
    "metrics_pd = pd.read_csv(os.path.join(gt_dir, 'thresholding', 'all_graph_metrics_by_zscore.csv'), usecols=['Subject_ID'] + metrics)\n",
    "metrics_pd['cohort'] = [1 if subject in pps else 0 for subject in metrics_pd.Subject_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T14:25:07.390206",
     "start_time": "2017-02-21T14:24:39.498456"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Threshold by average degree. \n",
    "metrics_pd = metrics_pd.where(metrics_pd.avg_degree > (2 * np.log(metrics_pd.n_nodes)))\n",
    "\n",
    "## Threshold by density.\n",
    "metrics_pd.density = np.round(metrics_pd.density,2)\n",
    "metrics_pd = metrics_pd.where(((metrics_pd.density<=density_range[0])&(metrics_pd.density>=density_range[1])))\n",
    "\n",
    "## Threshold by # of components. \n",
    "metrics_pd = metrics_pd.where(metrics_pd.n_components==1)\n",
    "\n",
    "## Remove NaNs. \n",
    "metrics_pd = metrics_pd.dropna()\n",
    "# metrics_pd = metrics_pd.set_index('Subject_ID',drop=True)\n",
    "\n",
    "## Specify subjects.\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "n_subs = len(subjects)\n",
    "\n",
    "## Load regions.\n",
    "regions = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')]\n",
    "n_regions = len(regions)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Initilize arrays to store graph metrics.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "avg_path_length = np.zeros((metrics_pd.shape[0]))\n",
    "glob_efficiency = np.zeros((metrics_pd.shape[0]))\n",
    "loc_efficiency = np.zeros((metrics_pd.shape[0]))\n",
    "\n",
    "node_degrees = np.zeros((metrics_pd.shape[0],len(regions)))\n",
    "degree_centrality = np.zeros((metrics_pd.shape[0],len(regions)))\n",
    "betweenness_centrality = np.zeros((metrics_pd.shape[0],len(regions)))\n",
    "eigenvector_centrality = np.zeros((metrics_pd.shape[0],len(regions)))\n",
    "\n",
    "for i,subject,threshold in zip(np.arange(metrics_pd.shape[0]), metrics_pd.index, metrics_pd.threshold): \n",
    "            \n",
    "    ## Load z-scores of correlations.\n",
    "    zmat = np.load(os.path.join(gt_dir, 'correlations', '%s_zscores_unthresholded.npz' %subject))['zmat']\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Create graph.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(regions)\n",
    "\n",
    "    for r1,x in enumerate(regions):\n",
    "        for r2,y in enumerate(regions):\n",
    "            ## Add edge if z-score is greater than threshold.\n",
    "            if not np.isnan(zmat[r1,r2]) and zmat[r1,r2] > threshold:\n",
    "                G.add_edge(x,y)\n",
    "\n",
    "    ## Remove self-edges. \n",
    "    G.remove_nodes_from(G.nodes_with_selfloops())\n",
    "\n",
    "    ## Global metrics.\n",
    "    avg_path_length[i] = nx.average_shortest_path_length(G)\n",
    "    glob_efficiency[i] = global_efficiency(G)\n",
    "    loc_efficiency[i] = local_efficiency(G)\n",
    "    \n",
    "    ## Compute node-specific metrics.\n",
    "    deg = nx.degree(G)\n",
    "    deg_cent = nx.centrality.degree_centrality(G)\n",
    "    bet_cent = nx.centrality.betweenness_centrality(G)\n",
    "    eig_cent = nx.centrality.eigenvector_centrality(G)\n",
    "    \n",
    "    ## Store node-specific metrics.\n",
    "    node_degrees[i] = np.array([deg[region] if region in deg.keys() else 0 for region in regions])\n",
    "    degree_centrality[i] = np.array([deg_cent[region] if region in deg_cent.keys() else 0 for region in regions])\n",
    "    betweenness_centrality[i] = np.array([bet_cent[region] if region in bet_cent.keys() else 0 for region in regions])\n",
    "    eigenvector_centrality[i] = np.array([eig_cent[region] if region in eig_cent.keys() else 0 for region in regions])\n",
    "\n",
    "## Add graph metrics to dataframe. \n",
    "metrics_pd['avg_path_length'] = avg_path_length\n",
    "metrics_pd['global_efficiency'] = glob_efficiency\n",
    "metrics_pd['local_efficiency'] = loc_efficiency\n",
    "metrics_pd = np.round(metrics_pd,3)\n",
    "\n",
    "## Group graph metrics by density. \n",
    "density_groupby = metrics_pd.groupby('density').mean()\n",
    "density_groupby['density'] = density_groupby.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T14:29:48.635431",
     "start_time": "2017-02-21T14:29:45.785722"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "for m in (density_groupby.columns): \n",
    "    \n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    sns.lmplot('density', m, data=density_groupby)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T14:23:50.093730",
     "start_time": "2017-02-21T14:23:50.087265"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "## Requires that graph be fully connected. \n",
    "\n",
    "def efficiency(G, u, v):\n",
    "    return 1. / nx.shortest_path_length(G, u, v)\n",
    "\n",
    "def global_efficiency(G):\n",
    "    n = len(G)\n",
    "    denom = n * (n - 1)\n",
    "    return sum(efficiency(G, u, v) for u, v in permutations(G, 2)) / denom\n",
    "\n",
    "def local_efficiency(G):\n",
    "    return sum(global_efficiency(nx.ego_graph(G, v)) for v in G) / len(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Construction (without GSR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import networkx as nx\n",
    "import scipy.io as sio\n",
    "%matplotlib inline\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define functions.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "def create_network(arr, regions): \n",
    "    \n",
    "    ## Create graph.\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(regions)\n",
    "    \n",
    "    for i,x in enumerate(regions):\n",
    "            for j,y in enumerate(regions):\n",
    "                if not np.isnan(arr[i,j]): \n",
    "                    G.add_edge(x,y)\n",
    "\n",
    "    G.remove_nodes_from(G.nodes_with_selfloops())\n",
    "    return G\n",
    "    \n",
    "    \n",
    "## Specify directories.\n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Load regions.\n",
    "regions = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_regions\", 'r')]\n",
    "n_regions = len(regions)\n",
    "\n",
    "## Specify subjects.\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "n_subs = len(subjects)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Construct network without GSR.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "Gnets_nogsr = []\n",
    "\n",
    "for i,subject in enumerate(subjects): \n",
    "    zmat_nogsr = np.load(os.path.join(gt_dir, 'correlations', '%s_zscores_nogsr_unthresholded.npz' %subject))['zmat']\n",
    "    \n",
    "    G = create_network(zmat_nogsr, regions)\n",
    "    Gnets_nogsr.append(G)\n",
    "    \n",
    "Gnets_nogsr = np.array(Gnets_nogsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute global network metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T14:35:16.645957",
     "start_time": "2017-02-14T14:35:04.991356"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Network metrics without GSR.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "metrics = ['n_nodes', 'n_edges', 'avg_degree', 'avg_path', 'density', 'n_components']\n",
    "metrics_dat_nogsr = np.zeros((len(metrics), n_subs))\n",
    "\n",
    "for i in range(n_subs):\n",
    "    metrics_dat_nogsr[0,i] = nx.number_of_nodes(Gnets_nogsr[i])\n",
    "    metrics_dat_nogsr[1,i] = nx.number_of_edges(Gnets_nogsr[i])\n",
    "    metrics_dat_nogsr[2,i] = sum(nx.degree(Gnets_nogsr[i]).values())/Gnets_nogsr[i].number_of_nodes()\n",
    "    metrics_dat_nogsr[3,i] = nx.average_shortest_path_length(Gnets_nogsr[i])\n",
    "    metrics_dat_nogsr[4,i] = nx.density(Gnets_nogsr[i])\n",
    "    metrics_dat_nogsr[5,i] = len([l for l in nx.connected_components(Gnets_nogsr[i])])\n",
    "    \n",
    "## Average degree of nodes in G should be >= 2*np.log(G.number_of_nodes()) Bullmore 2006/Drakesmith 2015.\n",
    "if not np.all(metrics_dat_nogsr[2] > 2*np.log2(metrics_dat_nogsr[0])): print 'Average degree of nodes not <= 2*log(#nodes)'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-12T13:25:41.974565",
     "start_time": "2017-01-12T13:25:41.972066"
    }
   },
   "source": [
    "### Plot distribution of global metrics. (GSR vs. noGSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T14:54:26.857694",
     "start_time": "2017-02-17T14:54:26.032960"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "## Plotting\n",
    "plt.figure(figsize=(24,18))\n",
    "for r in range(len(metrics)-1):\n",
    "    ax = plt.subplot2grid((2,3),(r/3,r%3),colspan=1,rowspan=1)\n",
    "    sns.distplot(metrics_dat[r], hist=False, label='GSR', hist_kws={'alpha':1})\n",
    "#     sns.distplot(metrics_dat_nogsr[r], hist=False, label='No GSR', hist_kws={'alpha':0.5})\n",
    "    ax.set_title(metrics[r], fontsize=20)\n",
    "    ax.legend(fontsize=14)\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "plt.suptitle('Distribution of Graph Metrics (thresholded)', fontsize=24)\n",
    "# plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/GSR_graph_metrics_dist_unthresh.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Questionnaires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    return [( float(i) - min(arr) )/( max(arr) - min(arr)) for i in arr] \n",
    "\n",
    "## Load questionnaires. \n",
    "csv = read_csv('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/questionnaires/subtypes_qscores.csv')\n",
    "\n",
    "## Restrict to questionnaires of interest.\n",
    "csv.columns = ['Subject_ID', 'Diagnosis','MASQ_AA', 'SHAPS_Cont', 'AAQ']\n",
    "csv = csv.set_index('Subject_ID', drop=True)\n",
    "\n",
    "## Restrict to psychiatric subjects. \n",
    "csv = csv[csv['Diagnosis'] == 1]\n",
    "\n",
    "## Remove subjects with corrupted data.\n",
    "csv = csv[csv.index != 'CU0068CUMR1R1'] ## need to re-proproc\n",
    "\n",
    "## Remove subjects with missing data.\n",
    "csv = csv.dropna()\n",
    "subjects = list(csv.index)\n",
    "\n",
    "## Create questionnaire matrix. \n",
    "qmat = np.vstack([csv['MASQ_AA'], csv['SHAPS_Cont'], csv['AAQ']])\n",
    "\n",
    "## Normalize questionnaires.\n",
    "for i in np.arange(qmat.shape[0]):\n",
    "    qmat[i] = normalize(qmat[i])\n",
    "    \n",
    "## Add intercept column. \n",
    "intercept = np.array([1]*len(subjects))\n",
    "qmat = np.vstack([intercept,qmat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Healthy Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T11:43:34.146976",
     "start_time": "2017-02-21T11:43:34.141117"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "def normalize(arr):\n",
    "    return [( float(i) - min(arr) )/( max(arr) - min(arr)) for i in arr] \n",
    "\n",
    "## Load questionnaires. \n",
    "csv = read_csv('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/questionnaires/subtypes_qscores.csv')\n",
    "\n",
    "## Restrict to questionnaires of interest.\n",
    "csv.columns = ['Subject_ID', 'Diagnosis','MASQ_AA', 'SHAPS_Cont', 'AAQ']\n",
    "csv = csv.set_index('Subject_ID', drop=True)\n",
    "\n",
    "## Restrict to healthy subjects. \n",
    "csv = csv[csv['Diagnosis'] == 2]\n",
    "\n",
    "## Remove subjects with missing data.\n",
    "csv = csv.dropna()\n",
    "subjects = list(csv.index)\n",
    "\n",
    "np.savetxt('/autofs/space/will_003/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_healthies',list(csv.index),fmt='%s')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "930px",
   "left": "0px",
   "right": "1708px",
   "top": "106px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
