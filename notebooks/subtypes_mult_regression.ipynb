{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T14:40:07.504902",
     "start_time": "2016-10-06T14:40:07.497837"
    }
   },
   "source": [
    "All Regions: \n",
    "\n",
    "Cortical (Laus60):\n",
    "\n",
    "OFC/mPFC: \n",
    "    Lateralorbitofrontal_1\n",
    "    Lateralorbitofrontal_2\n",
    "    Medialorbitofrontal_1 (1 & 2 for rh)\n",
    "    Rostralmiddlefrontal_3 (3 for lh, 2 for rh)\n",
    "Cingulate\n",
    "    caudalanteriorcingulate_1\n",
    "    Posteriorcingulate_1\n",
    "    Isthmuscingulate_1\n",
    "    rostralanteriorcingulate_1\n",
    "DLPFC/VLPFC: \n",
    "    caudalmiddlefrontal_1\n",
    "    Parsopercularis_1\n",
    "    Parsorbitalis_1\n",
    "    parstriangularis_1\n",
    "Insula (laus125): \n",
    "    Insula_1\n",
    "    insula_2\n",
    "    insula_3 (also 4 for lh)\n",
    "    Reference Kelly: posterior, ventral-mid, dorsal-anterior\n",
    "    \n",
    "Inferior Parietal Lobule:\n",
    "    inferiorparietal_1\n",
    "    inferiorparietal_2 (also 3 for rh)\n",
    "    \n",
    "Subcortical (aseg):\n",
    "Ventral Striatum\n",
    "NAcc\n",
    "Thalamus\n",
    "Dorsal Striatum\n",
    "Putamen \n",
    "Caudate\n",
    "Hippocampus \n",
    "Amygdala"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Networks: \n",
    "\n",
    "1. Reward Processing: \n",
    "    a. OFC\n",
    "    b. NAcc\n",
    "    c. Caudate\n",
    "    d. Putamen\n",
    "       \n",
    "2. Emotion Processing\n",
    "    a. Amygdala\n",
    "    b. Hippocampus \n",
    "    c. Insula\n",
    "    d. vmPFC\n",
    "    e. rACC\n",
    "    f. IPL\n",
    "    \n",
    "3. Cognitive Control\n",
    "    a. DLPFC\n",
    "    b. VLPFC\n",
    "    c. dACC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-20T12:51:43.190500",
     "start_time": "2016-10-20T12:51:42.417856"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from mne import read_label\n",
    "from pandas import read_table\n",
    "\n",
    "## Specify directories and parameters.\n",
    "fast_dir = '/space/will/3/users/EMBARC/EMBARC-FAST'\n",
    "fsd = 'rest'\n",
    "run = '001'\n",
    "\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "### Load cortical labels.\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "\n",
    "parc_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/labels'\n",
    "lh_labels = [read_label(os.path.join(parc_dir, 'laus', l)) for l in sorted(os.listdir(os.path.join(parc_dir, 'laus'))) if 'lh' in l] \n",
    "rh_labels = [read_label(os.path.join(parc_dir, 'laus', l)) for l in sorted(os.listdir(os.path.join(parc_dir, 'laus'))) if 'rh' in l] \n",
    "\n",
    "## Combine rh medialorbitofrontal labels. \n",
    "for i,l in enumerate(rh_labels):\n",
    "    if l.name == 'medialorbitofrontal_1-rh':\n",
    "        rh_labels[i].vertices = np.append(rh_labels[i+1].vertices,rh_labels[i+1].vertices)\n",
    "        rh_labels = np.delete(rh_labels,i+1)\n",
    "\n",
    "labels = np.append(lh_labels,rh_labels)\n",
    "\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "### Load subcortical labels.\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "\n",
    "## Read Freesurfer Color Lookup Table. \n",
    "rois = ['Left-Accumbens-area', 'Left-Thalamus-Proper', 'Left-Caudate', 'Left-Putamen', 'Left-Hippocampus', 'Left-Amygdala', \n",
    "                  'Right-Accumbens-area', 'Right-Thalamus-Proper', 'Right-Caudate', 'Right-Putamen', 'Right-Hippocampus', 'Right-Amygdala']\n",
    "\n",
    "subcort_labels = dict.fromkeys([l for l in rois])\n",
    "\n",
    "clut = read_table(os.path.join(parc_dir, 'FreeSurferColorLUT_truncated.txt'), sep=' *', skiprows=4, names=['No', 'Label','R','G','B','A'], engine='python')\n",
    "clut = clut.loc[np.where(np.in1d(clut.Label,rois))]\n",
    "\n",
    "## Load subcortical segmentation.\n",
    "aseg_obj = nib.load('/space/lilli/1/users/DARPA-Recons/fsaverage/mri.2mm/aseg.mgz')\n",
    "aseg_dat = aseg_obj.get_data()\n",
    "\n",
    "for l in rois:\n",
    "    subcort_labels[l] = np.where(aseg_dat == int(clut.No[clut.Label == l]))\n",
    "    \n",
    "print 'cortical lh: %d,' %len(lh_labels), 'rh: %d\\n' %len(rh_labels), 'subcortical: %d' %len(subcort_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load questionnaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-20T12:51:44.081403",
     "start_time": "2016-10-20T12:51:44.033381"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "def normalize(arr):\n",
    "    return [( float(i) - min(arr) )/( max(arr) - min(arr)) for i in arr] \n",
    "\n",
    "## Load questionnaires. \n",
    "csv = read_csv('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/questionnaires/subtypes_qscores.csv')\n",
    "\n",
    "## Restrict to questionnaires of interest.\n",
    "csv.columns = ['Subject_ID', 'Diagnosis','MASQ_AA', 'SHAPS_Cont', 'AAQ']\n",
    "csv = csv.set_index('Subject_ID', drop=True)\n",
    "\n",
    "## Restrict to psychiatric subjects. \n",
    "csv = csv[csv['Diagnosis'] == 1]\n",
    "\n",
    "## Remove subjects with corrupted data.\n",
    "csv = csv[csv.index != 'CU0068CUMR1R1'] ## need to re-proproc\n",
    "\n",
    "## Remove subjects with missing data.\n",
    "csv = csv.dropna()\n",
    "subjects = list(csv.index)\n",
    "\n",
    "## Create questionnaire matrix. \n",
    "qmat = np.vstack([csv['MASQ_AA'], csv['SHAPS_Cont'], csv['AAQ']])\n",
    "\n",
    "## Normalize questionnaires.\n",
    "for i in np.arange(qmat.shape[0]):\n",
    "    qmat[i] = normalize(qmat[i])\n",
    "    \n",
    "## Add intercept column. \n",
    "intercept = np.array([1]*len(subjects))\n",
    "qmat = np.vstack([intercept,qmat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlate Functional Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-20T13:53:15.473860",
     "start_time": "2016-10-20T12:51:47.738381"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from scipy.stats import pearsonr\n",
    "from mne.filter import band_pass_filter\n",
    "\n",
    "n_preds, n_subs = qmat.shape\n",
    "n_regions = len(labels) + len(subcort_labels)\n",
    "n_corrs = (n_regions * n_regions)/2 - n_regions/2\n",
    "\n",
    "correlations = np.zeros((n_subs, n_corrs))\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print i, subject\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load cortical data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#    \n",
    "        \n",
    "    lh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'lh'))).get_data()\n",
    "    rh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'rh'))).get_data()\n",
    "    \n",
    "    lh_dat = lh_dat.squeeze()\n",
    "    rh_dat = rh_dat.squeeze()\n",
    "    \n",
    "    ## Drop first 4 volumes. \n",
    "    lh_dat = np.delete(lh_dat, np.arange(4), axis=1)\n",
    "    rh_dat = np.delete(rh_dat, np.arange(4), axis=1)\n",
    "\n",
    "    n_verts = lh_dat.shape[0]\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load subcortical data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#   \n",
    "    \n",
    "    sc_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.%s.2mm.nii.gz' %('mni305')))).get_data()\n",
    "    sc_dat = np.delete(sc_dat, np.arange(4), axis=3)\n",
    "\n",
    "    n_times = sc_dat.shape[3]\n",
    "    \n",
    "    if n_times > 176:\n",
    "        print 'Removing last %d timepoints for %s' %(n_times-176,subject)\n",
    "        lh_dat = np.delete(lh_dat, np.arange(176,n_times), axis=1)\n",
    "        rh_dat = np.delete(rh_dat, np.arange(176,n_times), axis=1)\n",
    "        sc_dat = np.delete(sc_dat, np.arange(176,n_times), axis=3)\n",
    "            \n",
    "    elif n_times < 176: \n",
    "        print 'Mismatch in number of timepoints. %s, n_times: %d. Missing %d timepoints.' %(subject, n_times, 176-n_times)\n",
    "        break\n",
    "        \n",
    "    n_times = sc_dat.shape[3]\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Average by labels.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "    mean_dat = np.zeros((n_regions,n_times))\n",
    "    \n",
    "    for idx, label in enumerate(labels):\n",
    "        if 'lh' in label.name:\n",
    "            mean_dat[idx] = lh_dat[label.vertices].mean(axis=0)\n",
    "        elif 'rh' in label.name:\n",
    "            mean_dat[idx] = rh_dat[label.vertices].mean(axis=0)\n",
    "            \n",
    "    for idx,k in zip(np.arange(len(labels), n_regions),subcort_labels.keys()):\n",
    "        x,y,z = subcort_labels[k]\n",
    "        mean_dat[idx] = sc_dat[x,y,z].mean(axis=0)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Perform Nuissance Regression and Filtering.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    n_components = 5        # No. of components to use from PCA.\n",
    "\n",
    "    ## Load nuisance regressors. \n",
    "    motion = os.path.join(fast_dir, subject, fsd, run, 'mcprextreg')\n",
    "    glob_sig = os.path.join(fast_dir, subject,fsd, run, 'global.waveform.dat')\n",
    "    wm_sig = os.path.join(fast_dir, subject,fsd, run, 'wm.dat')\n",
    "    vcsf_sig = os.path.join(fast_dir, subject,fsd, run, 'vcsf.dat')\n",
    "\n",
    "    motion = read_table(motion, sep=' ', header=None)\n",
    "    glob_sig = read_table(glob_sig, sep=' *', header=None, engine='python').as_matrix()\n",
    "    wm_sig = read_table(wm_sig, sep=' *', header=None, engine='python')\n",
    "    vcsf_sig = read_table(vcsf_sig, sep=' *', header=None, engine='python')\n",
    "\n",
    "    ## Prepare white matter and cerebral spinal fluid regressors.\n",
    "    wm_sig = wm_sig[wm_sig.columns[:n_components]].as_matrix()\n",
    "    wm_stats = read_table(os.path.join(fast_dir, subject, fsd, run, 'wm.dat.pca-stats.dat'), \n",
    "                           sep=' *', header=None, engine='python')\n",
    "\n",
    "    vcsf_sig = vcsf_sig[vcsf_sig.columns[:n_components]].as_matrix()\n",
    "    vcsf_stats = read_table(os.path.join(fast_dir, subject, fsd, run, 'vcsf.dat.pca-stats.dat'), \n",
    "                           sep=' *', header=None, engine='python')\n",
    "\n",
    "    ## Merge nuisance regressors. \n",
    "    nuissance_regressors = np.hstack((glob_sig, wm_sig, vcsf_sig, motion))\n",
    "\n",
    "    ## Remove first 4 timespoints from nuissance regressors.\n",
    "    nuissance_regressors = np.delete(nuissance_regressors, np.arange(4), axis=0)\n",
    "    nuissance_regressors = nuissance_regressors.T\n",
    "    \n",
    "    if nuissance_regressors.shape[1] > 176:\n",
    "        nuissance_regressors = np.delete(nuissance_regressors, np.arange(176,nuissance_regressors.shape[1]), axis=1)\n",
    "\n",
    "    n_regressors = nuissance_regressors.shape[0]\n",
    "\n",
    "    ## Specify parameters for filtering. \n",
    "    tr = 3\n",
    "    Fs = 1. / tr \n",
    "    Fp1 = 0.01\n",
    "    Fp2 = 0.08\n",
    "\n",
    "    ## Filter data.\n",
    "    mean_dat = band_pass_filter(mean_dat.astype(np.float64), Fs, Fp1, Fp2, filter_length='120s', method='iir')\n",
    "\n",
    "    #Filter nuissance regressors. \n",
    "    nuissance_regressors = band_pass_filter(nuissance_regressors, Fs, Fp1, Fp2, filter_length='120s', method='iir')\n",
    "\n",
    "    ## Detect areas of high motion. \n",
    "    threshold = 5          # No. of Standard deviations.\n",
    "    bad_timepoints = np.where(np.abs(nuissance_regressors[-6:]).T > threshold, True, False).prod(axis=1)\n",
    "\n",
    "    if bad_timepoints.sum() > 0: \n",
    "        print '%s acquisitions showing motion greater than %s SDs. Removing.' %(bad_timepoints.sum(),threshold)\n",
    "        mask = bad_timepoints.astype(bool)\n",
    "\n",
    "        ## Remove timepoints with high motion. \n",
    "        mean_dat = mean_dat[:,mask]\n",
    "        nuissance_regressors = nuissance_regressors[:,mask]\n",
    "\n",
    "    ## Perform nuissance regression. \n",
    "    def zscore(arr): return (arr - arr.mean()) / arr.std()\n",
    "\n",
    "    betas,residuals, _, _ = np.linalg.lstsq(nuissance_regressors.T, mean_dat.T)\n",
    "    predicted_nuissance_signal = np.dot(betas.T, nuissance_regressors)\n",
    "    regressed = mean_dat - predicted_nuissance_signal\n",
    "\n",
    "    ## Calculate pair-wise correlations. Save correlation matrix.\n",
    "    corrmat = np.corrcoef(regressed)\n",
    "\n",
    "    corrmat[np.tril_indices(corrmat.shape[0], 0)] = np.nan          ## Set lower-triangular to nans. \n",
    "    corrmat = corrmat[~np.isnan(corrmat)]                           ## Remove nans and flatten.\n",
    "\n",
    "    correlations[i] = corrmat\n",
    "\n",
    "## Save correlations. \n",
    "np.savetxt(os.path.join(fast_dir, 'scripts', 'subtypes_mult_regression', 'correlations'),correlations)\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform multiple regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T16:47:37.801121",
     "start_time": "2016-12-02T16:47:37.774259"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "## Load ?h_corr. \n",
    "fast_dir = '/space/will/3/users/EMBARC/EMBARC-FAST'\n",
    "correlations = np.loadtxt(os.path.join(fast_dir, 'scripts', 'subtypes_mult_regression', 'correlations'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-21T10:52:47.959246",
     "start_time": "2016-10-21T10:52:47.955520"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(correlations.shape[0])[:1]:\n",
    "    model = sm.OLS(correlations[:,i],qmat.T)\n",
    "    results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-21T10:58:49.452398",
     "start_time": "2016-10-21T10:58:49.445183"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02140098  0.01977132 -0.05151442  0.02105375]\n",
      "[-0.41822802  0.30741381 -0.56176854  0.70463761]\n",
      "[ 0.  0.  1. -1.]\n",
      "                             Test for Constraints                             \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "c0            -0.0726      0.097     -0.745      0.457        -0.265     0.120\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print results.params\n",
    "print results.tvalues\n",
    "r = np.zeros_like(results.params)\n",
    "r[2:] = [1,-1]\n",
    "print r \n",
    "\n",
    "t_test = results.t_test(r)\n",
    "print t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-21T11:03:20.251527",
     "start_time": "2016-10-21T11:03:20.246458"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "<F test: F=array([[ 0.30327601]]), p=0.823006777045, df_denom=199, df_num=3>\n",
      "0.303276008695\n"
     ]
    }
   ],
   "source": [
    "A = np.identity(len(results.params))\n",
    "A = A[1:,:]\n",
    "print A\n",
    "print(results.f_test(A))\n",
    "print results.fvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-21T10:44:53.444921",
     "start_time": "2016-10-21T10:44:53.394635"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.14285714  0.25      ]\n",
      "[ 1.87867287  0.98019606]\n",
      "                             Test for Constraints                             \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "c0             2.1429      1.141      1.879      0.119        -0.789     5.075\n",
      "==============================================================================\n",
      "<F test: F=array([[ 19.46078431]]), p=0.00437250591095, df_denom=5, df_num=2>\n"
     ]
    }
   ],
   "source": [
    "Y = [1,3,4,5,2,3,4]\n",
    "X = range(1,8)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(Y,X)\n",
    "results = model.fit()\n",
    "print results.params\n",
    "print results.tvalues\n",
    "print(results.t_test([1, 0]))\n",
    "print(results.f_test(np.identity(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-21T10:47:12.300800",
     "start_time": "2016-10-21T10:47:12.298268"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "(7, 2)\n"
     ]
    }
   ],
   "source": [
    "print len(Y)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-14T14:47:30.743839",
     "start_time": "2016-10-14T14:47:30.737855"
    }
   },
   "source": [
    "## Plot betas from multiple regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T15:45:42.261000",
     "start_time": "2016-10-17T15:45:39.547420"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "from matplotlib import cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "lh_names = ['caudalanteriorcingulate_1-lh', 'caudalmiddlefrontal_1-lh', 'insula_1-lh', 'insula_2-lh', 'isthmuscingulate_1-lh', 'lateralorbitofrontal_1-lh', \n",
    "            'lateralorbitofrontal_2-lh', 'medialorbitofrontal_1-lh', 'parsopercularis_1-lh', 'parsorbitalis_1-lh', 'parstriangularis_1-lh', \n",
    "            'posteriorcingulate_1-lh', 'rostralanteriorcingulate_1-lh', 'rostralmiddlefrontal_3-lh']\n",
    "\n",
    "for i,ques in enumerate(csv.columns[1:]):\n",
    "    \n",
    "    regmatfull = np.zeros((n_regions,n_regions))\n",
    "    regmatfull[np.triu_indices(regmatfull.shape[0],1)] = lh_qbetas[:,i]\n",
    "\n",
    "    fig = plt.figure(figsize=(16,16))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    cmap = cm.get_cmap('seismic', 50)\n",
    "    labels = lh_names\n",
    "    plt.title(ques)\n",
    "    a = np.max([np.abs(np.min(regmatfull)),np.max(regmatfull)])\n",
    "    cax = ax1.imshow(regmatfull, interpolation='nearest', cmap=cmap, vmin=-a, vmax=a)\n",
    "    ax1.set_xticks(np.arange(n_regions))\n",
    "    ax1.set_xticklabels(labels,ha='center',rotation=30,fontsize=12)\n",
    "    ax1.set_yticks(np.arange(n_regions))\n",
    "    ax1.set_yticklabels(labels,fontsize=12)\n",
    "    cbar = fig.colorbar(cax)\n",
    "    plt.tight_layout()\n",
    "#     plt.show()\n",
    "    plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_mult_regression/%s.png' %ques)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
