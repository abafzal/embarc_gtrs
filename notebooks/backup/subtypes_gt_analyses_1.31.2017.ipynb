{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-31T14:26:10.139897",
     "start_time": "2017-01-31T14:26:09.404814"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cortical lh: 57, rh: 57\n",
      "subcortical: 12\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from mne import read_label\n",
    "from pandas import read_csv\n",
    "from pandas import read_table\n",
    "\n",
    "## Specify directories and parameters.\n",
    "fast_dir = '/space/will/3/users/EMBARC/EMBARC-FAST'\n",
    "fsd = 'rest'\n",
    "run = '001'\n",
    "\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "### Load cortical labels.\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "\n",
    "parc_dir = '/space/lilli/1/users/DARPA-Recons/fscopy/label/yeo114_orig'\n",
    "lh_labels = [read_label(os.path.join(parc_dir, l)) for l in sorted(os.listdir(parc_dir)) if 'LH' in l]\n",
    "rh_labels = [read_label(os.path.join(parc_dir, l)) for l in sorted(os.listdir(parc_dir)) if 'RH' in l]\n",
    "\n",
    "labels = np.append(lh_labels,rh_labels)\n",
    "\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "### Load subcortical labels.\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## \n",
    "\n",
    "## Read Freesurfer Color Lookup Table. \n",
    "rois = ['Left-Accumbens-area', 'Left-Thalamus-Proper', 'Left-Caudate', 'Left-Putamen', 'Left-Hippocampus', 'Left-Amygdala', \n",
    "        'Right-Accumbens-area', 'Right-Thalamus-Proper', 'Right-Caudate', 'Right-Putamen', 'Right-Hippocampus', 'Right-Amygdala']\n",
    "\n",
    "subcort_labels = dict.fromkeys([l for l in rois])\n",
    "\n",
    "clut = read_table(os.path.join('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/labels', 'FreeSurferColorLUT_truncated.txt'), \n",
    "                  sep=' *', skiprows=4, names=['No', 'Label','R','G','B','A'], engine='python')\n",
    "clut = clut.loc[np.where(np.in1d(clut.Label,rois))]\n",
    "\n",
    "## Load subcortical segmentation.\n",
    "aseg_obj = nib.load('/space/lilli/1/users/DARPA-Recons/fsaverage/mri.2mm/aseg.mgz')\n",
    "aseg_dat = aseg_obj.get_data()\n",
    "\n",
    "for l in rois: subcort_labels[l] = np.where(aseg_dat == int(clut.No[clut.Label == l]))\n",
    "print 'cortical lh: %d,' %len(lh_labels), 'rh: %d\\n' %len(rh_labels), 'subcortical: %d' %len(subcort_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-31T14:26:14.751542",
     "start_time": "2017-01-31T14:26:14.693444"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    return [( float(i) - min(arr) )/( max(arr) - min(arr)) for i in arr] \n",
    "\n",
    "## Load questionnaires. \n",
    "csv = read_csv('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/questionnaires/subtypes_qscores.csv')\n",
    "\n",
    "## Restrict to questionnaires of interest.\n",
    "csv.columns = ['Subject_ID', 'Diagnosis','MASQ_AA', 'SHAPS_Cont', 'AAQ']\n",
    "csv = csv.set_index('Subject_ID', drop=True)\n",
    "\n",
    "## Restrict to psychiatric subjects. \n",
    "csv = csv[csv['Diagnosis'] == 1]\n",
    "\n",
    "## Remove subjects with corrupted data.\n",
    "csv = csv[csv.index != 'CU0068CUMR1R1'] ## need to re-proproc\n",
    "\n",
    "## Remove subjects with missing data.\n",
    "csv = csv.dropna()\n",
    "subjects = list(csv.index)\n",
    "\n",
    "## Create questionnaire matrix. \n",
    "qmat = np.vstack([csv['MASQ_AA'], csv['SHAPS_Cont'], csv['AAQ']])\n",
    "\n",
    "## Normalize questionnaires.\n",
    "for i in np.arange(qmat.shape[0]):\n",
    "    qmat[i] = normalize(qmat[i])\n",
    "    \n",
    "## Add intercept column. \n",
    "intercept = np.array([1]*len(subjects))\n",
    "qmat = np.vstack([intercept,qmat])\n",
    "\n",
    "## Save out list of subjects.\n",
    "with open('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects', 'w') as f:\n",
    "    for s in subjects: \n",
    "        f.write(s+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data. Average by Labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-31T14:02:15.928162",
     "start_time": "2017-01-31T13:55:39.325130"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 CU0092CUMR1R1\n",
      "1 CU0087CUMR1R1\n",
      "2 CU0095CUMR1R1\n",
      "3 CU0104CUMR1R1\n",
      "4 CU0131CUMR1R1\n",
      "5 CU0069CUMR1R1\n",
      "6 CU0025CUMR1R1\n",
      "7 CU0094CUMR1R1\n",
      "8 CU0067CUMR1R1\n",
      "9 CU0106CUMR1R1\n",
      "10 CU0009CUMR1R1\n",
      "11 CU0105CUMR1R1\n",
      "12 CU0070CUMR1R1\n",
      "13 CU0129CUMR1R1\n",
      "14 CU0113CUMR1R1\n",
      "15 CU0125CUMR1R1\n",
      "16 CU0133CUMR1R1\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "from pandas import DataFrame\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.decomposition import PCA\n",
    "from mne.filter import band_pass_filter\n",
    "\n",
    "## Define functions and directories.\n",
    "def zscore(arr): return (arr - arr.mean()) / arr.std()\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Define matrices.\n",
    "n_preds, n_subs = qmat.shape\n",
    "n_regions = len(labels) + len(subcort_labels)\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "# ## Subjects with manual coregistration.\n",
    "# subjects = ['CU0092CUMR1R1', 'CU0087CUMR1R1', 'CU0095CUMR1R1', 'CU0104CUMR1R1', 'CU0131CUMR1R1', 'CU0069CUMR1R1', \n",
    "#             'CU0025CUMR1R1', 'CU0094CUMR1R1', 'CU0067CUMR1R1', 'CU0106CUMR1R1', 'CU0009CUMR1R1', 'CU0105CUMR1R1', \n",
    "#             'CU0070CUMR1R1', 'CU0129CUMR1R1', 'CU0113CUMR1R1', 'CU0125CUMR1R1', 'CU0133CUMR1R1']\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print i, subject\n",
    "\n",
    "    ## Load cortical/subcortical data.\n",
    "    lh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'lh'))).get_data().squeeze()\n",
    "    rh_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.fsaverage.%s.nii.gz' %'rh'))).get_data().squeeze()\n",
    "    sc_dat = (nib.load(os.path.join(fast_dir, subject, fsd, run, 'fmcpr.sm6.%s.2mm.nii.gz' %('mni305')))).get_data()\n",
    "    \n",
    "    ## Drop first 4 volumes. \n",
    "    lh_dat = np.delete(lh_dat, np.arange(4), axis=1)\n",
    "    rh_dat = np.delete(rh_dat, np.arange(4), axis=1)\n",
    "    sc_dat = np.delete(sc_dat, np.arange(4), axis=3)\n",
    "\n",
    "    n_times = sc_dat.shape[3]\n",
    "    n_verts = lh_dat.shape[0]\n",
    "    \n",
    "    ## Remove any extra timepoints collected.\n",
    "    if n_times > 176:\n",
    "        print 'Removing last %d timepoints for %s' %(n_times-176,subject)\n",
    "        lh_dat = np.delete(lh_dat, np.arange(176,n_times), axis=1)\n",
    "        rh_dat = np.delete(rh_dat, np.arange(176,n_times), axis=1)\n",
    "        sc_dat = np.delete(sc_dat, np.arange(176,n_times), axis=3)\n",
    "        n_times = sc_dat.shape[3]\n",
    "            \n",
    "    elif n_times < 176: \n",
    "        print 'Mismatch in number of timepoints. %s, n_times: %d. Missing %d timepoints.' %(subject, n_times, 176-n_times)\n",
    "        break\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Average by labels.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "    mean_dat = np.zeros((n_regions,n_times))\n",
    "    \n",
    "    for idx, label in enumerate(lh_labels):\n",
    "        mean_dat[idx] = lh_dat[label.vertices].mean(axis=0)\n",
    "        \n",
    "    for idx, label in enumerate(rh_labels): \n",
    "        mean_dat[idx+len(lh_labels)] = rh_dat[label.vertices].mean(axis=0)\n",
    "            \n",
    "    for idx,k in zip(np.arange(len(labels), n_regions),subcort_labels.keys()):\n",
    "        x,y,z = subcort_labels[k]\n",
    "        mean_dat[idx] = sc_dat[x,y,z].mean(axis=0)\n",
    "        \n",
    "    ## Save data averaged by labels. \n",
    "    np.savez_compressed(os.path.join(gt_dir, 'raw', '%s_raw' %subject), mean_dat=mean_dat)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Nuisance Regress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-01-31T19:26:17.844Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.signal import detrend\n",
    "\n",
    "## Specify directories. \n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "## Subjects with manual coregistration.\n",
    "subjects = ['CU0092CUMR1R1', 'CU0087CUMR1R1', 'CU0095CUMR1R1', 'CU0104CUMR1R1', 'CU0131CUMR1R1', 'CU0069CUMR1R1', \n",
    "            'CU0025CUMR1R1', 'CU0094CUMR1R1', 'CU0067CUMR1R1', 'CU0106CUMR1R1', 'CU0009CUMR1R1', 'CU0105CUMR1R1', \n",
    "            'CU0070CUMR1R1', 'CU0129CUMR1R1', 'CU0113CUMR1R1', 'CU0125CUMR1R1', 'CU0133CUMR1R1']\n",
    "\n",
    "for i, subject in enumerate(subjects[:1]):\n",
    "    \n",
    "    print i, subject\n",
    "    \n",
    "    ## Load data.\n",
    "    mean_dat = np.load(os.path.join(gt_dir, 'raw', '%s_raw.npz' %subject))['mean_dat']\n",
    "    \n",
    "    ## Remove first 4 timepoints from data.\n",
    "    mean_dat = np.delete(mean_dat, np.arange(4), axis=1)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Perform Nuissance Regression and Filtering.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    n_components = 5        # No. of components to use from PCA.\n",
    "\n",
    "    ## Load global signal regressors. \n",
    "    gs = os.path.join(fast_dir, subject,fsd, run, 'global.waveform.dat')\n",
    "    gs = read_table(gs, sep=' *', header=None, engine='python').as_matrix()\n",
    "    \n",
    "    ## Load wm and csf regressors. \n",
    "    wm = read_table(os.path.join(fast_dir, subject,fsd, run, 'wm.dat'), sep=' *', header=None, engine='python')\n",
    "    vcsf = read_table(os.path.join(fast_dir, subject,fsd, run, 'vcsf.dat'), sep=' *', header=None, engine='python')\n",
    "    \n",
    "    ## Keep only top 5 components.\n",
    "    n_components = 5 \n",
    "    wm = wm[wm.columns[:n_components]].as_matrix()\n",
    "    vcsf = vcsf[vcsf.columns[:n_components]].as_matrix()\n",
    "    \n",
    "    ## Remove extra timepoints. \n",
    "    if gs.shape[0] > 176: gs = np.delete(gs, np.arange(176,gs.shape[0]), axis=0)\n",
    "    if wm.shape[0] > 176: wm = np.delete(wm, np.arange(176,wm.shape[0]), axis=0)\n",
    "    if vcsf.shape[0] > 176: vcsf = np.delete(vcsf, np.arange(176,vcsf.shape[0]), axis=0)\n",
    "    \n",
    "    ## Load motion regressors.\n",
    "    mc = os.path.join(fast_dir, subject, fsd, run, 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:7]\n",
    "    if mc.shape[0] > 176:\n",
    "        mc = np.delete(mc, np.arange(176,mc.shape[0]), axis=0)\n",
    "\n",
    "    ## Demean/detrend data.\n",
    "    mc = detrend(mc, axis=0, type='constant')\n",
    "    mc = detrend(mc, axis=0, type='linear') ## Keeps breaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-01-31T19:19:59.466Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    ## Construct basis sets.\n",
    "    '''\n",
    "    SAM'S NOTE(edited): Might want to fix the second step in the below three lines. \n",
    "    Probably want to insert a row of zeros into the last timepoints of the \"rolled\" \n",
    "    regressors, otherwise you are putting motion from the first frame into the last\n",
    "    of the run\n",
    "    '''\n",
    "    mc = np.concatenate([mc, np.roll(mc, -1, 0)], axis=-1)\n",
    "    mc = np.concatenate([mc, np.power(mc,2)], axis=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-01-31T19:19:24.833Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    ## Replace last timepoint with zeros \n",
    "    ## Otherwise you are putting motion from the first frame into the last of the run\n",
    "    mc[-1] = np.zeros((mc[-1].shape))\n",
    "\n",
    "    ## Apply PCA.  \n",
    "    pca = PCA(n_components=24)\n",
    "    mc = pca.fit_transform(mc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-01-31T19:08:05.013Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    ## Concatenate nuisance regressors. \n",
    "    ns = np.hstack((mc,gs,wm,vcsf))\n",
    "    \n",
    "    ## Remove first 4 timepoints.\n",
    "    ns = np.delete(ns, np.arange(4), axis=0) ## remove first 4 timepoints\n",
    "    \n",
    "    ## Filter parameters.\n",
    "    tr = 3\n",
    "    Fs = 1. / tr \n",
    "    Fp1 = 0.01\n",
    "    Fp2 = 0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-01-31T19:07:04.190Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    ## Filter data and regressors.\n",
    "    mean_dat = band_pass_filter(mean_dat.astype(np.float64), Fs, Fp1, Fp2, filter_length='120s', method='iir')\n",
    "    ns = band_pass_filter(ns.T, Fs, Fp1, Fp2, filter_length='120s', method='iir').T\n",
    "    \n",
    "    ## Regress. \n",
    "    betas,_,_,_ = np.linalg.lstsq(ns,mean_dat.T)\n",
    "    regressed = mean_dat - np.dot(ns, betas).T\n",
    "    \n",
    "    ## Save regressed data.\n",
    "    np.savez_compressed(os.path.join(gt_dir, 'regressed', '%s_regressed' %subject), regressed=regressed)\n",
    "    \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-11T10:13:39.897679",
     "start_time": "2017-01-11T10:13:33.315034"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    \n",
    "    print i, subject\n",
    "    \n",
    "    ## Load regressed data. \n",
    "    regressed = np.load(os.path.join(gt_dir, 'regressed', '%s_regressed.npz' %subject))['regressed']\n",
    "    \n",
    "    ## Correlate. Remove negative correlations.\n",
    "    corrmat = np.corrcoef(regressed)\n",
    "    corrmat = np.where(corrmat<=0, np.nan, corrmat)\n",
    "    \n",
    "    # Fisher's r-to-z transform.\n",
    "    zmat = np.arctanh(corrmat)\n",
    "    zmat = np.where(zmat==np.inf,np.nan,zmat)\n",
    "\n",
    "    ## Save correlation.\n",
    "    np.savez(os.path.join(gt_dir, 'correlations', '%s_correlations' %subject), corrmat=corrmat)\n",
    "    np.savez(os.path.join(gt_dir, 'correlations', '%s_zscores' %subject), zmat=zmat)\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-11T10:31:46.936000",
     "start_time": "2017-01-11T10:31:40.224662"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import networkx as nx\n",
    "import scipy.io as sio\n",
    "%matplotlib inline\n",
    "\n",
    "## Specify directories.\n",
    "gt_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt'\n",
    "\n",
    "## Load labels.\n",
    "labels = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_labels\", 'r')]\n",
    "n_labels = len(labels)\n",
    "\n",
    "## Specify subjects.\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "n_subs = len(subjects)\n",
    "\n",
    "Gnets = []\n",
    "for i,subject in enumerate(subjects): \n",
    "    zmat = np.load(os.path.join(gt_dir, 'correlations', '%s_zscores.npz' %subject))['zmat']\n",
    "    \n",
    "    ## Create graph.\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(labels)\n",
    "\n",
    "    for i,x in enumerate(labels):\n",
    "            for j,y in enumerate(labels):\n",
    "                if not np.isnan(zmat[i,j]): \n",
    "                    G.add_edge(x,y)\n",
    "\n",
    "    G.remove_nodes_from(G.nodes_with_selfloops())\n",
    "    Gnets.append(G)\n",
    "    \n",
    "Gnets = np.array(Gnets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute global network metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Network metrics. \n",
    "metrics = ['n_nodes', 'n_edges', 'avg_degree', 'avg_path', 'density', 'n_components']\n",
    "metrics_dat = np.zeros((len(metrics), n_subs))\n",
    "\n",
    "for i in range(n_subs):\n",
    "    metrics_dat[0,i] = nx.number_of_nodes(Gnets[i])\n",
    "    metrics_dat[1,i] = nx.number_of_edges(Gnets[i])\n",
    "    metrics_dat[2,i] = sum(nx.degree(Gnets[i]).values())/Gnets[i].number_of_nodes()\n",
    "    metrics_dat[3,i] = nx.average_shortest_path_length(Gnets[i])\n",
    "    metrics_dat[4,i] = nx.density(Gnets[i])\n",
    "    metrics_dat[5,i] = len([l for l in nx.connected_components(Gnets[i])])\n",
    "    \n",
    "## Average degree of nodes in G should be >= 2*np.log(G.number_of_nodes()) Bullmore 2006/Drakesmith 2015.\n",
    "if not np.all(metrics_dat[2] > 2*np.log2(metrics_dat[0])): print 'Average degree of nodes not <= 2*log(#nodes)'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-12T13:25:41.974565",
     "start_time": "2017-01-12T13:25:41.972066"
    }
   },
   "source": [
    "### Plot distribution of global metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-12T13:25:26.794615",
     "start_time": "2017-01-12T13:25:19.191691"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plotting\n",
    "plt.figure(figsize=(24,18))\n",
    "for r in range(len(metrics)):\n",
    "    ax = plt.subplot2grid((2,3),(r/3,r%3),colspan=1,rowspan=1)\n",
    "    ax.hist(metrics_dat[r], normed=True, bins=40)\n",
    "    ax.set_title(metrics[r], fontsize=16)\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "plt.suptitle('Distribution of Graph Metrics (unthresholded)', fontsize=24)\n",
    "plt.savefig('/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/plots/metrics_dist_unthresholded.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-12T13:21:47.400605",
     "start_time": "2017-01-12T13:21:47.398525"
    }
   },
   "source": [
    "### Compute node-specific metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-12T11:39:09.001741",
     "start_time": "2017-01-12T11:38:50.422857"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "node_metrics = ['degree', 'clustering coefficient']\n",
    "\n",
    "## Node-specific metrics.\n",
    "degree_dist = pd.DataFrame(columns=labels,index=subjects)\n",
    "clus_coeff = pd.DataFrame(columns=labels,index=subjects)\n",
    "for i,sub in enumerate(subjects):\n",
    "    degree_dist.loc[sub] = pd.Series(nx.degree(Gnets[i]))\n",
    "    clus_coeff.loc[sub] = pd.Series(nx.clustering(Gnets[i]))\n",
    "degree_dist = degree_dist.fillna(value=0)\n",
    "clus_coeff = clus_coeff.fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Motion.\n",
    "### Mask native timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-10T16:12:08.577768",
     "start_time": "2017-01-10T21:10:13.697Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Please see /space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/wm_masks.csh to make wm masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-16T21:56:53.651909",
     "start_time": "2017-01-16T14:32:38.328762"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pandas import read_csv\n",
    "from mne.filter import construct_iir_filter, filter_data\n",
    "def demean(arr): return arr - arr.mean()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "decim = 250\n",
    "# subjects = ['CU0016CUMR1R1']\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    print subject\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and prepare masks.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    mask_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001/masks' %subject\n",
    "    mri_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001' %subject\n",
    "    out_dir = '/space/will/3/users/EMBARC/EMBARC-FAST/%s/rest/001/motion' %subject\n",
    "\n",
    "    brainmask = os.path.join(mask_dir,'brain.nii.gz')\n",
    "    brainmask = np.where( nib.load(brainmask).get_data(), 1, 0 ) # Binarize the mask\n",
    "\n",
    "    wm = os.path.join(mask_dir,'wm.mgz')\n",
    "    wm = np.where( nib.load(wm).get_data(), 1, 0 ) # Binarize the mask\n",
    "\n",
    "    gm = brainmask - wm\n",
    "\n",
    "    ## Reduce to indices of interest.\n",
    "    gm = np.vstack(np.where(gm))[:,::decim]\n",
    "    wm = np.vstack(np.where(wm))[:,::decim]\n",
    "    del brainmask\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and slice through EPI image.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load data.\n",
    "    obj = nib.load(os.path.join(mri_dir, 'fmcpr.nii.gz'))\n",
    "    _,_,_,n_acq = obj.shape\n",
    "\n",
    "    ## Preallocoate space for timeseries.\n",
    "    gmts = np.zeros((n_acq, gm.shape[-1]))\n",
    "    wmts = np.zeros((n_acq, wm.shape[-1]))\n",
    "\n",
    "    for n in range(n_acq):\n",
    "\n",
    "        ## Slice image.\n",
    "        acq = obj.dataobj[..., n]\n",
    "\n",
    "        ## Store grey matter.\n",
    "        gmts[n] += acq[gm[0],gm[1],gm[2]]\n",
    "\n",
    "        ## Store white matter.\n",
    "        wmts[n] += acq[wm[0],wm[1],wm[2]]\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Preprocessing data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ## Construct highpass filter.\n",
    "    tr = 3\n",
    "    sfreq = 1. / tr\n",
    "    high_pass = 0.01\n",
    "    iir_params = dict(order=2, ftype='butter', output='sos') # Following Power et al. (2014)\n",
    "    iir_params = construct_iir_filter(iir_params, high_pass, None, sfreq, 'highpass', return_copy=False)  \n",
    "\n",
    "    ## Filter data.\n",
    "    gmts = filter_data(gmts.T, sfreq, high_pass, None, method='iir', iir_params=iir_params, verbose=False)\n",
    "    wmts = filter_data(wmts.T, sfreq, high_pass, None, method='iir', iir_params=iir_params, verbose=False)\n",
    "\n",
    "    ## De-mean.\n",
    "    gmts = np.apply_along_axis(demean, 1, gmts)\n",
    "    wmts = np.apply_along_axis(demean, 1, wmts)\n",
    "\n",
    "    ## Re-organize (center outwards).\n",
    "    gmts = gmts[ np.argsort( np.power( np.apply_along_axis(demean, 1, gm), 2 ).sum(axis=0) ) ]\n",
    "    wmts = gmts[ np.argsort( np.power( np.apply_along_axis(demean, 1, wm), 2 ).sum(axis=0) ) ]\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Save data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    if not os.path.isdir(out_dir): os.makedirs(out_dir)\n",
    "    f = os.path.join(out_dir, '%s_rest_qc_data' %subject)\n",
    "    np.savez_compressed(f, gm=gmts, wm=wmts, iir_params=iir_params )\n",
    "    del gmts, wmts, obj\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare motion and masked fMRI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "from scipy.signal import detrend\n",
    "from sklearn.decomposition import PCA\n",
    "def demean(arr): return arr - arr.mean()\n",
    "def rms(arr): return np.sqrt( np.mean( np.power(arr, 2) ) )\n",
    "%matplotlib inline\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "threshold = 0.5\n",
    "fast_dir = '/space/will/3/users/EMBARC/EMBARC-FAST'\n",
    "fsd = 'rest'\n",
    "run = '001'\n",
    "\n",
    "subjects = [line.strip() for line in open(\"/space/will/3/users/EMBARC/EMBARC-FAST/scripts/subtypes_gt/gt_subjects\", 'r')]\n",
    "\n",
    "for subject in subjects[:1]:\n",
    "    \n",
    "    print subject, \n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare motion data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Read motion data.\n",
    "    mc = os.path.join(fast_dir, subject, fsd, run, 'fmcpr.mcdat')\n",
    "    mc = np.loadtxt(mc)[:,1:7]\n",
    "\n",
    "    ## Remove first four timepoints.\n",
    "    mc = np.delete(mc, np.arange(4),axis=0)\n",
    "\n",
    "    ## Invert angular displacement.\n",
    "    copy = mc.copy()\n",
    "    mc[:,:3] = np.deg2rad(mc[:,:3]) \n",
    "    mc[:,:3] *= 50\n",
    "\n",
    "    ## Compute framewise displacement (See Power 2012, 2014).\n",
    "    fd = np.insert( np.abs( np.diff(mc[:,3:], axis=0) ).sum(axis=1), 0, 0 )\n",
    "\n",
    "    ## Demean/detrend data.\n",
    "    mc = detrend(copy, axis=0, type='constant')\n",
    "    mc = detrend(mc, axis=0, type='linear')\n",
    "\n",
    "    ## Construct basis sets.\n",
    "    mcreg24 = mc.copy()\n",
    "    mcreg24 = np.concatenate([mcreg24, np.roll(mcreg24, -1, 0)], axis=-1)\n",
    "    mcreg24 = np.concatenate([mcreg24, np.power(mcreg24,2)], axis=-1)\n",
    "\n",
    "    ## Apply PCA.  \n",
    "    pca = PCA(n_components=24)\n",
    "    mcreg24 = pca.fit_transform(mcreg24)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Prepare fMRI data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load gray/white matter timeseries.\n",
    "    npz = np.load(os.path.join(fast_dir, subject, fsd, run, 'motion/%s_rest_qc_data.npz' %subject))\n",
    "    gm_pre = np.apply_along_axis(demean, 1, np.delete(npz['gm'],np.arange(4),axis=1))\n",
    "    wm_pre = np.apply_along_axis(demean, 1, np.delete(npz['wm'],np.arange(4),axis=1))\n",
    "\n",
    "    gm_beta, _, _, _ = np.linalg.lstsq(mcreg24, gm_pre.T)\n",
    "    wm_beta, _, _, _ = np.linalg.lstsq(mcreg24, wm_pre.T)\n",
    "    gm_post = gm_pre - np.dot(mcreg24, gm_beta).T\n",
    "    wm_post = wm_pre - np.dot(mcreg24, wm_beta).T\n",
    "\n",
    "    ## Compute DVARS.\n",
    "    gm_DVARS = np.zeros((2,fd.shape[0]))\n",
    "    wm_DVARS = np.zeros((2,fd.shape[0]))\n",
    "    for n, arr in enumerate([gm_pre,gm_post]): gm_DVARS[n,1:] += np.apply_along_axis(rms, 0, np.diff(arr, axis=1))\n",
    "    for n, arr in enumerate([wm_pre,wm_post]): wm_DVARS[n,1:] += np.apply_along_axis(rms, 0, np.diff(arr, axis=1))\n",
    "            \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plotting\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-17T10:47:13.416135",
     "start_time": "2017-01-17T10:41:50.200846"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CU0002CUMR1R1 CU0014CUMR1R1 CU0016CUMR1R1 CU0024CUMR1R1 CU0025CUMR1R1 CU0029CUMR1R1 CU0033CUMR1R1 CU0034CUMR1R1 CU0036CUMR1R1 CU0039CUMR1R1 CU0040CUMR1R1 CU0046CUMR1R1 CU0047CUMR1R1 CU0051CUMR1R1 CU0052CUMR1R1 CU0056CUMR1R1 CU0057CUMR1R1 CU0059SBMR1R1 CU0060SBMR1R1 CU0061SBMR1R1 CU0067CUMR1R1 CU0069CUMR1R1 CU0070CUMR1R1 CU0071CUMR1R1 CU0072CUMR1R1 CU0074CUMR1R1 CU0077CUMR1R1 CU0078CUMR1R1 CU0081CUMR1R1 CU0082CUMR1R1 CU0083CUMR1R1 CU0085CUMR1R1 CU0087CUMR1R1 CU0089CUMR1R1 CU0090CUMR1R1 CU0092CUMR1R1 CU0093CUMR1R1 CU0094CUMR1R1 CU0095CUMR1R1 CU0097CUMR1R1 CU0100CUMR1R1 CU0102CUMR1R1 CU0103CUMR1R1 CU0104CUMR1R1 CU0105CUMR1R1 CU0106CUMR1R1 CU0108CUMR1R1 CU0110CUMR1R1 CU0113CUMR1R1 CU0115CUMR1R1 CU0117CUMR1R1 CU0119CUMR1R1 CU0120CUMR1R1 CU0125CUMR1R1 CU0126CUMR1R1 CU0128CUMR1R1 CU0129CUMR1R1 CU0130CUMR1R1 CU0131CUMR1R1 CU0133CUMR1R1 MG0018MGMR1R1 MG0021MGMR1R1 MG0032MGMR1R1 MG0039MGMR1R1 MG0060MGMR1R1 MG0064MGMR1R1 MG0069MGMR1R1 MG0076MGMR1R1 MG0101MGMR1R1 MG0106MGMR1R1 MG0112MGMR1R1 MG0116MGMR1R1 MG0125MGMR1R1 MG0126MGMR1R1 MG0135MGMR1R1 MG0137MGMR1R1 MG0138MGMR1R1 MG0152MGMR1R1 MG0158MGMR1R1 MG0164MGMR1R1 MG0168MGMR1R1 MG0172MGMR1R1 MG0185MGMR1R1 MG0187MGMR1R1 MG0213MGMR1R1 MG0218MGMR1R1 MG0228MGMR1R1 MG0231MGMR1R1 MG0238MGMR1R1 MG0239MGMR1R1 MG0242MGMR1R1 MG0246MGMR1R1 MG0251MGMR1R1 MG0256MGMR1R1 MG0257MGMR1R1 MG0261MGMR1R1 MG0269MGMR1R1 MG0270MGMR1R1 TX0001TXMR1R1 TX0005TXMR1R1 TX0006TXMR1R1 TX0010TXMR1R1 TX0012TXMR1R1 TX0014TXMR1R1 TX0020TXMR1R1 TX0031TXMR1R1 TX0043TXMR1R1 TX0046TXMR1R1 TX0047TXMR1R1 TX0050TXMR1R1 TX0055TXMR1R1 TX0059TXMR1R1 TX0061TXMR1R1 TX0066TXMR1R1 TX0067TXMR1R1 TX0068TXMR1R1 TX0070TXMR1R1 TX0071TXMR1R1 TX0074TXMR1R1 TX0078TXMR1R1 TX0085TXMR1R1 TX0088TXMR1R1 TX0091TXMR1R1 TX0092TXMR1R1 TX0093TXMR1R1 TX0094TXMR1R1 TX0095TXMR1R1 TX0096TXMR1R1 TX0097TXMR1R1 TX0100TXMR1R1 TX0101TXMR1R1 TX0103TXMR1R1 TX0106TXMR1R1 TX0107TXMR1R1 TX0108TXMR1R1 TX0110TXMR1R1 TX0112TXMR1R1 TX0114TXMR1R1 TX0115TXMR1R1 TX0119TXMR1R1 TX0122TXMR1R1 TX0123TXMR1R1 TX0129TXMR1R1 TX0130TXMR1R1 TX0132TXMR1R1 TX0133TXMR1R1 TX0135TXMR1R1 TX0136TXMR1R1 TX0139TXMR1R1 TX0140TXMR1R1 TX0141TXMR1R1 TX0142TXMR1R1 TX0145TXMR1R1 TX0149TXMR1R1 TX0151TXMR1R1 TX0153TXMR1R1 TX0155TXMR1R1 TX0159TXMR1R1 TX0162TXMR1R1 TX0165TXMR1R1 TX0169TXMR1R1 TX0172TXMR1R1 TX0175TXMR1R1 TX0177TXMR1R1 TX0179TXMR1R1 TX0182TXMR1R1 TX0187TXMR1R1 TX0188TXMR1R1 TX0193TXMR1R1 TX0194TXMR1R1 TX0195TXMR1R1 TX0204TXMR1R1 UM0003UMMR1R1 UM0004UMMR1R1 UM0008UMMR1R1 UM0011UMMR1R1 UM0013UMMR1R1 UM0015UMMR1R1 UM0025UMMR1R1 UM0027UMMR1R1 UM0036UMMR1R1 UM0037UMMR1R1 UM0038UMMR1R1 UM0047UMMR1R1 UM0048UMMR1R1 UM0049UMMR1R1 UM0050UMMR1R1 UM0056UMMR1R1 UM0058UMMR1R1 UM0060UMMR1R1 UM0065UMMR1R1 UM0066UMMR1R1 UM0078UMMR1R1 UM0079UMMR1R1 UM0080UMMR1R1 UM0082UMMR1R1 UM0083UMMR1R1 UM0088UMMR1R1 UM0090UMMR1R1 UM0114UMMR1R1 UM0118UMMR1R1 UM0119UMMR1R1 UM0121UMMR1R1 Done.\n"
     ]
    }
   ],
   "source": [
    "    fig = plt.figure(figsize=(12,16))\n",
    "    nrow = 19\n",
    "\n",
    "    ## Plot framewise displacement.\n",
    "    ax = plt.subplot2grid((nrow,1),(0,0),rowspan=1)\n",
    "    ax.plot(fd, linewidth=1.5, color='k')\n",
    "    ax.hlines(0.5, 0, fd.shape[0], linestyle='--', color='k', alpha=0.5)\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([0.0,0.5,1.0])\n",
    "    ax.set_yticklabels([0.0,0.5,1.0], rotation=90)\n",
    "    ax.set_ylabel('FD (mm)', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.set_title(subject, fontsize=24)\n",
    "\n",
    "    ## Plot grey matter (pre-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(1,0),rowspan=4)\n",
    "    cbar = ax.imshow(gm_pre, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('Grey Matter\\nPre-Regression', fontsize=12)\n",
    "\n",
    "    # Colobar setup.\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width, box.height])\n",
    "    axColor = plt.axes([0.905, 0.238, 0.015, 0.660])\n",
    "    cbar = plt.colorbar(cbar, cax = axColor, orientation=\"vertical\")\n",
    "    cbar.set_ticks([-50,0,50])\n",
    "    axColor.set_ylabel('BOLD', fontsize=12, rotation=270)\n",
    "\n",
    "    ## Plot grey matter (post-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(5,0),rowspan=4)\n",
    "    cbar = ax.imshow(gm_post, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('Grey Matter\\nPost-Regression', fontsize=12)\n",
    "\n",
    "    ## Plot DVARS.\n",
    "    ax = plt.subplot2grid((nrow,1),(9,0),rowspan=1)\n",
    "    for arr, label in zip(gm_DVARS, ['Pre','Post']): ax.plot(arr, linewidth=1.5, alpha=0.8, label=label)\n",
    "    ax.legend(loc=2, fontsize=8, frameon=False, borderpad=0.0,  handlelength=1.4, handletextpad=0.2)\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_xlabel('Acquisitions', fontsize=12)\n",
    "    ax.set_ylim(20,100)\n",
    "    ax.set_yticks([20,60,100])\n",
    "    ax.set_yticklabels([20,60,100], rotation=90)\n",
    "    ax.set_ylabel('DVARS_gm', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    ## Plot white matter (pre-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(11,0),rowspan=2)\n",
    "    cbar = ax.imshow(wm_pre, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('White Matter\\nPre-Regression', fontsize=12)\n",
    "\n",
    "    ## Plot white matter (post-regression).\n",
    "    ax = plt.subplot2grid((nrow,1),(13,0),rowspan=2)\n",
    "    cbar = ax.imshow(wm_post, aspect='auto', interpolation='none', origin='lower', cmap='bone', vmin=-50, vmax=50)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('White Matter\\nPost-Regression', fontsize=12)\n",
    "\n",
    "    ## Plot DVARS.\n",
    "    ax = plt.subplot2grid((nrow,1),(15,0))\n",
    "    for arr, label in zip(wm_DVARS, ['Pre','Post']): ax.plot(arr, linewidth=1.5, alpha=0.8, label=label)\n",
    "    ax.legend(loc=2, fontsize=8, frameon=False, borderpad=0.0,  handlelength=1.4, handletextpad=0.2)\n",
    "    ax.set_xlim(0,fd.shape[0])\n",
    "    ax.set_xlabel('Acquisitions', fontsize=12)\n",
    "    ax.set_ylim(20,100)\n",
    "    ax.set_yticks([20,60,100])\n",
    "    ax.set_yticklabels([20,60,100], rotation=90)\n",
    "    ax.set_ylabel('DVARS_wm', fontsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    plt.subplots_adjust(left=0.05, right=0.90, top=0.95, bottom=0.05, hspace=0.05)\n",
    "#     plt.show()\n",
    "    plt.savefig(os.path.join(fast_dir,subject,fsd,run,'motion/%s_supp_motion.png' %subject), dpi=180)\n",
    "    plt.close('all')\n",
    "    \n",
    "print 'Done.'"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "929px",
   "left": "0px",
   "right": "1708px",
   "top": "106px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
